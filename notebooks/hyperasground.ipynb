{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from hyperopt import Trials, STATUS_OK, tpe\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Dense, Embedding, Input, Reshape, concatenate, Flatten, Activation, LSTM\n",
    "from keras.models import Model\n",
    "from keras.utils import np_utils\n",
    "\n",
    "from hyperas import optim\n",
    "from hyperas.distributions import choice, uniform\n",
    "\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import multiprocessing\n",
    "\n",
    "from tqdm import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### CONFIGURATION SETUP ####\n",
    "\n",
    "data_path = \"../logs/bpic2011.xes\"\n",
    "traces_finalpath = data_path.replace(\".xes\", \"_traces_encoded.pickled\")\n",
    "traces_dictionarypath = data_path.replace(\".xes\", \"_dictionaries.pickled\")\n",
    "n_sp2_features = 624\n",
    "n_pfs_features = 25\n",
    "\n",
    "traces = pickle.load(open(traces_finalpath, \"rb\"))\n",
    "feature_dict = pickle.load(open(traces_dictionarypath, \"rb\"))\n",
    "\n",
    "### CONFIGURATION SETUP END ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_input_name(var_name):\n",
    "    return \"input_{0}\".format(''.join(c for c in var_name if c.isalnum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle complete traces and create test and training set\n",
    "random.shuffle(traces)\n",
    "sep_idx = int(0.8*len(traces))\n",
    "\n",
    "# extract the feature indices\n",
    "# data is organized like this: ordinal features | categorical features | SP2 features | PFS features | TARGET features\n",
    "# needed as every of these features will get its own layer\n",
    "feature_names  = traces[0].columns\n",
    "trace_columns = list(map(lambda e: bool(re.match('^TARGET$', e)), feature_names))\n",
    "target_col_start_index = trace_columns.index(True)\n",
    "\n",
    "categorical_feature_names = feature_dict.keys()\n",
    "pfs_col_start_index = target_col_start_index - n_pfs_features\n",
    "sp2_col_start_index = pfs_col_start_index - n_sp2_features\n",
    "cat_col_start_index = sp2_col_start_index - len(categorical_feature_names)\n",
    "\n",
    "ordinal_feature_names = feature_names[0:cat_col_start_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: normalize\n",
    "def wrapped__create_learning_dicts_from_trace(p):\n",
    "    return create_learning_dicts_from_trace(*p)\n",
    "# reshape X to be [samples, time steps, features]\n",
    "# How to understand keras feature shape requirements: https://github.com/keras-team/keras/issues/2045\n",
    "def create_learning_dicts_from_trace(t, sp2_col_start_index, n_sp2_features, pfs_col_start_index, n_pfs_features, target_col_start_index, feature_names):\n",
    "    t_dict = {'x':[], 'y':[]}\n",
    "    # generate one input sequence for every type of variable\n",
    "    # map every single-step batch in a dictionary that will correspond to input layer names!\n",
    "    for i in range(0, len(t)):\n",
    "        batch_dict = {}\n",
    "\n",
    "        # automatically run through all ordinal and categorical features\n",
    "        for col_idx, col in enumerate(feature_names[:sp2_col_start_index]):\n",
    "            input_name = generate_input_name(col)\n",
    "            batch_dict[input_name] = np.array(t.iloc[i, col_idx], dtype=np.float32).reshape([-1,1])\n",
    "\n",
    "        # create batches for sp2 and pfs2 seperately because of their variable encodings\n",
    "        batch_dict[generate_input_name(\"sp2\")] = np.asarray(t.iloc[i, sp2_col_start_index:pfs_col_start_index], dtype=np.float32).reshape([-1,n_sp2_features])\n",
    "        batch_dict[generate_input_name(\"pfs\")] = np.asarray(t.iloc[i, pfs_col_start_index:target_col_start_index], dtype=np.float32).reshape([-1,n_pfs_features])\n",
    "\n",
    "        t_dict['x'].append(batch_dict)\n",
    "    t_dict['y'] = np_utils.to_categorical(t.iloc[:, target_col_start_index:].values.reshape([-1,1,1]))\n",
    "    return t_dict\n",
    "\n",
    "ncores = multiprocessing.cpu_count()\n",
    "ppool = multiprocessing.Pool(ncores)\n",
    "train_traces = []\n",
    "test_traces  = []\n",
    "traces_for_input_dicts = [ (t, sp2_col_start_index, n_sp2_features, pfs_col_start_index, n_pfs_features, target_col_start_index, feature_names) for t in traces ]\n",
    "\n",
    "with tqdm(total=len(traces[:sep_idx]), desc=\"Converting traces to Keras learning data\", unit=\"traces\") as pbar:\n",
    "    for i, _ in tqdm(enumerate(ppool.imap(wrapped__create_learning_dicts_from_trace, traces_for_input_dicts[:sep_idx]))):\n",
    "        pbar.update()\n",
    "        train_traces.append(_)\n",
    "        \n",
    "with tqdm(total=len(traces[sep_idx:]), desc=\"Converting traces to Keras validation data\", unit=\"traces\") as pbar:\n",
    "    for i, _ in tqdm(enumerate(ppool.imap(wrapped__create_learning_dicts_from_trace, traces_for_input_dicts[sep_idx:]))):\n",
    "        pbar.update()\n",
    "        test_traces.append(_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(train_traces, test_traces):\n",
    "    '''\n",
    "    Model providing function:\n",
    "\n",
    "    Create Keras model with double curly brackets dropped-in as needed.\n",
    "    Return value has to be a valid python dictionary with two customary keys:\n",
    "        - loss: Specify a numeric evaluation metric to be minimized\n",
    "        - status: Just use STATUS_OK and see hyperopt documentation if not feasible\n",
    "    The last one is optional, though recommended, namely:\n",
    "        - model: specify the model just created so that we can later use it again.\n",
    "    '''\n",
    "    from keras.models import Sequential, Model\n",
    "    from keras.layers import Dense, Embedding, Input, Reshape, concatenate, Flatten, Activation, LSTM\n",
    "    \n",
    "    model_inputs = []\n",
    "    models = []\n",
    "\n",
    "    # forward all ordinal features\n",
    "    for ord_var in feature_names[:cat_col_start_index]:\n",
    "        il = Input(batch_shape=(1,1), name=generate_input_name(ord_var))\n",
    "        model = Reshape(target_shape=(1,1,))(il)\n",
    "        model_inputs.append(il)\n",
    "        models.append(model)\n",
    "\n",
    "    # create embedding layers for every categorical feature\n",
    "    for cat_var in categorical_feature_names :\n",
    "        model = Sequential()\n",
    "        no_of_unique_cat  = len(feature_dict[cat_var]['to_int'])\n",
    "        embedding_size = int(min(np.ceil((no_of_unique_cat)/2), 50 ))\n",
    "        vocab  = no_of_unique_cat+1\n",
    "\n",
    "        il = Input(batch_shape=(1,1), name=generate_input_name(cat_var))    \n",
    "        model = Embedding(vocab, embedding_size)(il)\n",
    "        model = Reshape(target_shape=(1,embedding_size,))(model)\n",
    "\n",
    "        model_inputs.append(il)\n",
    "        models.append(model)\n",
    "\n",
    "    # create input and embedding for sp2/pfs2 features\n",
    "    learn_sp2 = True\n",
    "    sequence_embedding = None\n",
    "\n",
    "    # Can't embed SP2 due to dimensionality with embedding layer, be stringent and do the same for PFS features\n",
    "    # instead, mimic the embedding internal architecture and use a Dense/Linear layer\n",
    "    if learn_sp2:\n",
    "        il = Input(batch_shape=(1,n_sp2_features), name=generate_input_name(\"sp2\"))\n",
    "        model_inputs.append(il)\n",
    "        sequence_embedding = il\n",
    "        # TODO mimic embedding architecture\n",
    "        sequence_embedding = Reshape(target_shape=(n_sp2_features,))(sequence_embedding)\n",
    "    else:\n",
    "        # TODO\n",
    "        pass\n",
    "\n",
    "    # merge the outputs of the embeddings, and everything that belongs to the most recent activity executions\n",
    "    main_output = concatenate(models, axis=2)\n",
    "    main_output = LSTM(25*32, batch_input_shape=(1,), return_sequences=True, stateful=True)(main_output) # should be multiple of 32 since it trains faster due to np.float32\n",
    "    main_output = LSTM(25*32, stateful=True)(main_output) # should be multiple of 32 since it trains faster due to np.float32\n",
    "\n",
    "    # after LSTM has learned on the sequence, bring in the SP2/PFS features, like in Shibatas paper\n",
    "    main_output = concatenate([main_output, sequence_embedding], axis=1)\n",
    "    main_output = Dense(20*32, activation='relu', name='dense_join')(main_output)\n",
    "    main_output = Dense(len(feature_dict[\"concept:name\"][\"to_int\"]), activation='sigmoid', name='dense_final')(main_output)\n",
    "\n",
    "    full_model = Model(inputs=model_inputs, outputs=[main_output])\n",
    "    full_model.compile(loss='categorical_crossentropy', optimizer={{choice(['adadelta', 'adam', 'sgd'])}}, metrics=['categorical_accuracy', 'mae'])\n",
    "    \n",
    "    n_epochs = 40\n",
    "    for epoch in range(n_epochs):\n",
    "        mean_tr_acc  = []\n",
    "        mean_tr_loss = []\n",
    "\n",
    "        for t in tqdm(train_traces, desc=\"Epoch {0}/{1}\".format(epoch,n_epochs)):\n",
    "            for x,y in zip(t['x'],t['y']):\n",
    "                tr_acc, tr_loss = full_model.train_on_batch(x, y)\n",
    "                mean_tr_acc.append(tr_acc)\n",
    "                mean_tr_loss.append(tr_loss)\n",
    "            full_model.reset_states()\n",
    "\n",
    "        print('Epoch {0} -- categorical_acc = {1} -- mae loss = {2}'.format(epoch, np.mean(mean_tr_acc), np.mean(mean_tr_loss)))\n",
    "        \n",
    "        if epoch % 5 == 0:\n",
    "            full_model.save(\"complete_model_{0}_{1}.h5\".format(full_model.optimizer, type(full_model.optimizer).__name__))\n",
    "            \n",
    "    npreds = 0\n",
    "    correct_preds = 0\n",
    "    for t in test_traces[0:1]:\n",
    "        for x,y in zip(t['x'],t['y']):\n",
    "            npreds += 1\n",
    "            pred_y = full_model.predict(x)\n",
    "            correct_preds += pred_y == y\n",
    "        full_model.reset_states()\n",
    "        \n",
    "    return {'loss': -1 * correct_preds/npreds, 'status': STATUS_OK, 'model': model}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data():\n",
    "    return train_traces, test_traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/felix.wolff2/master-thesis-code/notebooks/<ipython-input-11-8ce78beebb39>'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-8ce78beebb39>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m                                           \u001b[0malgo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtpe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuggest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                                           \u001b[0mmax_evals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m                                           trials=Trials())\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/thesis/lib/python3.6/site-packages/hyperas/optim.py\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(model, data, algo, max_evals, trials, functions, rseed, notebook_name, verbose, eval_space, return_space)\u001b[0m\n\u001b[1;32m     65\u001b[0m                                      \u001b[0mfull_model_string\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m                                      \u001b[0mnotebook_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnotebook_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m                                      verbose=verbose)\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0mbest_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/thesis/lib/python3.6/site-packages/hyperas/optim.py\u001b[0m in \u001b[0;36mbase_minimizer\u001b[0;34m(model, data, functions, algo, max_evals, trials, rseed, full_model_string, notebook_name, verbose, stack)\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0mmodel_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfull_model_string\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0mmodel_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_hyperopt_model_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunctions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnotebook_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m     \u001b[0mtemp_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'./temp_model.py'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0mwrite_temp_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/thesis/lib/python3.6/site-packages/hyperas/optim.py\u001b[0m in \u001b[0;36mget_hyperopt_model_string\u001b[0;34m(model, data, functions, notebook_name, verbose, stack)\u001b[0m\n\u001b[1;32m    176\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0mcalling_script_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcalling_script_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m             \u001b[0msource\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/felix.wolff2/master-thesis-code/notebooks/<ipython-input-11-8ce78beebb39>'"
     ]
    }
   ],
   "source": [
    "best_run, best_model = optim.minimize(model=create_model,\n",
    "                                          data=data,\n",
    "                                          algo=tpe.suggest,\n",
    "                                          max_evals=5,\n",
    "                                          trials=Trials())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:thesis]",
   "language": "python",
   "name": "conda-env-thesis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
