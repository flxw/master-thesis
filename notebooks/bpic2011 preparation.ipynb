{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import multiprocessing, threading\n",
    "import math\n",
    "import scipy.stats as ss\n",
    "import time\n",
    "import pickle\n",
    "import re\n",
    "import copy\n",
    "\n",
    "from opyenxes.model.XLog import XLog\n",
    "from opyenxes.data_in.XUniversalParser import XUniversalParser\n",
    "from opyenxes.classification.XEventAttributeClassifier import XEventAttributeClassifier\n",
    "\n",
    "from prefixspan import PrefixSpan\n",
    "\n",
    "from tqdm import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### configuration\n",
    "data_path = \"../logs/bpic2011.xes\"\n",
    "traces_picklepath = data_path.replace(\".xes\", \"_raw_traces.pickled\")\n",
    "traces_tmppath = data_path.replace(\".xes\", \"_traces_tmp.pickled\")\n",
    "traces_finalpath = data_path.replace(\".xes\", \"_traces_encoded.pickled\")\n",
    "traces_dictionarypath = data_path.replace(\".xes\", \"_dictionaries.pickled\")\n",
    "target_column = \"concept:name\"\n",
    "categorical_feature_names = [\"concept:name\", \"Specialism code\", \"org:group\"]\n",
    "### configuration end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_path) as bpic_file:\n",
    "    eventlog = XUniversalParser().parse(bpic_file)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncores = multiprocessing.cpu_count()\n",
    "ntraces = len(eventlog)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract data trace-wise from XES format and enrich with BOS/EOS markers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect all attributes\n",
    "column_names = []\n",
    "\n",
    "for event in eventlog[0]:\n",
    "    for attribute in event.get_attributes():\n",
    "        column_names.append(attribute)\n",
    "        \n",
    "column_names = set(column_names) # remove duplicates\n",
    "column_names = list(column_names)\n",
    "\n",
    "def create_dataframe_from_trace(t):\n",
    "    df = pd.DataFrame(columns=column_names, index=range(0,len(t)))\n",
    "    for event_idx, event in enumerate(t):\n",
    "        event_attributes = event.get_attributes()\n",
    "        df.iloc[event_idx][\"__case_id\"] = 0\n",
    "        \n",
    "        for attribute in event_attributes:\n",
    "            df[attribute].values[event_idx] = event_attributes[attribute].get_value()\n",
    "    \n",
    "    return df\n",
    "\n",
    "ppool = multiprocessing.Pool(ncores)\n",
    "traces = []\n",
    "with tqdm(total=len(eventlog), desc=\"Converting XES traces to Pandas dataframes\", unit=\"traces\") as pbar:\n",
    "    for i, _ in tqdm(enumerate(ppool.imap(create_dataframe_from_trace, eventlog))):\n",
    "        pbar.update()\n",
    "        traces.append(_)\n",
    "        \n",
    "del eventlog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(traces, open(traces_picklepath, \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "traces = pickle.load(open(traces_picklepath, \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eliminate correlated or unimportant features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cramers_v(confusion_matrix):\n",
    "    \"\"\" calculate Cramers V statistic for categorial-categorial association.\n",
    "        uses correction from Bergsma and Wicher,\n",
    "        Journal of the Korean Statistical Society 42 (2013): 323-328\n",
    "        https://stackoverflow.com/questions/46498455/categorical-features-correlation\"\"\"\n",
    "    chi2 = ss.chi2_contingency(confusion_matrix)[0]\n",
    "    n = confusion_matrix.sum()\n",
    "    phi2 = chi2 / n\n",
    "    r, k = confusion_matrix.shape\n",
    "    phi2corr = max(0, phi2 - ((k-1)*(r-1))/(n-1))\n",
    "    rcorr = r - ((r-1)**2)/(n-1)\n",
    "    kcorr = k - ((k-1)**2)/(n-1)\n",
    "    return np.sqrt(phi2corr / min((kcorr-1), (rcorr-1)))\n",
    "\n",
    "for col_a,col_b in itertools.product(eventlog_df.columns, repeat=2): \n",
    "    candidate = pd.crosstab(eventlog_df[col_a], eventlog_df[col_b]).as_matrix()\n",
    "    print(\"{: >30} {: >30} {: >20}\".format(col_a, col_b, cramers_v(candidate)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lifecyle:transition is always \"complete\"\n",
    "# Producer code correlates perfectly with org:group\n",
    "# Activity code correlates perfectly with concept:name\n",
    "for t in traces:\n",
    "    t.drop(columns=[\"lifecycle:transition\", \"Producer code\", \"Activity code\", \"Section\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create standard feature set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set data types for columns that were not autodetected\n",
    "for i in range(0,len(traces)):\n",
    "    traces[i][\"Specialism code\"] = pd.to_numeric(traces[i][\"Specialism code\"], errors=\"ignore\")\n",
    "    traces[i][\"Number of executions\"] = pd.to_numeric(traces[i][\"Number of executions\"], errors=\"ignore\")\n",
    "\n",
    "eventlog_df = pd.concat(traces, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create SP2 feature set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/42636765/how-to-set-all-the-values-of-an-existing-pandas-dataframe-to-zero\n",
    "# This one-hot encodes all entries in concept:name column for later incrementation once it has been seen\n",
    "# sp2_features = pd.get_dummies(eventlog_df[\"concept:name\"], prefix=\"SP2\") # can't use windowed representation here as it might skew distribution of values\n",
    "# eventlog_sp2_df = process_results.copy(deep=True)\n",
    "# sp2_features    = sp2_features.drop(sp2_features.index[sp2_features.index[len(eventlog_sp2_df):]])\n",
    "# assert(len(sp2_features) == len(eventlog_sp2_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Enriching traces with SP2 features:   0%|          | 0/1143 [00:00<?, ?traces/s]\n",
      "Enriching traces with SP2 features:   0%|          | 1/1143 [00:00<03:13,  5.90traces/s]\n",
      "Enriching traces with SP2 features:   1%|▏         | 15/1143 [00:00<02:16,  8.26traces/s]\n",
      "Enriching traces with SP2 features:   2%|▏         | 28/1143 [00:00<01:38, 11.37traces/s]\n",
      "Enriching traces with SP2 features:   6%|▋         | 73/1143 [00:00<01:07, 15.91traces/s]\n",
      "Enriching traces with SP2 features:  10%|█         | 117/1143 [00:00<00:46, 22.09traces/s]\n",
      "Enriching traces with SP2 features:  16%|█▌        | 179/1143 [00:00<00:31, 31.08traces/s]\n",
      "Enriching traces with SP2 features:  18%|█▊        | 208/1143 [00:01<00:22, 41.63traces/s]\n",
      "Enriching traces with SP2 features:  21%|██        | 235/1143 [00:01<00:16, 55.75traces/s]\n",
      "Enriching traces with SP2 features:  23%|██▎       | 262/1143 [00:01<00:13, 63.86traces/s]\n",
      "Enriching traces with SP2 features:  25%|██▍       | 284/1143 [00:01<00:11, 71.66traces/s]\n",
      "Enriching traces with SP2 features:  27%|██▋       | 303/1143 [00:01<00:10, 83.72traces/s]\n",
      "Enriching traces with SP2 features:  31%|███       | 357/1143 [00:01<00:07, 112.13traces/s]\n",
      "Enriching traces with SP2 features:  34%|███▎      | 385/1143 [00:02<00:05, 132.08traces/s]\n",
      "Enriching traces with SP2 features:  37%|███▋      | 418/1143 [00:02<00:04, 160.93traces/s]\n",
      "Enriching traces with SP2 features:  39%|███▉      | 447/1143 [00:02<00:03, 183.51traces/s]\n",
      "Enriching traces with SP2 features:  42%|████▏     | 475/1143 [00:02<00:03, 179.50traces/s]\n",
      "Enriching traces with SP2 features:  44%|████▎     | 500/1143 [00:02<00:03, 189.00traces/s]\n",
      "Enriching traces with SP2 features:  47%|████▋     | 537/1143 [00:02<00:02, 211.39traces/s]\n",
      "Enriching traces with SP2 features:  50%|████▉     | 570/1143 [00:02<00:02, 234.99traces/s]\n",
      "570it [00:02, 233.79it/s]\u001b[A\n",
      "Enriching traces with SP2 features:  55%|█████▍    | 625/1143 [00:02<00:02, 231.06traces/s]\n",
      "Enriching traces with SP2 features:  57%|█████▋    | 650/1143 [00:03<00:02, 234.49traces/s]\n",
      "Enriching traces with SP2 features:  59%|█████▉    | 679/1143 [00:03<00:02, 223.81traces/s]\n",
      "Enriching traces with SP2 features:  63%|██████▎   | 723/1143 [00:03<00:01, 244.64traces/s]\n",
      "Enriching traces with SP2 features:  66%|██████▌   | 749/1143 [00:03<00:01, 217.42traces/s]\n",
      "Enriching traces with SP2 features:  70%|██████▉   | 797/1143 [00:03<00:01, 259.22traces/s]\n",
      "801it [00:03, 233.34it/s]\u001b[A\n",
      "Enriching traces with SP2 features:  78%|███████▊  | 895/1143 [00:04<00:01, 218.55traces/s]\n",
      "Enriching traces with SP2 features:  82%|████████▏ | 933/1143 [00:04<00:00, 234.09traces/s]\n",
      "933it [00:04, 236.61it/s]\u001b[A\n",
      "Enriching traces with SP2 features:  85%|████████▍ | 966/1143 [00:04<00:00, 228.97traces/s]\n",
      "Enriching traces with SP2 features:  87%|████████▋ | 999/1143 [00:04<00:00, 248.46traces/s]\n",
      "Enriching traces with SP2 features:  90%|█████████ | 1029/1143 [00:04<00:00, 240.25traces/s]\n",
      "Enriching traces with SP2 features:  93%|█████████▎| 1060/1143 [00:04<00:00, 254.21traces/s]\n",
      "Enriching traces with SP2 features:  95%|█████████▌| 1089/1143 [00:04<00:00, 221.41traces/s]\n",
      "Enriching traces with SP2 features: 100%|██████████| 1143/1143 [00:04<00:00, 231.24traces/s]\n"
     ]
    }
   ],
   "source": [
    "# loop through every trace and encode the presence of an activity\n",
    "sp2_prefix = \"SP2_\"\n",
    "activity_labels = [ \"{0}{1}\".format(sp2_prefix,a) for a in eventlog_df[\"concept:name\"].unique() ]\n",
    "\n",
    "def enrich_trace_with_sp2(t):\n",
    "    sp2_df = pd.DataFrame(columns=activity_labels, index=range(0,len(t)), dtype=np.bool)\n",
    "    for col in sp2_df.columns: sp2_df[col].values[:] = 0\n",
    "    sp2_df[\"{0}{1}\".format(sp2_prefix, t[\"concept:name\"][0])].values[0]  = 1\n",
    "    \n",
    "    for i in range(1,len(t)):\n",
    "        first_activity_name = t[\"concept:name\"].iloc[i]\n",
    "        col = \"{0}{1}\".format(sp2_prefix,first_activity_name)\n",
    "        \n",
    "        sp2_df.values[i] = sp2_df.values[i-1]\n",
    "        sp2_df[col].values[i] = 1\n",
    "        \n",
    "    return sp2_df\n",
    "\n",
    "ppool = multiprocessing.Pool(ncores)\n",
    "sp2_traces = []\n",
    "with tqdm(total=len(traces), desc=\"Enriching traces with SP2 features\", unit=\"traces\") as pbar:\n",
    "    for i, _ in tqdm(enumerate(ppool.imap(enrich_trace_with_sp2, traces))):\n",
    "        pbar.update()\n",
    "        sp2_traces.append(_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enrich with PrefixSpan features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_patterns(pt):\n",
    "    for p in pt:\n",
    "        print(\"Support: {0}%\".format(100*p[0]/len(traces)))\n",
    "        for n in p[1]:\n",
    "            print(\"    > \", int_to_event[n])\n",
    "        print()\n",
    "\n",
    "# since most patterns begin and end with the <eos> and <bos> markers, the features only become valuable towards the end...\n",
    "events       = eventlog_df[\"concept:name\"].unique()\n",
    "event_to_int = dict((c, i) for i,c in enumerate(events) if c not in [\"<bos>\",\"<eos>\"])\n",
    "int_to_event = dict((i, c) for i,c in enumerate(events) if c not in [\"<bos>\",\"<eos>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prefixspan requires an array of arrays with one subarray for every trace\n",
    "encoded_traces = [ t[\"concept:name\"].map(event_to_int).tolist() for t in traces ]\n",
    "prefixspan_traces = PrefixSpan(encoded_traces)\n",
    "closed_sequences = prefixspan_traces.topk(25, closed=True) # support is how often the subsequence appears in total\n",
    "# http://sequenceanalysis.github.io/slides/analyzing_sequential_user_behavior_part2.pdf, slide 5\n",
    "# print_patterns(ps_topkc)\n",
    "\n",
    "# only take subsequence which are at a certain level of support? like if ss[0]/len(traces) < .90\n",
    "#ps_topkc = list(filter(lambda x: x[0]/len(traces) > .90, ps_topkc))\n",
    "closed_sequences = [ p[1] for p in closed_sequences ]\n",
    "pftrace_args = [ (t, closed_sequences[:], event_to_int) for t in traces ] # enrich traces with copy of mined subsequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Enriching traces with mined subsequence features:   0%|          | 0/1143 [00:00<?, ?traces/s]\n",
      "Enriching traces with mined subsequence features:   1%|          | 13/1143 [00:00<00:13, 85.92traces/s]\n",
      "Enriching traces with mined subsequence features:   5%|▍         | 55/1143 [00:00<00:09, 112.50traces/s]\n",
      "Enriching traces with mined subsequence features:   7%|▋         | 78/1143 [00:00<00:08, 129.00traces/s]\n",
      "Enriching traces with mined subsequence features:  10%|█         | 117/1143 [00:00<00:06, 155.07traces/s]\n",
      "Enriching traces with mined subsequence features:  16%|█▌        | 182/1143 [00:00<00:04, 200.68traces/s]\n",
      "Enriching traces with mined subsequence features:  19%|█▉        | 219/1143 [00:00<00:04, 230.16traces/s]\n",
      "Enriching traces with mined subsequence features:  23%|██▎       | 266/1143 [00:00<00:03, 271.15traces/s]\n",
      "Enriching traces with mined subsequence features:  28%|██▊       | 318/1143 [00:00<00:02, 308.59traces/s]\n",
      "Enriching traces with mined subsequence features:  31%|███▏      | 358/1143 [00:01<00:02, 310.29traces/s]\n",
      "Enriching traces with mined subsequence features:  37%|███▋      | 418/1143 [00:01<00:02, 357.93traces/s]\n",
      "Enriching traces with mined subsequence features:  41%|████      | 469/1143 [00:01<00:01, 346.97traces/s]\n",
      "Enriching traces with mined subsequence features:  45%|████▌     | 519/1143 [00:01<00:01, 377.08traces/s]\n",
      "519it [00:01, 377.38it/s]\u001b[A\n",
      "Enriching traces with mined subsequence features:  49%|████▉     | 561/1143 [00:01<00:02, 276.53traces/s]\n",
      "Enriching traces with mined subsequence features:  54%|█████▍    | 621/1143 [00:01<00:01, 328.00traces/s]\n",
      "Enriching traces with mined subsequence features:  58%|█████▊    | 663/1143 [00:01<00:01, 344.69traces/s]\n",
      "Enriching traces with mined subsequence features:  62%|██████▏   | 704/1143 [00:01<00:01, 356.91traces/s]\n",
      "Enriching traces with mined subsequence features:  65%|██████▌   | 745/1143 [00:02<00:01, 316.17traces/s]\n",
      "Enriching traces with mined subsequence features:  70%|███████   | 801/1143 [00:02<00:01, 332.94traces/s]\n",
      "801it [00:02, 339.60it/s]\u001b[A\n",
      "Enriching traces with mined subsequence features:  73%|███████▎  | 838/1143 [00:02<00:01, 272.04traces/s]\n",
      "Enriching traces with mined subsequence features:  80%|███████▉  | 912/1143 [00:02<00:00, 335.16traces/s]\n",
      "Enriching traces with mined subsequence features:  84%|████████▎ | 956/1143 [00:02<00:00, 312.57traces/s]\n",
      "Enriching traces with mined subsequence features:  87%|████████▋ | 995/1143 [00:02<00:00, 330.79traces/s]\n",
      "Enriching traces with mined subsequence features:  90%|█████████ | 1034/1143 [00:02<00:00, 343.30traces/s]\n",
      "Enriching traces with mined subsequence features:  94%|█████████▍| 1073/1143 [00:03<00:00, 323.52traces/s]\n",
      "1093it [00:03, 358.57it/s]\u001b[A\n",
      "Enriching traces with mined subsequence features: 100%|██████████| 1143/1143 [00:03<00:00, 357.04traces/s]\n"
     ]
    }
   ],
   "source": [
    "def wrapped__enrich_trace_with_subseq(args):\n",
    "    return enrich_trace_with_subseq(*args)\n",
    "\n",
    "def enrich_trace_with_subseq(t, ps, event_to_int):\n",
    "    col_prefix = \"PFS_\"\n",
    "    subseq_labels = [ \"{0}{1}\".format(col_prefix,ss_idx) for ss_idx, ss in enumerate(ps) ]\n",
    "    subseq_df = pd.DataFrame(columns=subseq_labels, index=range(0,len(t)), dtype=np.bool)\n",
    "    \n",
    "    for col in subseq_df.columns: subseq_df[col].values[:] = 0\n",
    "    for i in range(0,len(t)): # loop through sequence, prune items from mined sequences, and once a subsequence array is empty, this subsequence has occured :)\n",
    "        activity_code = event_to_int.get(t[\"concept:name\"].iloc[i], None)\n",
    "        \n",
    "        for subseq_idx in range(0,len(ps)):\n",
    "            if ps[subseq_idx] == []:\n",
    "                continue\n",
    "            if ps[subseq_idx][0] == activity_code:\n",
    "                ps[subseq_idx].pop(0)\n",
    "                if ps[subseq_idx] == []:\n",
    "                    subseq_df.values[i:,subseq_idx] = 1\n",
    "        \n",
    "    return subseq_df\n",
    "\n",
    "ppool = multiprocessing.Pool(ncores)\n",
    "pf_traces = []\n",
    "\n",
    "with tqdm(total=len(pftrace_args), desc=\"Enriching traces with mined subsequence features\", unit=\"traces\") as pbar:\n",
    "    for i, _ in tqdm(enumerate(ppool.imap(wrapped__enrich_trace_with_subseq, pftrace_args))):\n",
    "        pbar.update()\n",
    "        pf_traces.append(_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dictionary encoding for categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert timestamp to total running time\n",
    "bos_idx = 0\n",
    "for i in range(0, len(traces)):\n",
    "    tlen = len(traces[i])-1\n",
    "    dfs = traces[i][\"time:timestamp\"] - traces[i][\"time:timestamp\"][bos_idx]\n",
    "    traces[i][\"time:timestamp\"] = dfs.map(lambda d: int(d.total_seconds()/(60*60))) # convert to hours\n",
    "    \n",
    "# Create dictionaries here\n",
    "feature_dict = {}\n",
    "for cf in categorical_feature_names:\n",
    "    cf_dict = { 'to_int': {}, 'to_cat': {} }\n",
    "    events = eventlog_df[cf].unique().tolist()\n",
    "    if cf == target_column: events.append(\"<EOS>\")\n",
    "    cf_dict['to_int'] = dict((c, i) for i, c in enumerate(events))\n",
    "    cf_dict['to_cat'] = dict((i, c) for i, c in enumerate(events))\n",
    "    feature_dict[cf] = cf_dict\n",
    "    \n",
    "# Concatenate all features into one feature dataframe per trace\n",
    "for i in range(0,len(traces)):\n",
    "    targets = traces[i][target_column].shift(-1).to_frame(\"TARGET\")\n",
    "    targets.values[len(traces[i])-1] = \"<EOS>\"\n",
    "    targets[\"TARGET\"] = targets[\"TARGET\"].map(feature_dict[target_column]['to_int'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataframe creation and saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinal_feature_names = traces[0].columns.difference(categorical_feature_names)\n",
    "final_traces = copy.deepcopy(traces)\n",
    "\n",
    "# Concatenate all features into one feature dataframe per trace\n",
    "for i in range(0,len(traces)):\n",
    "    # normalize values while encoding\n",
    "    for cf in categorical_feature_names:\n",
    "        final_traces[i][cf] = traces[i][cf].map(feature_dict[cf]['to_int'])\n",
    "        #final_traces[i][cf] /= max(feature_dict[cf]['to_int'].values())\n",
    "    \n",
    "    ordinal_trace = traces[i][ordinal_feature_names]\n",
    "    categorical_trace = traces[i][categorical_feature_names]\n",
    "    final_traces[i] = pd.concat([ordinal_trace, categorical_trace, sp2_traces[i], pf_traces[i], targets], ignore_index=False, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(sum([len(t) for t in final_traces]) == len(eventlog_df))\n",
    "pickle.dump(final_traces, open(traces_finalpath, \"wb\"), protocol=pickle.HIGHEST_PROTOCOL)\n",
    "pickle.dump(feature_dict, open(traces_dictionarypath, \"wb\"), protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_traces = pickle.load(open(traces_finalpath, \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "000000001 occurs 0000000004x\n",
      "000000002 occurs 0000000059x\n",
      "000000003 occurs 0000000049x\n",
      "000000004 occurs 0000000046x\n",
      "000000005 occurs 0000000043x\n",
      "000000006 occurs 0000000032x\n",
      "000000007 occurs 0000000034x\n",
      "000000008 occurs 0000000021x\n",
      "000000009 occurs 0000000022x\n",
      "000000010 occurs 0000000015x\n",
      "000000011 occurs 0000000016x\n",
      "000000012 occurs 0000000009x\n",
      "000000013 occurs 0000000009x\n",
      "000000014 occurs 0000000011x\n",
      "000000015 occurs 0000000009x\n",
      "000000016 occurs 0000000002x\n",
      "000000017 occurs 0000000003x\n",
      "000000018 occurs 0000000005x\n",
      "000000019 occurs 0000000008x\n",
      "000000020 occurs 0000000005x\n",
      "000000021 occurs 0000000004x\n",
      "000000022 occurs 0000000003x\n",
      "000000023 occurs 0000000003x\n",
      "000000024 occurs 0000000004x\n",
      "000000025 occurs 0000000002x\n",
      "000000026 occurs 0000000006x\n",
      "000000027 occurs 0000000006x\n",
      "000000028 occurs 0000000007x\n",
      "000000029 occurs 0000000007x\n",
      "000000030 occurs 0000000012x\n",
      "000000031 occurs 0000000009x\n",
      "000000032 occurs 0000000011x\n",
      "000000033 occurs 0000000005x\n",
      "000000034 occurs 0000000007x\n",
      "000000035 occurs 0000000006x\n",
      "000000036 occurs 0000000008x\n",
      "000000037 occurs 0000000004x\n",
      "000000038 occurs 0000000004x\n",
      "000000039 occurs 0000000005x\n",
      "000000040 occurs 0000000004x\n",
      "000000041 occurs 0000000002x\n",
      "000000042 occurs 0000000005x\n",
      "000000043 occurs 0000000003x\n",
      "000000044 occurs 0000000006x\n",
      "000000045 occurs 0000000002x\n",
      "000000046 occurs 0000000004x\n",
      "000000047 occurs 0000000003x\n",
      "000000048 occurs 0000000002x\n",
      "000000049 occurs 0000000004x\n",
      "000000050 occurs 0000000005x\n",
      "000000051 occurs 0000000003x\n",
      "000000052 occurs 0000000005x\n",
      "000000053 occurs 0000000004x\n",
      "000000054 occurs 0000000004x\n",
      "000000055 occurs 0000000003x\n",
      "000000056 occurs 0000000003x\n",
      "000000057 occurs 0000000002x\n",
      "000000058 occurs 0000000003x\n",
      "000000059 occurs 0000000004x\n",
      "000000060 occurs 0000000001x\n",
      "000000062 occurs 0000000003x\n",
      "000000063 occurs 0000000006x\n",
      "000000064 occurs 0000000001x\n",
      "000000065 occurs 0000000002x\n",
      "000000066 occurs 0000000003x\n",
      "000000067 occurs 0000000002x\n",
      "000000068 occurs 0000000002x\n",
      "000000069 occurs 0000000005x\n",
      "000000070 occurs 0000000004x\n",
      "000000071 occurs 0000000004x\n",
      "000000072 occurs 0000000001x\n",
      "000000073 occurs 0000000005x\n",
      "000000074 occurs 0000000002x\n",
      "000000075 occurs 0000000007x\n",
      "000000076 occurs 0000000004x\n",
      "000000077 occurs 0000000001x\n",
      "000000078 occurs 0000000004x\n",
      "000000079 occurs 0000000002x\n",
      "000000080 occurs 0000000004x\n",
      "000000081 occurs 0000000003x\n",
      "000000082 occurs 0000000003x\n",
      "000000083 occurs 0000000001x\n",
      "000000084 occurs 0000000001x\n",
      "000000085 occurs 0000000005x\n",
      "000000086 occurs 0000000001x\n",
      "000000087 occurs 0000000002x\n",
      "000000088 occurs 0000000005x\n",
      "000000089 occurs 0000000001x\n",
      "000000090 occurs 0000000001x\n",
      "000000091 occurs 0000000003x\n",
      "000000092 occurs 0000000003x\n",
      "000000093 occurs 0000000002x\n",
      "000000094 occurs 0000000001x\n",
      "000000095 occurs 0000000004x\n",
      "000000096 occurs 0000000003x\n",
      "000000097 occurs 0000000001x\n",
      "000000098 occurs 0000000003x\n",
      "000000099 occurs 0000000001x\n",
      "000000100 occurs 0000000006x\n",
      "000000101 occurs 0000000002x\n",
      "000000102 occurs 0000000004x\n",
      "000000103 occurs 0000000001x\n",
      "000000104 occurs 0000000001x\n",
      "000000105 occurs 0000000006x\n",
      "000000106 occurs 0000000004x\n",
      "000000108 occurs 0000000005x\n",
      "000000109 occurs 0000000004x\n",
      "000000110 occurs 0000000005x\n",
      "000000111 occurs 0000000002x\n",
      "000000113 occurs 0000000002x\n",
      "000000114 occurs 0000000005x\n",
      "000000115 occurs 0000000004x\n",
      "000000116 occurs 0000000003x\n",
      "000000117 occurs 0000000005x\n",
      "000000118 occurs 0000000002x\n",
      "000000119 occurs 0000000001x\n",
      "000000120 occurs 0000000001x\n",
      "000000121 occurs 0000000001x\n",
      "000000122 occurs 0000000005x\n",
      "000000123 occurs 0000000003x\n",
      "000000124 occurs 0000000003x\n",
      "000000125 occurs 0000000001x\n",
      "000000126 occurs 0000000002x\n",
      "000000127 occurs 0000000003x\n",
      "000000128 occurs 0000000003x\n",
      "000000129 occurs 0000000001x\n",
      "000000130 occurs 0000000004x\n",
      "000000131 occurs 0000000002x\n",
      "000000132 occurs 0000000006x\n",
      "000000133 occurs 0000000002x\n",
      "000000134 occurs 0000000001x\n",
      "000000135 occurs 0000000003x\n",
      "000000136 occurs 0000000003x\n",
      "000000137 occurs 0000000002x\n",
      "000000138 occurs 0000000003x\n",
      "000000139 occurs 0000000002x\n",
      "000000140 occurs 0000000002x\n",
      "000000141 occurs 0000000004x\n",
      "000000142 occurs 0000000003x\n",
      "000000144 occurs 0000000001x\n",
      "000000146 occurs 0000000001x\n",
      "000000147 occurs 0000000004x\n",
      "000000148 occurs 0000000002x\n",
      "000000149 occurs 0000000001x\n",
      "000000150 occurs 0000000002x\n",
      "000000151 occurs 0000000006x\n",
      "000000152 occurs 0000000001x\n",
      "000000154 occurs 0000000001x\n",
      "000000155 occurs 0000000001x\n",
      "000000156 occurs 0000000004x\n",
      "000000158 occurs 0000000002x\n",
      "000000159 occurs 0000000002x\n",
      "000000160 occurs 0000000001x\n",
      "000000162 occurs 0000000001x\n",
      "000000163 occurs 0000000003x\n",
      "000000165 occurs 0000000002x\n",
      "000000166 occurs 0000000002x\n",
      "000000167 occurs 0000000001x\n",
      "000000169 occurs 0000000007x\n",
      "000000170 occurs 0000000002x\n",
      "000000171 occurs 0000000002x\n",
      "000000172 occurs 0000000003x\n",
      "000000173 occurs 0000000004x\n",
      "000000174 occurs 0000000002x\n",
      "000000176 occurs 0000000003x\n",
      "000000178 occurs 0000000002x\n",
      "000000179 occurs 0000000003x\n",
      "000000180 occurs 0000000004x\n",
      "000000182 occurs 0000000001x\n",
      "000000185 occurs 0000000003x\n",
      "000000186 occurs 0000000002x\n",
      "000000187 occurs 0000000001x\n",
      "000000188 occurs 0000000002x\n",
      "000000189 occurs 0000000002x\n",
      "000000190 occurs 0000000002x\n",
      "000000191 occurs 0000000001x\n",
      "000000192 occurs 0000000002x\n",
      "000000193 occurs 0000000003x\n",
      "000000194 occurs 0000000002x\n",
      "000000195 occurs 0000000001x\n",
      "000000196 occurs 0000000002x\n",
      "000000197 occurs 0000000001x\n",
      "000000198 occurs 0000000002x\n",
      "000000200 occurs 0000000002x\n",
      "000000201 occurs 0000000002x\n",
      "000000202 occurs 0000000001x\n",
      "000000203 occurs 0000000002x\n",
      "000000204 occurs 0000000001x\n",
      "000000205 occurs 0000000002x\n",
      "000000207 occurs 0000000001x\n",
      "000000210 occurs 0000000002x\n",
      "000000212 occurs 0000000002x\n",
      "000000213 occurs 0000000001x\n",
      "000000215 occurs 0000000003x\n",
      "000000216 occurs 0000000002x\n",
      "000000217 occurs 0000000001x\n",
      "000000219 occurs 0000000003x\n",
      "000000221 occurs 0000000001x\n",
      "000000222 occurs 0000000001x\n",
      "000000223 occurs 0000000002x\n",
      "000000225 occurs 0000000001x\n",
      "000000226 occurs 0000000001x\n",
      "000000229 occurs 0000000002x\n",
      "000000233 occurs 0000000001x\n",
      "000000234 occurs 0000000002x\n",
      "000000238 occurs 0000000001x\n",
      "000000239 occurs 0000000003x\n",
      "000000241 occurs 0000000001x\n",
      "000000243 occurs 0000000001x\n",
      "000000246 occurs 0000000004x\n",
      "000000248 occurs 0000000001x\n",
      "000000249 occurs 0000000001x\n",
      "000000251 occurs 0000000001x\n",
      "000000252 occurs 0000000001x\n",
      "000000253 occurs 0000000001x\n",
      "000000256 occurs 0000000001x\n",
      "000000259 occurs 0000000001x\n",
      "000000260 occurs 0000000002x\n",
      "000000261 occurs 0000000001x\n",
      "000000263 occurs 0000000001x\n",
      "000000264 occurs 0000000001x\n",
      "000000266 occurs 0000000001x\n",
      "000000267 occurs 0000000001x\n",
      "000000268 occurs 0000000001x\n",
      "000000269 occurs 0000000001x\n",
      "000000270 occurs 0000000001x\n",
      "000000273 occurs 0000000001x\n",
      "000000274 occurs 0000000001x\n",
      "000000279 occurs 0000000001x\n",
      "000000280 occurs 0000000001x\n",
      "000000282 occurs 0000000002x\n",
      "000000283 occurs 0000000003x\n",
      "000000285 occurs 0000000001x\n",
      "000000286 occurs 0000000002x\n",
      "000000287 occurs 0000000002x\n",
      "000000289 occurs 0000000003x\n",
      "000000290 occurs 0000000001x\n",
      "000000291 occurs 0000000002x\n",
      "000000293 occurs 0000000001x\n",
      "000000294 occurs 0000000003x\n",
      "000000295 occurs 0000000003x\n",
      "000000301 occurs 0000000001x\n",
      "000000303 occurs 0000000002x\n",
      "000000304 occurs 0000000001x\n",
      "000000307 occurs 0000000003x\n",
      "000000309 occurs 0000000002x\n",
      "000000315 occurs 0000000001x\n",
      "000000319 occurs 0000000003x\n",
      "000000321 occurs 0000000001x\n",
      "000000322 occurs 0000000001x\n",
      "000000323 occurs 0000000001x\n",
      "000000324 occurs 0000000001x\n",
      "000000327 occurs 0000000001x\n",
      "000000328 occurs 0000000002x\n",
      "000000329 occurs 0000000001x\n",
      "000000331 occurs 0000000002x\n",
      "000000333 occurs 0000000002x\n",
      "000000334 occurs 0000000001x\n",
      "000000339 occurs 0000000001x\n",
      "000000340 occurs 0000000001x\n",
      "000000341 occurs 0000000001x\n",
      "000000343 occurs 0000000001x\n",
      "000000349 occurs 0000000001x\n",
      "000000350 occurs 0000000001x\n",
      "000000351 occurs 0000000001x\n",
      "000000354 occurs 0000000002x\n",
      "000000355 occurs 0000000001x\n",
      "000000356 occurs 0000000001x\n",
      "000000357 occurs 0000000001x\n",
      "000000358 occurs 0000000002x\n",
      "000000359 occurs 0000000001x\n",
      "000000360 occurs 0000000001x\n",
      "000000365 occurs 0000000001x\n",
      "000000366 occurs 0000000002x\n",
      "000000370 occurs 0000000001x\n",
      "000000376 occurs 0000000001x\n",
      "000000378 occurs 0000000001x\n",
      "000000385 occurs 0000000001x\n",
      "000000391 occurs 0000000001x\n",
      "000000394 occurs 0000000001x\n",
      "000000399 occurs 0000000001x\n",
      "000000400 occurs 0000000002x\n",
      "000000407 occurs 0000000002x\n",
      "000000409 occurs 0000000002x\n",
      "000000413 occurs 0000000001x\n",
      "000000417 occurs 0000000001x\n",
      "000000420 occurs 0000000001x\n",
      "000000426 occurs 0000000002x\n",
      "000000428 occurs 0000000001x\n",
      "000000435 occurs 0000000001x\n",
      "000000438 occurs 0000000001x\n",
      "000000440 occurs 0000000003x\n",
      "000000456 occurs 0000000002x\n",
      "000000458 occurs 0000000001x\n",
      "000000467 occurs 0000000002x\n",
      "000000468 occurs 0000000001x\n",
      "000000469 occurs 0000000001x\n",
      "000000471 occurs 0000000001x\n",
      "000000477 occurs 0000000001x\n",
      "000000485 occurs 0000000001x\n",
      "000000487 occurs 0000000001x\n",
      "000000488 occurs 0000000001x\n",
      "000000489 occurs 0000000001x\n",
      "000000502 occurs 0000000001x\n",
      "000000511 occurs 0000000001x\n",
      "000000515 occurs 0000000001x\n",
      "000000526 occurs 0000000001x\n",
      "000000527 occurs 0000000001x\n",
      "000000533 occurs 0000000001x\n",
      "000000537 occurs 0000000001x\n",
      "000000542 occurs 0000000001x\n",
      "000000550 occurs 0000000001x\n",
      "000000553 occurs 0000000001x\n",
      "000000563 occurs 0000000001x\n",
      "000000564 occurs 0000000001x\n",
      "000000565 occurs 0000000001x\n",
      "000000567 occurs 0000000001x\n",
      "000000572 occurs 0000000001x\n",
      "000000580 occurs 0000000001x\n",
      "000000587 occurs 0000000001x\n",
      "000000597 occurs 0000000001x\n",
      "000000611 occurs 0000000001x\n",
      "000000621 occurs 0000000001x\n",
      "000000624 occurs 0000000001x\n",
      "000000627 occurs 0000000001x\n",
      "000000644 occurs 0000000001x\n",
      "000000648 occurs 0000000001x\n",
      "000000652 occurs 0000000001x\n",
      "000000691 occurs 0000000001x\n",
      "000000700 occurs 0000000001x\n",
      "000000704 occurs 0000000001x\n",
      "000000732 occurs 0000000001x\n",
      "000000750 occurs 0000000001x\n",
      "000000760 occurs 0000000001x\n",
      "000000765 occurs 0000000001x\n",
      "000000784 occurs 0000000001x\n",
      "000000785 occurs 0000000001x\n",
      "000000791 occurs 0000000001x\n",
      "000000810 occurs 0000000001x\n",
      "000000811 occurs 0000000001x\n",
      "000000824 occurs 0000000001x\n",
      "000000829 occurs 0000000001x\n",
      "000000840 occurs 0000000001x\n",
      "000000850 occurs 0000000001x\n",
      "000000859 occurs 0000000001x\n",
      "000000866 occurs 0000000001x\n",
      "000000902 occurs 0000000001x\n",
      "000000925 occurs 0000000001x\n",
      "000000955 occurs 0000000001x\n",
      "000000964 occurs 0000000001x\n",
      "000000990 occurs 0000000001x\n",
      "000001003 occurs 0000000001x\n",
      "000001052 occurs 0000000001x\n",
      "000001112 occurs 0000000001x\n",
      "000001183 occurs 0000000001x\n",
      "000001191 occurs 0000000001x\n",
      "000001200 occurs 0000000001x\n",
      "000001350 occurs 0000000001x\n",
      "000001368 occurs 0000000001x\n",
      "000001432 occurs 0000000001x\n",
      "000001690 occurs 0000000001x\n",
      "000001814 occurs 0000000001x\n"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "tlens = [len(t) for t in final_traces]\n",
    "len_occurences = np.bincount(tlens)\n",
    "ii = np.nonzero(len_occurences)[0]\n",
    "\n",
    "for ln, nocc in zip(ii,len_occurences[ii]):\n",
    "    print(\"{0:0>9d} occurs {1:0>10d}x\".format(ln,nocc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   1,    2,    3,    4,    5,    6,    7,    8,    9,   10,   11,\n",
       "         12,   13,   14,   15,   16,   17,   18,   19,   20,   21,   22,\n",
       "         23,   24,   25,   26,   27,   28,   29,   30,   31,   32,   33,\n",
       "         34,   35,   36,   37,   38,   39,   40,   41,   42,   43,   44,\n",
       "         45,   46,   47,   48,   49,   50,   51,   52,   53,   54,   55,\n",
       "         56,   57,   58,   59,   60,   62,   63,   64,   65,   66,   67,\n",
       "         68,   69,   70,   71,   72,   73,   74,   75,   76,   77,   78,\n",
       "         79,   80,   81,   82,   83,   84,   85,   86,   87,   88,   89,\n",
       "         90,   91,   92,   93,   94,   95,   96,   97,   98,   99,  100,\n",
       "        101,  102,  103,  104,  105,  106,  108,  109,  110,  111,  113,\n",
       "        114,  115,  116,  117,  118,  119,  120,  121,  122,  123,  124,\n",
       "        125,  126,  127,  128,  129,  130,  131,  132,  133,  134,  135,\n",
       "        136,  137,  138,  139,  140,  141,  142,  144,  146,  147,  148,\n",
       "        149,  150,  151,  152,  154,  155,  156,  158,  159,  160,  162,\n",
       "        163,  165,  166,  167,  169,  170,  171,  172,  173,  174,  176,\n",
       "        178,  179,  180,  182,  185,  186,  187,  188,  189,  190,  191,\n",
       "        192,  193,  194,  195,  196,  197,  198,  200,  201,  202,  203,\n",
       "        204,  205,  207,  210,  212,  213,  215,  216,  217,  219,  221,\n",
       "        222,  223,  225,  226,  229,  233,  234,  238,  239,  241,  243,\n",
       "        246,  248,  249,  251,  252,  253,  256,  259,  260,  261,  263,\n",
       "        264,  266,  267,  268,  269,  270,  273,  274,  279,  280,  282,\n",
       "        283,  285,  286,  287,  289,  290,  291,  293,  294,  295,  301,\n",
       "        303,  304,  307,  309,  315,  319,  321,  322,  323,  324,  327,\n",
       "        328,  329,  331,  333,  334,  339,  340,  341,  343,  349,  350,\n",
       "        351,  354,  355,  356,  357,  358,  359,  360,  365,  366,  370,\n",
       "        376,  378,  385,  391,  394,  399,  400,  407,  409,  413,  417,\n",
       "        420,  426,  428,  435,  438,  440,  456,  458,  467,  468,  469,\n",
       "        471,  477,  485,  487,  488,  489,  502,  511,  515,  526,  527,\n",
       "        533,  537,  542,  550,  553,  563,  564,  565,  567,  572,  580,\n",
       "        587,  597,  611,  621,  624,  627,  644,  648,  652,  691,  700,\n",
       "        704,  732,  750,  760,  765,  784,  785,  791,  810,  811,  824,\n",
       "        829,  840,  850,  859,  866,  902,  925,  955,  964,  990, 1003,\n",
       "       1052, 1112, 1183, 1191, 1200, 1350, 1368, 1432, 1690, 1814])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len_occurences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "used_bytes = sum([t.memory_usage(index=True, deep=True).sum() for t in traces])\n",
    "print(\"Memory usage of encoded data: {0} KB\".format(used_bytes / 1024))\n",
    "print(\"Memory usage of encoded data: {0} MB\".format(used_bytes / 1024**2))\n",
    "print(\"Memory usage of encoded data: {0} GB\".format(used_bytes / 1024**3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_columns = traces[0].columns.tolist()\n",
    "trace_columns = list(map(lambda e: bool(re.match('^TARGET$', e)), trace_columns))\n",
    "trace_columns.index(True) # when do the target columns start?!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:thesis]",
   "language": "python",
   "name": "conda-env-thesis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
