{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import multiprocessing, threading\n",
    "import math\n",
    "import scipy.stats as ss\n",
    "\n",
    "from opyenxes.model.XLog import XLog\n",
    "from opyenxes.data_in.XUniversalParser import XUniversalParser\n",
    "from opyenxes.classification.XEventAttributeClassifier import XEventAttributeClassifier\n",
    "from prefixspan import PrefixSpan\n",
    "\n",
    "data_path = \"../logs/bpic2011.xes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_path) as bpic_file:\n",
    "    eventlog = XUniversalParser().parse(bpic_file)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract data from XES format and enrich with BOS/EOS markers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect all attributes\n",
    "attribute_list = []\n",
    "\n",
    "# extract column names from any trace, here the first is used\n",
    "for event in eventlog[0]:\n",
    "    event_attributes = event.get_attributes()\n",
    "    \n",
    "    for attribute in event_attributes:\n",
    "        attribute_list.append(attribute)\n",
    "        \n",
    "attribute_list = set(attribute_list) # remove duplicates\n",
    "column_names   = [\"__case_id\"] + list(attribute_list)\n",
    "\n",
    "eventcount    = sum([2+len(t) for t in eventlog])\n",
    "event_indices = range(0, eventcount) # total number of entries in log\n",
    "eventlog_df   = pd.DataFrame(columns=column_names, index=event_indices)\n",
    "\n",
    "def set_row_value(df, row, colnames, val):\n",
    "    for column in colnames:\n",
    "        df.iloc[row][column] = val\n",
    "        \n",
    "def process_log_chunk(trace_offset, row_offset, chunk):\n",
    "    row_idx = row_offset\n",
    "    for trace_idx, raw_trace in enumerate(chunk):\n",
    "        # insert start-of-sequence marker\n",
    "        set_row_value(eventlog_df, row_idx, column_names, \"<bos>\")\n",
    "        row_idx += 1\n",
    "\n",
    "        for event_idx, event in enumerate(raw_trace):\n",
    "            event_attributes = event.get_attributes()\n",
    "            eventlog_df.iloc[row_idx][\"__case_id\"] = trace_idx + trace_offset\n",
    "\n",
    "            for attribute in event_attributes:\n",
    "                eventlog_df.iloc[row_idx][attribute] = event_attributes[attribute].get_value()\n",
    "\n",
    "            row_idx += 1\n",
    "        # finalize trace by inserting end-of-sequence marker    \n",
    "        set_row_value(eventlog_df, row_idx, column_names, \"<eos>\")\n",
    "        row_idx += 1\n",
    "        \n",
    "    print(row_offset, row_idx)\n",
    "\n",
    "threads  = []\n",
    "chunk_sz = int(math.ceil(len(eventlog) / multiprocessing.cpu_count()))\n",
    "row_offset = 0\n",
    "trace_offset = 0\n",
    "for core in range(0, multiprocessing.cpu_count()):\n",
    "    trace_offset = core*chunk_sz\n",
    "    chunk = eventlog[trace_offset : (core+1)*chunk_sz]\n",
    "    t = threading.Thread(target=process_log_chunk, args=(trace_offset, row_offset, chunk))\n",
    "    row_offset += sum([2+len(t) for t in chunk])\n",
    "    \n",
    "    threads.append(t)\n",
    "    \n",
    "[t.start() for t in threads]\n",
    "[t.join()  for t in threads]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#eventlog_df.to_csv(data_path.replace(\".xes\", \"_parse.csv\"))\n",
    "# eventlog_df = pd.read_csv(data_path.replace(\".xes\", \"_prepared.csv\"), index_col=[0])\n",
    "eventlog_df = pd.DataFrame.from_csv(data_path.replace(\".xes\", \"_parsed.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eliminate correlated or unimportant features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eventlog_df.describe().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cramers_v(confusion_matrix):\n",
    "    \"\"\" calculate Cramers V statistic for categorial-categorial association.\n",
    "        uses correction from Bergsma and Wicher,\n",
    "        Journal of the Korean Statistical Society 42 (2013): 323-328\n",
    "        https://stackoverflow.com/questions/46498455/categorical-features-correlation\"\"\"\n",
    "    chi2 = ss.chi2_contingency(confusion_matrix)[0]\n",
    "    n = confusion_matrix.sum()\n",
    "    phi2 = chi2 / n\n",
    "    r, k = confusion_matrix.shape\n",
    "    phi2corr = max(0, phi2 - ((k-1)*(r-1))/(n-1))\n",
    "    rcorr = r - ((r-1)**2)/(n-1)\n",
    "    kcorr = k - ((k-1)**2)/(n-1)\n",
    "    return np.sqrt(phi2corr / min((kcorr-1), (rcorr-1)))\n",
    "\n",
    "for col_a,col_b in itertools.product(eventlog_df.columns, repeat=2): \n",
    "    candidate = pd.crosstab(eventlog_df[col_a], eventlog_df[col_b]).as_matrix()\n",
    "    print(\"{: >30} {: >30} {: >20}\".format(col_a, col_b, cramers_v(candidate)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lifecyle:transition is always \"complete\"\n",
    "# Producer code correlates perfectly with org:group\n",
    "# Activity code correlates perfectly with concept:name\n",
    "eventlog_df = eventlog_df.drop(columns=[\"lifecycle:transition\", \"Producer code\", \"Activity code\", \"Section\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create windowed featureset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_windows(soffset, eoffset, core):\n",
    "    i = soffset\n",
    "    windows = []\n",
    "    \n",
    "    while i < eoffset:\n",
    "        window = []\n",
    "        for j in range(0, window_size):\n",
    "            window_part = eventlog_df.iloc[[i+j]].drop(columns=[\"__case_id\"]).add_prefix(\"w{0}!!\".format(j))\n",
    "            window_part.reset_index(drop=True, inplace=True)\n",
    "            window.append(window_part)\n",
    "            \n",
    "        window = pd.concat(window, axis=1)\n",
    "        window_part_width = int(window.shape[1] / window_size)\n",
    "        \n",
    "        if((window.iloc[0,window_part_width:] == \"<bos>\").any()):\n",
    "            i += window_size-2\n",
    "            continue\n",
    "            \n",
    "        i += 1\n",
    "        windows.append(window)\n",
    "    \n",
    "    thread_results[core] = windows\n",
    "\n",
    "# generating the windows takes incredibly long, speed up via parallel processing here\n",
    "corecount = multiprocessing.cpu_count()\n",
    "threads  = [None] * corecount\n",
    "thread_results = [None] * corecount\n",
    "chunk_sz = int(math.ceil(eventcount / corecount))\n",
    "row_offset = 0\n",
    "window_size = 5\n",
    "\n",
    "for core in range(0, corecount):\n",
    "    t = threading.Thread(target=generate_windows, args=(core*chunk_sz, (core+1)*chunk_sz, core))\n",
    "    threads[core] = t\n",
    "    \n",
    "[t.start() for t in threads]\n",
    "[t.join()  for t in threads]\n",
    "thread_results = list(itertools.chain.from_iterable(thread_results))\n",
    "thread_results = pd.concat(thread_results, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = eventlog_df.iloc[[1]].drop(columns=[\"__case_id\"]).add_prefix(\"w1_\")\n",
    "y = eventlog_df.iloc[[2]].drop(columns=[\"__case_id\"]).add_prefix(\"w2_\")\n",
    "\n",
    "w.reset_index(drop=True, inplace=True)\n",
    "y.reset_index(drop=True, inplace=True)\n",
    "\n",
    "pd.concat([w,y], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enrich with SP2 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/42636765/how-to-set-all-the-values-of-an-existing-pandas-dataframe-to-zero\n",
    "# This one-hot encodes all entries in concept:name column for later incrementation once it has been seen\n",
    "sp2_features = pd.get_dummies(eventlog_df[\"concept:name\"], prefix=\"SP2\")\n",
    "for col in sp2_features.columns: sp2_features[col].values[:] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enrich with PrefixSpan features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_patterns(pt):\n",
    "    for p in pt:\n",
    "        print(\"Support: {0}%\".format(100*p[0]/len(event_traces)))\n",
    "        for n in p[1]:\n",
    "            print(\"    > \", int_to_event[n])\n",
    "        print()\n",
    "        \n",
    "events       = list(set(eventlog_df[\"concept:name\"]))\n",
    "event_to_int = dict((c, i) for i,c in enumerate(events))\n",
    "int_to_event = dict((i, c) for i,c in enumerate(events))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prefixspan requires an array of arrays with one subarray for every trace\n",
    "indices = np.where(eventlog_df[\"concept:name\"] == \"<bos>\")[0].tolist()\n",
    "arr     = eventlog_df[\"concept:name\"].map(event_to_int).tolist()\n",
    "event_traces   = np.array_split(arr, indices)[1:] # remove randomly inserted array at the start of this list\n",
    "encoded_traces = [ [e for e in t] for t in event_traces ]\n",
    "prefixspan_traces = PrefixSpan(encoded_traces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: create indices for sequence items here for dictionary encoding\n",
    "ps_topkc = prefixspan_traces.topk(15, closed=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:thesis]",
   "language": "python",
   "name": "conda-env-thesis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
