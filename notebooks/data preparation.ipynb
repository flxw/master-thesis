{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import multiprocessing, threading\n",
    "import math\n",
    "import scipy.stats as ss\n",
    "import time\n",
    "\n",
    "from opyenxes.model.XLog import XLog\n",
    "from opyenxes.data_in.XUniversalParser import XUniversalParser\n",
    "from opyenxes.classification.XEventAttributeClassifier import XEventAttributeClassifier\n",
    "from prefixspan import PrefixSpan\n",
    "\n",
    "data_path = \"../logs/bpic2011.xes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unknown extension: http://www.xes-standard.org/meta_time.xesext\n",
      "Unknown extension: http://www.xes-standard.org/meta_life.xesext\n",
      "Unknown extension: http://www.xes-standard.org/meta_org.xesext\n",
      "Unknown extension: http://www.xes-standard.org/meta_concept.xesext\n",
      "Unknown extension: http://www.xes-standard.org/meta_3TU.xesext\n",
      "Unknown extension: http://www.xes-standard.org/meta_general.xesext\n"
     ]
    }
   ],
   "source": [
    "with open(data_path) as bpic_file:\n",
    "    eventlog = XUniversalParser().parse(bpic_file)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract data from XES format and enrich with BOS/EOS markers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect all attributes\n",
    "attribute_list = []\n",
    "\n",
    "# extract column names from any trace, here the first is used\n",
    "for event in eventlog[0]:\n",
    "    event_attributes = event.get_attributes()\n",
    "    \n",
    "    for attribute in event_attributes:\n",
    "        attribute_list.append(attribute)\n",
    "        \n",
    "attribute_list = set(attribute_list) # remove duplicates\n",
    "column_names   = [\"__case_id\"] + list(attribute_list)\n",
    "\n",
    "eventcount    = sum([2+len(t) for t in eventlog])\n",
    "event_indices = range(0, eventcount) # total number of entries in log\n",
    "eventlog_df   = pd.DataFrame(columns=column_names, index=event_indices)\n",
    "\n",
    "def set_row_value(df, row, colnames, val):\n",
    "    for column in colnames:\n",
    "        df.iloc[row][column] = val\n",
    "        \n",
    "def process_log_chunk(trace_offset, row_offset, chunk):\n",
    "    row_idx = row_offset\n",
    "    for trace_idx, raw_trace in enumerate(chunk):\n",
    "        # insert start-of-sequence marker\n",
    "        set_row_value(eventlog_df, row_idx, column_names, \"<bos>\")\n",
    "        row_idx += 1\n",
    "\n",
    "        for event_idx, event in enumerate(raw_trace):\n",
    "            event_attributes = event.get_attributes()\n",
    "            eventlog_df.iloc[row_idx][\"__case_id\"] = trace_idx + trace_offset\n",
    "\n",
    "            for attribute in event_attributes:\n",
    "                eventlog_df.iloc[row_idx][attribute] = event_attributes[attribute].get_value()\n",
    "\n",
    "            row_idx += 1\n",
    "        # finalize trace by inserting end-of-sequence marker    \n",
    "        set_row_value(eventlog_df, row_idx, column_names, \"<eos>\")\n",
    "        row_idx += 1\n",
    "        \n",
    "    print(\"Finished processing rows \", row_offset, \" through\", row_idx)\n",
    "\n",
    "threads  = []\n",
    "chunk_sz = int(math.ceil(len(eventlog) / multiprocessing.cpu_count()))\n",
    "row_offset = 0\n",
    "trace_offset = 0\n",
    "for core in range(0, multiprocessing.cpu_count()):\n",
    "    trace_offset = core*chunk_sz\n",
    "    chunk = eventlog[trace_offset : (core+1)*chunk_sz]\n",
    "    t = threading.Thread(target=process_log_chunk, args=(trace_offset, row_offset, chunk))\n",
    "    row_offset += sum([2+len(t) for t in chunk])\n",
    "    \n",
    "    threads.append(t)\n",
    "    \n",
    "[t.start() for t in threads]\n",
    "[t.join()  for t in threads]\n",
    "print(\"Finished parsing the event log!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del eventlog\n",
    "eventlog_df.to_csv(data_path.replace(\".xes\", \"_parsed.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "eventlog_df = pd.read_csv(data_path.replace(\".xes\", \"_parsed.csv\"), index_col=[0])\n",
    "eventcount  = len(eventlog_df)\n",
    "# eventlog_df = pd.DataFrame.from_csv(data_path.replace(\".xes\", \"_parsed.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eliminate correlated or unimportant features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cramers_v(confusion_matrix):\n",
    "    \"\"\" calculate Cramers V statistic for categorial-categorial association.\n",
    "        uses correction from Bergsma and Wicher,\n",
    "        Journal of the Korean Statistical Society 42 (2013): 323-328\n",
    "        https://stackoverflow.com/questions/46498455/categorical-features-correlation\"\"\"\n",
    "    chi2 = ss.chi2_contingency(confusion_matrix)[0]\n",
    "    n = confusion_matrix.sum()\n",
    "    phi2 = chi2 / n\n",
    "    r, k = confusion_matrix.shape\n",
    "    phi2corr = max(0, phi2 - ((k-1)*(r-1))/(n-1))\n",
    "    rcorr = r - ((r-1)**2)/(n-1)\n",
    "    kcorr = k - ((k-1)**2)/(n-1)\n",
    "    return np.sqrt(phi2corr / min((kcorr-1), (rcorr-1)))\n",
    "\n",
    "for col_a,col_b in itertools.product(eventlog_df.columns, repeat=2): \n",
    "    candidate = pd.crosstab(eventlog_df[col_a], eventlog_df[col_b]).as_matrix()\n",
    "    print(\"{: >30} {: >30} {: >20}\".format(col_a, col_b, cramers_v(candidate)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lifecyle:transition is always \"complete\"\n",
    "# Producer code correlates perfectly with org:group\n",
    "# Activity code correlates perfectly with concept:name\n",
    "eventlog_df = eventlog_df.drop(columns=[\"lifecycle:transition\", \"Producer code\", \"Activity code\", \"Section\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create windowed featureset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Window generation: 3580 windows on core 39 in 35.15171241760254s\n",
      "Window generation: 3699 windows on core 38 in 36.69233059883118s\n",
      "Window generation: 3711 windows on core 35 in 36.87334680557251s\n",
      "Window generation: 3731 windows on core 36 in 36.939133405685425s\n",
      "Window generation: 3695 windows on core 33 in 37.3782684803009s\n",
      "Window generation: 3699 windows on core 37 in 37.55412936210632s\n",
      "Window generation: 3755 windows on core 34 in 37.71281957626343s\n",
      "Window generation: 3755 windows on core 26 in 38.60425925254822s\n",
      "Window generation: 3723 windows on core 31 in 38.47282314300537s\n",
      "Window generation: 3739 windows on core 24 in 38.813809633255005s\n",
      "Window generation: 3715 windows on core 29 in 39.097424268722534s\n",
      "Window generation: 3611 windows on core 25 in 39.23093771934509s\n",
      "Window generation: 3683 windows on core 20 in 39.34364461898804s\n",
      "Window generation: 3723 windows on core 30 in 39.239952087402344s\n",
      "Window generation: 3691 windows on core 32 in 39.28441596031189s\n",
      "Window generation: 3667 windows on core 23 in 39.767221212387085s\n",
      "Window generation: 3651 windows on core 14 in 39.90144205093384s\n",
      "Window generation: 3755 windows on core 27 in 39.75986409187317s\n",
      "Window generation: 3715 windows on core 28 in 39.79475927352905s\n",
      "Window generation: 3655 windows on core 22 in 39.95917510986328s\n",
      "Window generation: 3727 windows on core 18 in 40.074663400650024s\n",
      "Window generation: 3695 windows on core 21 in 40.030399799346924s\n",
      "Window generation: 3595 windows on core 13 in 40.954021692276s\n",
      "Window generation: 3679 windows on core 11 in 41.247968912124634s\n",
      "Window generation: 3683 windows on core 19 in 41.31832480430603s\n",
      "Window generation: 3759 windows on core 12 in 41.707587480545044s\n",
      "Window generation: 3663 windows on core 15 in 41.73944878578186s\n",
      "Window generation: 3715 windows on core 8 in 41.80250096321106s\n",
      "Window generation: 3763 windows on core 16 in 41.821181535720825s\n",
      "Window generation: 3595 windows on core 9 in 41.95826959609985s\n",
      "Window generation: 3775 windows on core 17 in 41.980432987213135s\n",
      "Window generation: 3691 windows on core 10 in 42.07653594017029s\n",
      "Window generation: 3587 windows on core 6 in 42.378743410110474s\n",
      "Window generation: 3723 windows on core 5 in 42.78615641593933s\n",
      "Window generation: 3707 windows on core 1 in 43.05354285240173s\n",
      "Window generation: 3659 windows on core 7 in 43.08464479446411s\n",
      "Window generation: 3703 windows on core 4 in 43.18602633476257s\n",
      "Window generation: 3719 windows on core 0 in 43.69156002998352s\n",
      "Window generation: 3707 windows on core 2 in 43.98291635513306s\n",
      "Window generation: 3751 windows on core 3 in 44.66411471366882s\n"
     ]
    }
   ],
   "source": [
    "def generate_windows(chunk, window_size, core):\n",
    "    i = 0\n",
    "    windows = []\n",
    "    t_start = time.time()\n",
    "        \n",
    "    while i < len(chunk)-window_size+1:\n",
    "        window = []\n",
    "        for j in range(0, window_size):\n",
    "            window_part = chunk.iloc[[i+j]].drop(columns=[\"__case_id\"]).add_prefix(\"w{0}!!\".format(j))\n",
    "            window_part.reset_index(drop=True, inplace=True)\n",
    "            window.append(window_part)\n",
    "            \n",
    "        window = pd.concat(window, axis=1)\n",
    "        window_part_width = int(window.shape[1] / window_size)\n",
    "        \n",
    "        if((window.iloc[0,window_part_width:] == \"<bos>\").any()):\n",
    "            i += window_size-1\n",
    "            continue\n",
    "            \n",
    "        i += 1\n",
    "        windows.append(window)\n",
    "    \n",
    "    print(\"Window generation: {0} windows on core {1} in {2}s\".format(len(windows), core, time.time() - t_start))\n",
    "    return pd.concat(windows).reset_index(drop=True)\n",
    "\n",
    "# generating the windows takes incredibly long because of NUMA effects, put everything into processes to avoid contention\n",
    "corecount = multiprocessing.cpu_count()\n",
    "process_pool = multiprocessing.Pool(corecount)\n",
    "chunk_sz = int(math.ceil(eventcount / corecount))\n",
    "window_size = 5\n",
    "process_args = []\n",
    "\n",
    "for core in range(0, corecount):\n",
    "    process_args.append((eventlog_df[core*chunk_sz:(core+1)*chunk_sz].reset_index(drop=True), window_size, core))\n",
    "\n",
    "process_results = process_pool.starmap(generate_windows, process_args)\n",
    "process_results = pd.concat(process_results, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_results.to_csv(data_path.replace(\".xes\", \"_windowed.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enrich with SP2 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/42636765/how-to-set-all-the-values-of-an-existing-pandas-dataframe-to-zero\n",
    "# This one-hot encodes all entries in concept:name column for later incrementation once it has been seen\n",
    "sp2_features = pd.get_dummies(eventlog_df[\"concept:name\"], prefix=\"SP2\") # can't use windowed representation here as it might skew distribution of values\n",
    "for col in sp2_features.columns: sp2_features[col].values[:] = 0\n",
    "\n",
    "eventlog_sp2_df = process_results.copy(deep=True)\n",
    "sp2_features    = sp2_features.drop(sp2_features.index[sp2_features.index[len(eventlog_sp2_df):]])\n",
    "assert(len(sp2_features) == len(eventlog_sp2_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enriched 147849 rows with SP2 features in 613.7024304866791s\n"
     ]
    }
   ],
   "source": [
    "t_start = time.time()\n",
    "for i in range(0, len(sp2_features)):\n",
    "    first_activity_name = eventlog_sp2_df[\"w0!!concept:name\"].iloc[i]\n",
    "    if (first_activity_name != \"<bos>\"):\n",
    "        sp2_features.iloc[i] = sp2_features.iloc[i-1]\n",
    "    else: # TODO implement stepping through entire window here to extract all appeared items\n",
    "        pass\n",
    "    \n",
    "    sp2_features[\"SP2_{0}\".format(activity_name)].iloc[i] = 1\n",
    "    \n",
    "print(\"Enriched {0} rows with SP2 features in {1}s\".format(len(sp2_features), time.time()-t_start))\n",
    "# TODO parallelize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp2_features[\"SP2_<eos>\"]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enrich with PrefixSpan features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_patterns(pt):\n",
    "    for p in pt:\n",
    "        print(\"Support: {0}%\".format(100*p[0]/len(event_traces)))\n",
    "        for n in p[1]:\n",
    "            print(\"    > \", int_to_event[n])\n",
    "        print()\n",
    "        \n",
    "events       = list(set(eventlog_df[\"concept:name\"]))\n",
    "event_to_int = dict((c, i) for i,c in enumerate(events))\n",
    "int_to_event = dict((i, c) for i,c in enumerate(events))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prefixspan requires an array of arrays with one subarray for every trace\n",
    "indices = np.where(eventlog_df[\"concept:name\"] == \"<bos>\")[0].tolist()\n",
    "arr     = eventlog_df[\"concept:name\"].map(event_to_int).tolist()\n",
    "event_traces   = np.array_split(arr, indices)[1:] # remove randomly inserted array at the start of this list\n",
    "encoded_traces = [ [e for e in t] for t in event_traces ]\n",
    "prefixspan_traces = PrefixSpan(encoded_traces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: create indices for sequence items here for dictionary encoding\n",
    "ps_topkc = prefixspan_traces.topk(15, closed=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:thesis]",
   "language": "python",
   "name": "conda-env-thesis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
