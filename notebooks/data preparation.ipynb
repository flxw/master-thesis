{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import multiprocessing, threading\n",
    "import math\n",
    "import scipy.stats as ss\n",
    "import time\n",
    "import pickle\n",
    "import re\n",
    "\n",
    "from opyenxes.model.XLog import XLog\n",
    "from opyenxes.data_in.XUniversalParser import XUniversalParser\n",
    "from opyenxes.classification.XEventAttributeClassifier import XEventAttributeClassifier\n",
    "\n",
    "from prefixspan import PrefixSpan\n",
    "\n",
    "from tqdm import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### configuration\n",
    "data_path = \"../logs/bpic2011.xes\"\n",
    "traces_picklepath = data_path.replace(\".xes\", \"_raw_traces.pickled\")\n",
    "traces_tmppath = data_path.replace(\".xes\", \"_traces_tmp.pickled\")\n",
    "traces_finalpath = data_path.replace(\".xes\", \"_traces_encoded.pickled\")\n",
    "traces_dictionarypath = data_path.replace(\".xes\", \"_dictionaries.pickled\")\n",
    "target_column = \"concept:name\"\n",
    "categorical_features = [\"concept:name\", \"Specialism code\", \"org:group\"]\n",
    "### configuration end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_path) as bpic_file:\n",
    "    eventlog = XUniversalParser().parse(bpic_file)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncores = multiprocessing.cpu_count()\n",
    "ntraces = len(eventlog)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract data trace-wise from XES format and enrich with BOS/EOS markers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect all attributes\n",
    "column_names = []\n",
    "\n",
    "for event in eventlog[0]:\n",
    "    for attribute in event.get_attributes():\n",
    "        column_names.append(attribute)\n",
    "        \n",
    "column_names = set(column_names) # remove duplicates\n",
    "column_names = list(column_names)\n",
    "\n",
    "def create_dataframe_from_trace(t):\n",
    "    df = pd.DataFrame(columns=column_names, index=range(0,len(t)))\n",
    "    for event_idx, event in enumerate(t):\n",
    "        event_attributes = event.get_attributes()\n",
    "        df.iloc[event_idx][\"__case_id\"] = 0\n",
    "        \n",
    "        for attribute in event_attributes:\n",
    "            df[attribute].values[event_idx] = event_attributes[attribute].get_value()\n",
    "    \n",
    "    return df\n",
    "\n",
    "ppool = multiprocessing.Pool(ncores)\n",
    "traces = []\n",
    "with tqdm(total=len(eventlog), desc=\"Converting XES traces to Pandas dataframes\", unit=\"traces\") as pbar:\n",
    "    for i, _ in tqdm(enumerate(ppool.imap(create_dataframe_from_trace, eventlog))):\n",
    "        pbar.update()\n",
    "        traces.append(_)\n",
    "        \n",
    "del eventlog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(traces, open(traces_picklepath, \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traces = pickle.load(open(traces_picklepath, \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eliminate correlated or unimportant features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cramers_v(confusion_matrix):\n",
    "    \"\"\" calculate Cramers V statistic for categorial-categorial association.\n",
    "        uses correction from Bergsma and Wicher,\n",
    "        Journal of the Korean Statistical Society 42 (2013): 323-328\n",
    "        https://stackoverflow.com/questions/46498455/categorical-features-correlation\"\"\"\n",
    "    chi2 = ss.chi2_contingency(confusion_matrix)[0]\n",
    "    n = confusion_matrix.sum()\n",
    "    phi2 = chi2 / n\n",
    "    r, k = confusion_matrix.shape\n",
    "    phi2corr = max(0, phi2 - ((k-1)*(r-1))/(n-1))\n",
    "    rcorr = r - ((r-1)**2)/(n-1)\n",
    "    kcorr = k - ((k-1)**2)/(n-1)\n",
    "    return np.sqrt(phi2corr / min((kcorr-1), (rcorr-1)))\n",
    "\n",
    "for col_a,col_b in itertools.product(eventlog_df.columns, repeat=2): \n",
    "    candidate = pd.crosstab(eventlog_df[col_a], eventlog_df[col_b]).as_matrix()\n",
    "    print(\"{: >30} {: >30} {: >20}\".format(col_a, col_b, cramers_v(candidate)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lifecyle:transition is always \"complete\"\n",
    "# Producer code correlates perfectly with org:group\n",
    "# Activity code correlates perfectly with concept:name\n",
    "for t in traces:\n",
    "    t.drop(columns=[\"lifecycle:transition\", \"Producer code\", \"Activity code\", \"Section\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create standard feature set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set data types for columns that were not autodetected\n",
    "for i in range(0,len(traces)):\n",
    "    traces[i][\"Specialism code\"] = pd.to_numeric(traces[i][\"Specialism code\"], errors=\"ignore\")\n",
    "    traces[i][\"Number of executions\"] = pd.to_numeric(traces[i][\"Number of executions\"], errors=\"ignore\")\n",
    "\n",
    "eventlog_df = pd.concat(traces, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create SP2 feature set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/42636765/how-to-set-all-the-values-of-an-existing-pandas-dataframe-to-zero\n",
    "# This one-hot encodes all entries in concept:name column for later incrementation once it has been seen\n",
    "# sp2_features = pd.get_dummies(eventlog_df[\"concept:name\"], prefix=\"SP2\") # can't use windowed representation here as it might skew distribution of values\n",
    "# eventlog_sp2_df = process_results.copy(deep=True)\n",
    "# sp2_features    = sp2_features.drop(sp2_features.index[sp2_features.index[len(eventlog_sp2_df):]])\n",
    "# assert(len(sp2_features) == len(eventlog_sp2_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through every trace and encode the presence of an activity\n",
    "sp2_prefix = \"SP2_\"\n",
    "activity_labels = [ \"{0}{1}\".format(sp2_prefix,a) for a in eventlog_df[\"concept:name\"].unique() ]\n",
    "\n",
    "def enrich_trace_with_sp2(t):\n",
    "    sp2_df = pd.DataFrame(columns=activity_labels, index=range(0,len(t)), dtype=np.bool)\n",
    "    for col in sp2_df.columns: sp2_df[col].values[:] = 0\n",
    "    sp2_df[\"{0}{1}\".format(sp2_prefix, t[\"concept:name\"][0])].values[0]  = 1\n",
    "    \n",
    "    for i in range(1,len(t)):\n",
    "        first_activity_name = t[\"concept:name\"].iloc[i]\n",
    "        col = \"{0}{1}\".format(sp2_prefix,first_activity_name)\n",
    "        \n",
    "        sp2_df.values[i] = sp2_df.values[i-1]\n",
    "        sp2_df[col].values[i] = 1\n",
    "        \n",
    "    return sp2_df\n",
    "\n",
    "ppool = multiprocessing.Pool(ncores)\n",
    "sp2_traces = []\n",
    "with tqdm(total=len(traces), desc=\"Enriching traces with SP2 features\", unit=\"traces\") as pbar:\n",
    "    for i, _ in tqdm(enumerate(ppool.imap(enrich_trace_with_sp2, traces))):\n",
    "        pbar.update()\n",
    "        sp2_traces.append(_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enrich with PrefixSpan features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_patterns(pt):\n",
    "    for p in pt:\n",
    "        print(\"Support: {0}%\".format(100*p[0]/len(traces)))\n",
    "        for n in p[1]:\n",
    "            print(\"    > \", int_to_event[n])\n",
    "        print()\n",
    "\n",
    "# since most patterns begin and end with the <eos> and <bos> markers, the features only become valuable towards the end...\n",
    "events       = eventlog_df[\"concept:name\"].unique()\n",
    "event_to_int = dict((c, i) for i,c in enumerate(events) if c not in [\"<bos>\",\"<eos>\"])\n",
    "int_to_event = dict((i, c) for i,c in enumerate(events) if c not in [\"<bos>\",\"<eos>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traces = save_traces[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prefixspan requires an array of arrays with one subarray for every trace\n",
    "encoded_traces = [ t[\"concept:name\"].map(event_to_int).tolist() for t in traces ]\n",
    "prefixspan_traces = PrefixSpan(encoded_traces)\n",
    "closed_sequences = prefixspan_traces.topk(25, closed=True) # support is how often the subsequence appears in total\n",
    "# http://sequenceanalysis.github.io/slides/analyzing_sequential_user_behavior_part2.pdf, slide 5\n",
    "# print_patterns(ps_topkc)\n",
    "\n",
    "# only take subsequence which are at a certain level of support? like if ss[0]/len(traces) < .90\n",
    "#ps_topkc = list(filter(lambda x: x[0]/len(traces) > .90, ps_topkc))\n",
    "closed_sequences = [ p[1] for p in closed_sequences ]\n",
    "pftrace_args = [ (t, closed_sequences[:], event_to_int) for t in traces ] # enrich traces with copy of mined subsequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrapped__enrich_trace_with_subseq(args):\n",
    "    return enrich_trace_with_subseq(*args)\n",
    "\n",
    "def enrich_trace_with_subseq(t, ps, event_to_int):\n",
    "    col_prefix = \"PFS_\"\n",
    "    subseq_labels = [ \"{0}{1}\".format(col_prefix,ss_idx) for ss_idx, ss in enumerate(ps) ]\n",
    "    subseq_df = pd.DataFrame(columns=subseq_labels, index=range(0,len(t)), dtype=np.bool)\n",
    "    \n",
    "    for col in subseq_df.columns: subseq_df[col].values[:] = 0\n",
    "    for i in range(0,len(t)): # loop through sequence, prune items from mined sequences, and once a subsequence array is empty, this subsequence has occured :)\n",
    "        activity_code = event_to_int.get(t[\"concept:name\"].iloc[i], None)\n",
    "        \n",
    "        for subseq_idx in range(0,len(ps)):\n",
    "            if ps[subseq_idx] == []:\n",
    "                continue\n",
    "            if ps[subseq_idx][0] == activity_code:\n",
    "                ps[subseq_idx].pop(0)\n",
    "                if ps[subseq_idx] == []:\n",
    "                    subseq_df.values[i:,subseq_idx] = 1\n",
    "        \n",
    "    return subseq_df\n",
    "\n",
    "ppool = multiprocessing.Pool(ncores)\n",
    "pf_traces = []\n",
    "\n",
    "with tqdm(total=len(pftrace_args), desc=\"Enriching traces with mined subsequence features\", unit=\"traces\") as pbar:\n",
    "    for i, _ in tqdm(enumerate(ppool.imap(wrapped__enrich_trace_with_subseq, pftrace_args))):\n",
    "        pbar.update()\n",
    "        pf_traces.append(_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dictionary encoding for categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert timestamp to total running time\n",
    "bos_idx = 0\n",
    "for i in range(0, len(traces)):\n",
    "    tlen = len(traces[i])-1\n",
    "    dfs = traces[i][\"time:timestamp\"] - traces[i][\"time:timestamp\"][bos_idx]\n",
    "    traces[i][\"time:timestamp\"] = dfs.map(lambda d: int(d.total_seconds()/(60*60))) # convert to hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "save_traces = copy.deepcopy(traces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traces = copy.deepcopy(save_traces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_dict = {}\n",
    "\n",
    "for cf in categorical_features:\n",
    "    cf_dict = { 'to_int': {}, 'to_cat': {} }\n",
    "    events = eventlog_df[cf].unique().tolist()\n",
    "    if cf == target_column: events.append(\"<EOS>\")\n",
    "    cf_dict['to_int'] = dict((c, i) for i, c in enumerate(events))\n",
    "    cf_dict['to_cat'] = dict((i, c) for i, c in enumerate(events))\n",
    "    feature_dict[cf] = cf_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataframe creation and saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinal_features = traces[0].columns.difference(categorical_features)\n",
    "\n",
    "# Concatenate all features into one feature dataframe per trace\n",
    "for i in range(0,len(traces)):\n",
    "    targets = traces[i][target_column].shift(-1).to_frame(\"TARGET\")\n",
    "    targets.values[len(traces[i])-1] = \"<EOS>\"\n",
    "    targets[\"TARGET\"] = targets[\"TARGET\"].map(feature_dict[target_column]['to_int'])\n",
    "    \n",
    "    for cf in categorical_features:\n",
    "        traces[i][cf] = traces[i][cf].map(feature_dict[cf]['to_int'])\n",
    "    \n",
    "    ordinal_trace = traces[i][ordinal_features]\n",
    "    categorical_trace = traces[i][categorical_features]\n",
    "    traces[i] = pd.concat([ordinal_trace, categorical_trace, sp2_traces[i], pf_traces[i], targets], ignore_index=False, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(sum([len(t) for t in traces]) == len(eventlog_df))\n",
    "pickle.dump(traces, open(traces_finalpath, \"wb\"), protocol=pickle.HIGHEST_PROTOCOL)\n",
    "pickle.dump(feature_dict, open(traces_dictionarypath, \"wb\"), protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "used_bytes = sum([t.memory_usage(index=True, deep=True).sum() for t in traces])\n",
    "print(\"Memory usage of encoded data: {0} KB\".format(used_bytes / 1024))\n",
    "print(\"Memory usage of encoded data: {0} MB\".format(used_bytes / 1024**2))\n",
    "print(\"Memory usage of encoded data: {0} GB\".format(used_bytes / 1024**3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_columns = traces[0].columns.tolist()\n",
    "trace_columns = list(map(lambda e: bool(re.match('^TARGET$', e)), trace_columns))\n",
    "trace_columns.index(True) # when do the target columns start?!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:thesis-notebook]",
   "language": "python",
   "name": "conda-env-thesis-notebook-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
