{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import multiprocessing, threading\n",
    "import math\n",
    "import scipy.stats as ss\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "from opyenxes.model.XLog import XLog\n",
    "from opyenxes.data_in.XUniversalParser import XUniversalParser\n",
    "from opyenxes.classification.XEventAttributeClassifier import XEventAttributeClassifier\n",
    "\n",
    "from prefixspan import PrefixSpan\n",
    "\n",
    "from tqdm import *\n",
    "\n",
    "data_path = \"../logs/bpic2011.xes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_path) as bpic_file:\n",
    "    eventlog = XUniversalParser().parse(bpic_file)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'eventlog' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-6563285e91f5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mncores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmultiprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mntraces\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meventlog\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'eventlog' is not defined"
     ]
    }
   ],
   "source": [
    "ncores = multiprocessing.cpu_count()\n",
    "ntraces = len(eventlog)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract data trace-wise from XES format and enrich with BOS/EOS markers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect all attributes\n",
    "column_names = []\n",
    "\n",
    "for event in eventlog[0]:\n",
    "    for attribute in event.get_attributes():\n",
    "        column_names.append(attribute)\n",
    "        \n",
    "column_names = set(column_names) # remove duplicates\n",
    "column_names = list(column_names)\n",
    "\n",
    "def create_dataframe_from_trace(t):\n",
    "    df = pd.DataFrame(columns=column_names, index=range(0,len(t)))\n",
    "    for event_idx, event in enumerate(t):\n",
    "        event_attributes = event.get_attributes()\n",
    "        df.iloc[event_idx][\"__case_id\"] = 0\n",
    "\n",
    "        for attribute in event_attributes:\n",
    "            df[attribute].values[event_idx] = event_attributes[attribute].get_value()\n",
    "    \n",
    "    return df\n",
    "\n",
    "ppool = multiprocessing.Pool(ncores)\n",
    "traces = []\n",
    "with tqdm(total=len(eventlog), desc=\"Converting XES traces to Pandas dataframes\", unit=\"traces\") as pbar:\n",
    "    for i, _ in tqdm(enumerate(ppool.imap(create_dataframe_from_trace, eventlog))):\n",
    "        pbar.update()\n",
    "        traces.append(_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del eventlog\n",
    "traces_picklepath = data_path.replace(\".xes\", \"_traces.pickled\")\n",
    "pickle.dump(traces, open(traces_picklepath, \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "traces_picklepath = data_path.replace(\".xes\", \"_traces.pickled\")\n",
    "traces = pickle.load(open(traces_picklepath, \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eliminate correlated or unimportant features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cramers_v(confusion_matrix):\n",
    "    \"\"\" calculate Cramers V statistic for categorial-categorial association.\n",
    "        uses correction from Bergsma and Wicher,\n",
    "        Journal of the Korean Statistical Society 42 (2013): 323-328\n",
    "        https://stackoverflow.com/questions/46498455/categorical-features-correlation\"\"\"\n",
    "    chi2 = ss.chi2_contingency(confusion_matrix)[0]\n",
    "    n = confusion_matrix.sum()\n",
    "    phi2 = chi2 / n\n",
    "    r, k = confusion_matrix.shape\n",
    "    phi2corr = max(0, phi2 - ((k-1)*(r-1))/(n-1))\n",
    "    rcorr = r - ((r-1)**2)/(n-1)\n",
    "    kcorr = k - ((k-1)**2)/(n-1)\n",
    "    return np.sqrt(phi2corr / min((kcorr-1), (rcorr-1)))\n",
    "\n",
    "for col_a,col_b in itertools.product(eventlog_df.columns, repeat=2): \n",
    "    candidate = pd.crosstab(eventlog_df[col_a], eventlog_df[col_b]).as_matrix()\n",
    "    print(\"{: >30} {: >30} {: >20}\".format(col_a, col_b, cramers_v(candidate)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lifecyle:transition is always \"complete\"\n",
    "# Producer code correlates perfectly with org:group\n",
    "# Activity code correlates perfectly with concept:name\n",
    "for t in traces:\n",
    "    t.drop(columns=[\"lifecycle:transition\", \"Producer code\", \"Activity code\", \"Section\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create standard featureset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "eventlog_df = [None] * len(traces)\n",
    "nattr = len(traces[0].columns)\n",
    "bos_df = pd.DataFrame([(\"<bos>\",)*nattr], columns = traces[0].columns)\n",
    "eos_df = pd.DataFrame([(\"<eos>\",)*nattr], columns = traces[0].columns)\n",
    "\n",
    "for i in range(0,len(traces)):\n",
    "    traces[i] = pd.concat([bos_df, traces[i], eos_df], ignore_index=True)\n",
    "\n",
    "eventlog_df = pd.concat(traces, ignore_index=True)\n",
    "# TODO: one-hot encoding still remains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create SP2 feature set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/42636765/how-to-set-all-the-values-of-an-existing-pandas-dataframe-to-zero\n",
    "# This one-hot encodes all entries in concept:name column for later incrementation once it has been seen\n",
    "# sp2_features = pd.get_dummies(eventlog_df[\"concept:name\"], prefix=\"SP2\") # can't use windowed representation here as it might skew distribution of values\n",
    "# eventlog_sp2_df = process_results.copy(deep=True)\n",
    "# sp2_features    = sp2_features.drop(sp2_features.index[sp2_features.index[len(eventlog_sp2_df):]])\n",
    "# assert(len(sp2_features) == len(eventlog_sp2_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through every trace and encode the presence of an activity\n",
    "sp2_prefix = \"SP2_\"\n",
    "activity_labels = [ \"{0}{1}\".format(sp2_prefix,a) for a in eventlog_df[\"concept:name\"].unique() ]\n",
    "\n",
    "def enrich_trace_with_sp2(t):\n",
    "    sp2_df = pd.DataFrame(columns=activity_labels, index=range(0,len(t)))\n",
    "    for col in sp2_df.columns: sp2_df[col].values[:] = 0\n",
    "    sp2_df[\"{0}<bos>\".format(sp2_prefix)].values[0]  = 1\n",
    "    \n",
    "    for i in range(1,len(t)):\n",
    "        first_activity_name = t[\"concept:name\"].iloc[i]\n",
    "        col = \"{0}{1}\".format(sp2_prefix,first_activity_name)\n",
    "        \n",
    "        sp2_df.values[i] = sp2_df.values[i-1]\n",
    "        sp2_df[col].values[i] = 1\n",
    "        \n",
    "    return pd.concat([t, sp2_df], axis=1)\n",
    "\n",
    "ppool = multiprocessing.Pool(ncores)\n",
    "ttraces = []\n",
    "with tqdm(total=len(traces), desc=\"Enriching traces with SP2 features\", unit=\"traces\") as pbar:\n",
    "    for i, _ in tqdm(enumerate(ppool.imap(enrich_trace_with_sp2, traces))):\n",
    "        pbar.update()\n",
    "        ttraces.append(_)\n",
    "        \n",
    "traces = ttraces\n",
    "del ttraces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enrich with PrefixSpan features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_patterns(pt):\n",
    "    for p in pt:\n",
    "        print(\"Support: {0}%\".format(100*p[0]/len(traces)))\n",
    "        for n in p[1]:\n",
    "            print(\"    > \", int_to_event[n])\n",
    "        print()\n",
    "\n",
    "# since most patterns begin and end with the <eos> and <bos> markers, the features only become valuable towards the end...\n",
    "events       = eventlog_df[\"concept:name\"].unique()\n",
    "event_to_int = dict((c, i) for i,c in enumerate(events) if c not in [\"<bos>\",\"<eos>\"])\n",
    "int_to_event = dict((i, c) for i,c in enumerate(events) if c not in [\"<bos>\",\"<eos>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traces = save_traces[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_traces = traces[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prefixspan requires an array of arrays with one subarray for every trace\n",
    "encoded_traces = [ t[\"concept:name\"].map(event_to_int).tolist() for t in traces ]\n",
    "prefixspan_traces = PrefixSpan(encoded_traces)\n",
    "ps_topkc = prefixspan_traces.topk(25, closed=True) # support is how often the subsequence appears in total\n",
    "# http://sequenceanalysis.github.io/slides/analyzing_sequential_user_behavior_part2.pdf, slide 5\n",
    "# print_patterns(ps_topkc)\n",
    "\n",
    "# only take subsequence which are at a certain level of support? like if ss[0]/len(traces) < .90\n",
    "#ps_topkc = list(filter(lambda x: x[0]/len(traces) > .90, ps_topkc))\n",
    "ps_topkc = [ p[1] for p in ps_topkc ]\n",
    "ptraces = [ (t, ps_topkc[:], event_to_int) for t in traces ] # enrich traces with copy of mined subsequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrapped__enrich_trace_with_subseq(args):\n",
    "    return enrich_trace_with_subseq(*args)\n",
    "\n",
    "def enrich_trace_with_subseq(t, ps, event_to_int):\n",
    "    col_prefix = \"PFS_\"\n",
    "    subseq_labels = [ \"{0}{1}\".format(col_prefix,ss_idx) for ss_idx, ss in enumerate(ps) ]\n",
    "    subseq_df = pd.DataFrame(columns=subseq_labels, index=range(0,len(t)))\n",
    "    \n",
    "    for col in subseq_df.columns: subseq_df[col].values[:] = 0\n",
    "    for i in range(0,len(t)): # loop through sequence, prune items from mined sequences, and once a subsequence array is empty, this subsequence has occured :)\n",
    "        activity_code = event_to_int.get(t[\"concept:name\"].iloc[i], None)\n",
    "        \n",
    "        for subseq_idx in range(0,len(ps)):\n",
    "            if ps[subseq_idx] == []:\n",
    "                continue\n",
    "            if ps[subseq_idx][0] == activity_code:\n",
    "                ps[subseq_idx].pop(0)\n",
    "                if ps[subseq_idx] == []:\n",
    "                    subseq_df.values[i:,subseq_idx] = 1\n",
    "        \n",
    "    return pd.concat([t, subseq_df], axis=1)\n",
    "\n",
    "ppool = multiprocessing.Pool(ncores)\n",
    "ttraces = []\n",
    "# ttraces = ppool.starmap(enrich_trace_with_subseq, ptraces[0:10])\n",
    "with tqdm(total=len(traces), desc=\"Enriching traces with mined subsequence features\", unit=\"traces\") as pbar:\n",
    "    for i, _ in tqdm(enumerate(ppool.imap(wrapped__enrich_trace_with_subseq, ptraces))):\n",
    "        pbar.update()\n",
    "        ttraces.append(_)\n",
    "        \n",
    "ptraces = ttraces\n",
    "del ttraces"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:thesis]",
   "language": "python",
   "name": "conda-env-thesis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
