{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import multiprocessing, threading\n",
    "import math\n",
    "import scipy.stats as ss\n",
    "import time\n",
    "\n",
    "from opyenxes.model.XLog import XLog\n",
    "from opyenxes.data_in.XUniversalParser import XUniversalParser\n",
    "from opyenxes.classification.XEventAttributeClassifier import XEventAttributeClassifier\n",
    "from prefixspan import PrefixSpan\n",
    "\n",
    "data_path = \"../logs/bpic2011.xes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unknown extension: http://www.xes-standard.org/meta_time.xesext\n",
      "Unknown extension: http://www.xes-standard.org/meta_life.xesext\n",
      "Unknown extension: http://www.xes-standard.org/meta_org.xesext\n",
      "Unknown extension: http://www.xes-standard.org/meta_concept.xesext\n",
      "Unknown extension: http://www.xes-standard.org/meta_3TU.xesext\n",
      "Unknown extension: http://www.xes-standard.org/meta_general.xesext\n"
     ]
    }
   ],
   "source": [
    "with open(data_path) as bpic_file:\n",
    "    eventlog = XUniversalParser().parse(bpic_file)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract data from XES format and enrich with BOS/EOS markers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect all attributes\n",
    "attribute_list = []\n",
    "\n",
    "# extract column names from any trace, here the first is used\n",
    "for event in eventlog[0]:\n",
    "    event_attributes = event.get_attributes()\n",
    "    \n",
    "    for attribute in event_attributes:\n",
    "        attribute_list.append(attribute)\n",
    "        \n",
    "attribute_list = set(attribute_list) # remove duplicates\n",
    "column_names   = [\"__case_id\"] + list(attribute_list)\n",
    "\n",
    "eventcount    = sum([2+len(t) for t in eventlog])\n",
    "event_indices = range(0, eventcount) # total number of entries in log\n",
    "eventlog_df   = pd.DataFrame(columns=column_names, index=event_indices)\n",
    "\n",
    "def set_row_value(df, row, colnames, val):\n",
    "    for column in colnames:\n",
    "        df.iloc[row][column] = val\n",
    "        \n",
    "def process_log_chunk(trace_offset, row_offset, chunk):\n",
    "    row_idx = row_offset\n",
    "    for trace_idx, raw_trace in enumerate(chunk):\n",
    "        # insert start-of-sequence marker\n",
    "        set_row_value(eventlog_df, row_idx, column_names, \"<bos>\")\n",
    "        row_idx += 1\n",
    "\n",
    "        for event_idx, event in enumerate(raw_trace):\n",
    "            event_attributes = event.get_attributes()\n",
    "            eventlog_df.iloc[row_idx][\"__case_id\"] = trace_idx + trace_offset\n",
    "\n",
    "            for attribute in event_attributes:\n",
    "                eventlog_df.iloc[row_idx][attribute] = event_attributes[attribute].get_value()\n",
    "\n",
    "            row_idx += 1\n",
    "        # finalize trace by inserting end-of-sequence marker    \n",
    "        set_row_value(eventlog_df, row_idx, column_names, \"<eos>\")\n",
    "        row_idx += 1\n",
    "        \n",
    "    print(\"Finished processing rows \", row_offset, \" through\", row_idx)\n",
    "\n",
    "threads  = []\n",
    "chunk_sz = int(math.ceil(len(eventlog) / multiprocessing.cpu_count()))\n",
    "row_offset = 0\n",
    "trace_offset = 0\n",
    "for core in range(0, multiprocessing.cpu_count()):\n",
    "    trace_offset = core*chunk_sz\n",
    "    chunk = eventlog[trace_offset : (core+1)*chunk_sz]\n",
    "    t = threading.Thread(target=process_log_chunk, args=(trace_offset, row_offset, chunk))\n",
    "    row_offset += sum([2+len(t) for t in chunk])\n",
    "    \n",
    "    threads.append(t)\n",
    "    \n",
    "[t.start() for t in threads]\n",
    "[t.join()  for t in threads]\n",
    "print(\"Finished parsing the event log!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del eventlog\n",
    "eventlog_df.to_csv(data_path.replace(\".xes\", \"_parsed.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "eventlog_df = pd.read_csv(data_path.replace(\".xes\", \"_parsed.csv\"), index_col=[0])\n",
    "eventcount  = len(eventlog_df)\n",
    "# eventlog_df = pd.DataFrame.from_csv(data_path.replace(\".xes\", \"_parsed.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eliminate correlated or unimportant features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cramers_v(confusion_matrix):\n",
    "    \"\"\" calculate Cramers V statistic for categorial-categorial association.\n",
    "        uses correction from Bergsma and Wicher,\n",
    "        Journal of the Korean Statistical Society 42 (2013): 323-328\n",
    "        https://stackoverflow.com/questions/46498455/categorical-features-correlation\"\"\"\n",
    "    chi2 = ss.chi2_contingency(confusion_matrix)[0]\n",
    "    n = confusion_matrix.sum()\n",
    "    phi2 = chi2 / n\n",
    "    r, k = confusion_matrix.shape\n",
    "    phi2corr = max(0, phi2 - ((k-1)*(r-1))/(n-1))\n",
    "    rcorr = r - ((r-1)**2)/(n-1)\n",
    "    kcorr = k - ((k-1)**2)/(n-1)\n",
    "    return np.sqrt(phi2corr / min((kcorr-1), (rcorr-1)))\n",
    "\n",
    "for col_a,col_b in itertools.product(eventlog_df.columns, repeat=2): \n",
    "    candidate = pd.crosstab(eventlog_df[col_a], eventlog_df[col_b]).as_matrix()\n",
    "    print(\"{: >30} {: >30} {: >20}\".format(col_a, col_b, cramers_v(candidate)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lifecyle:transition is always \"complete\"\n",
    "# Producer code correlates perfectly with org:group\n",
    "# Activity code correlates perfectly with concept:name\n",
    "eventlog_df = eventlog_df.drop(columns=[\"lifecycle:transition\", \"Producer code\", \"Activity code\", \"Section\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create windowed featureset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished window  300 400  on core  3\n",
      "Finished window  1400 1500  on core  14\n",
      "Finished window  1200 1300  on core  12\n",
      "Finished window  1000 1100  on core  10\n",
      "Finished window  400 500  on core  4Finished window  600 700  on core  6\n",
      "\n",
      "Finished window Finished window  1100 1200  on core   2500 2600 Finished window  2300 2400  on core  on core  25\n",
      "11\n",
      " 23\n",
      "Finished window  0 100  on core  0\n",
      "Finished window  500 Finished window Finished window  600  on core  5\n",
      " 100 200  on core  1\n",
      "Finished window  2700 2800  on core  27\n",
      "3300 3400  on core  33\n",
      "Finished window  700 800  on core  7\n",
      "Finished window  3100 3200  on core  31Finished window  3000 3100  on core  30\n",
      "\n",
      "Finished window  3200 3300  on core  Finished window  1300 1400  on core  13\n",
      "Finished window  1500 1600  on core  32\n",
      "Finished window  3800 390015\n",
      "  on core  Finished window  900 1000  on core  9\n",
      "38Finished window \n",
      " 2600 2700  on core  26\n",
      "Finished window Finished window  3900 4000  on core  39\n",
      "Finished window  200 300  on core   2100 2200  on core  212Finished window  800 900  on core Finished window  1700 1800  on core \n",
      " 8\n",
      " 17\n",
      "\n",
      "Finished window  1900 2000  on core  19\n",
      "Finished window  1600 1700  on core  16\n",
      "Finished window  3500Finished window  3600 3700  on core   3600  on core  35\n",
      "36\n",
      "Finished window  2400 2500  on core  24\n",
      "Finished window  3700 3800  on core  37\n",
      "Finished window  3400 Finished window  1800 1900  on core  18\n",
      "3500  on core Finished window  2900  34Finished window  2000 2100  on core  20\n",
      "\n",
      "3000  on core  29\n",
      "Finished window  2200 2300  on core  22\n",
      "Finished window  2800 2900  on core  28\n"
     ]
    }
   ],
   "source": [
    "def generate_windows(soffset, eoffset, core):\n",
    "    i = soffset\n",
    "    windows = []\n",
    "    t_start = time.time()\n",
    "    \n",
    "    while i < eoffset:\n",
    "        window = []\n",
    "        for j in range(0, window_size):\n",
    "            window_part = eventlog_df.iloc[[i+j]].drop(columns=[\"__case_id\"]).add_prefix(\"w{0}!!\".format(j))\n",
    "            window_part.reset_index(drop=True, inplace=True)\n",
    "            window.append(window_part)\n",
    "            \n",
    "        window = pd.concat(window, axis=1)\n",
    "        window_part_width = int(window.shape[1] / window_size)\n",
    "        \n",
    "        if((window.iloc[0,window_part_width:] == \"<bos>\").any()):\n",
    "            i += window_size-2\n",
    "            continue\n",
    "            \n",
    "        i += 1\n",
    "        windows.append(window)\n",
    "    \n",
    "    thread_results[core] = windows\n",
    "    print(\"Finished window [{0},{1}] on core {2} in {3}\".format(soffset, eoffset, core, t_start - time.time()))\n",
    "\n",
    "# generating the windows takes incredibly long, speed up via parallel processing here\n",
    "corecount = multiprocessing.cpu_count()\n",
    "threads  = [None] * corecount\n",
    "thread_results = [None] * corecount\n",
    "chunk_sz = 100\n",
    "window_size = 5\n",
    "\n",
    "for core in range(0, corecount):\n",
    "    t = threading.Thread(target=generate_windows, args=(core*chunk_sz, (core+1)*chunk_sz, core))\n",
    "    threads[core] = t\n",
    "    \n",
    "[t.start() for t in threads]\n",
    "[t.join()  for t in threads]\n",
    "thread_results = list(itertools.chain.from_iterable(thread_results))\n",
    "thread_results = pd.concat(thread_results, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thread_results.to_csv(data_path.replace(\".xes\", \"_windowed.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enrich with SP2 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/42636765/how-to-set-all-the-values-of-an-existing-pandas-dataframe-to-zero\n",
    "# This one-hot encodes all entries in concept:name column for later incrementation once it has been seen\n",
    "sp2_features = pd.get_dummies(eventlog_df[\"concept:name\"], prefix=\"SP2\")\n",
    "for col in sp2_features.columns: sp2_features[col].values[:] = 0\n",
    "    \n",
    "eventlog_sp2_df = thread_results.copy(deep=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enrich with PrefixSpan features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_patterns(pt):\n",
    "    for p in pt:\n",
    "        print(\"Support: {0}%\".format(100*p[0]/len(event_traces)))\n",
    "        for n in p[1]:\n",
    "            print(\"    > \", int_to_event[n])\n",
    "        print()\n",
    "        \n",
    "events       = list(set(eventlog_df[\"concept:name\"]))\n",
    "event_to_int = dict((c, i) for i,c in enumerate(events))\n",
    "int_to_event = dict((i, c) for i,c in enumerate(events))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prefixspan requires an array of arrays with one subarray for every trace\n",
    "indices = np.where(eventlog_df[\"concept:name\"] == \"<bos>\")[0].tolist()\n",
    "arr     = eventlog_df[\"concept:name\"].map(event_to_int).tolist()\n",
    "event_traces   = np.array_split(arr, indices)[1:] # remove randomly inserted array at the start of this list\n",
    "encoded_traces = [ [e for e in t] for t in event_traces ]\n",
    "prefixspan_traces = PrefixSpan(encoded_traces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: create indices for sequence items here for dictionary encoding\n",
    "ps_topkc = prefixspan_traces.topk(15, closed=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:thesis]",
   "language": "python",
   "name": "conda-env-thesis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
