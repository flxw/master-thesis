{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import multiprocessing, threading\n",
    "import math\n",
    "import scipy.stats as ss\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "from opyenxes.model.XLog import XLog\n",
    "from opyenxes.data_in.XUniversalParser import XUniversalParser\n",
    "from opyenxes.classification.XEventAttributeClassifier import XEventAttributeClassifier\n",
    "\n",
    "from prefixspan import PrefixSpan\n",
    "\n",
    "from tqdm import *\n",
    "\n",
    "data_path = \"../logs/bpic2011.xes\"\n",
    "traces_picklepath = data_path.replace(\".xes\", \"_raw_traces.pickled\")\n",
    "traces_tmppath = data_path.replace(\".xes\", \"_traces_tmp.pickled\")\n",
    "traces_finalpath = data_path.replace(\".xes\", \"_traces_encoded.pickled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_path) as bpic_file:\n",
    "    eventlog = XUniversalParser().parse(bpic_file)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncores = multiprocessing.cpu_count()\n",
    "ntraces = len(eventlog)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract data trace-wise from XES format and enrich with BOS/EOS markers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect all attributes\n",
    "column_names = []\n",
    "\n",
    "for event in eventlog[0]:\n",
    "    for attribute in event.get_attributes():\n",
    "        column_names.append(attribute)\n",
    "        \n",
    "column_names = set(column_names) # remove duplicates\n",
    "column_names = list(column_names)\n",
    "\n",
    "def create_dataframe_from_trace(t):\n",
    "    df = pd.DataFrame(columns=column_names, index=range(0,len(t)))\n",
    "    for event_idx, event in enumerate(t):\n",
    "        event_attributes = event.get_attributes()\n",
    "        df.iloc[event_idx][\"__case_id\"] = 0\n",
    "\n",
    "        for attribute in event_attributes:\n",
    "            df[attribute].values[event_idx] = event_attributes[attribute].get_value()\n",
    "    \n",
    "    return df\n",
    "\n",
    "ppool = multiprocessing.Pool(ncores)\n",
    "traces = []\n",
    "with tqdm(total=len(eventlog), desc=\"Converting XES traces to Pandas dataframes\", unit=\"traces\") as pbar:\n",
    "    for i, _ in tqdm(enumerate(ppool.imap(create_dataframe_from_trace, eventlog))):\n",
    "        pbar.update()\n",
    "        traces.append(_)\n",
    "        \n",
    "del eventlog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(traces, open(traces_picklepath, \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "traces = pickle.load(open(traces_picklepath, \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eliminate correlated or unimportant features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cramers_v(confusion_matrix):\n",
    "    \"\"\" calculate Cramers V statistic for categorial-categorial association.\n",
    "        uses correction from Bergsma and Wicher,\n",
    "        Journal of the Korean Statistical Society 42 (2013): 323-328\n",
    "        https://stackoverflow.com/questions/46498455/categorical-features-correlation\"\"\"\n",
    "    chi2 = ss.chi2_contingency(confusion_matrix)[0]\n",
    "    n = confusion_matrix.sum()\n",
    "    phi2 = chi2 / n\n",
    "    r, k = confusion_matrix.shape\n",
    "    phi2corr = max(0, phi2 - ((k-1)*(r-1))/(n-1))\n",
    "    rcorr = r - ((r-1)**2)/(n-1)\n",
    "    kcorr = k - ((k-1)**2)/(n-1)\n",
    "    return np.sqrt(phi2corr / min((kcorr-1), (rcorr-1)))\n",
    "\n",
    "for col_a,col_b in itertools.product(eventlog_df.columns, repeat=2): \n",
    "    candidate = pd.crosstab(eventlog_df[col_a], eventlog_df[col_b]).as_matrix()\n",
    "    print(\"{: >30} {: >30} {: >20}\".format(col_a, col_b, cramers_v(candidate)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lifecyle:transition is always \"complete\"\n",
    "# Producer code correlates perfectly with org:group\n",
    "# Activity code correlates perfectly with concept:name\n",
    "for t in traces:\n",
    "    t.drop(columns=[\"lifecycle:transition\", \"Producer code\", \"Activity code\", \"Section\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create standard feature set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "eventlog_df = [None] * len(traces)\n",
    "nattr = len(traces[0].columns)\n",
    "bos_df = pd.DataFrame([(\"<bos>\",)*nattr], columns = traces[0].columns)\n",
    "eos_df = pd.DataFrame([(\"<eos>\",)*nattr], columns = traces[0].columns)\n",
    "\n",
    "# Do not enrich with BOS,EOS features for now\n",
    "# for i in range(0,len(traces)):\n",
    "#     traces[i] = pd.concat([bos_df, traces[i], eos_df], ignore_index=True)\n",
    "\n",
    "eventlog_df = pd.concat(traces, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create SP2 feature set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/42636765/how-to-set-all-the-values-of-an-existing-pandas-dataframe-to-zero\n",
    "# This one-hot encodes all entries in concept:name column for later incrementation once it has been seen\n",
    "# sp2_features = pd.get_dummies(eventlog_df[\"concept:name\"], prefix=\"SP2\") # can't use windowed representation here as it might skew distribution of values\n",
    "# eventlog_sp2_df = process_results.copy(deep=True)\n",
    "# sp2_features    = sp2_features.drop(sp2_features.index[sp2_features.index[len(eventlog_sp2_df):]])\n",
    "# assert(len(sp2_features) == len(eventlog_sp2_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Enriching traces with SP2 features:   0%|          | 0/1143 [00:00<?, ?traces/s]\n",
      "Enriching traces with SP2 features:   0%|          | 1/1143 [00:00<04:48,  3.96traces/s]\n",
      "Enriching traces with SP2 features:   1%|          | 13/1143 [00:00<03:24,  5.54traces/s]\n",
      "Enriching traces with SP2 features:   6%|▋         | 73/1143 [00:00<02:16,  7.84traces/s]\n",
      "Enriching traces with SP2 features:  10%|█         | 117/1143 [00:00<01:32, 11.11traces/s]\n",
      "Enriching traces with SP2 features:  14%|█▍        | 161/1143 [00:00<01:02, 15.60traces/s]\n",
      "Enriching traces with SP2 features:  18%|█▊        | 210/1143 [00:01<00:42, 21.85traces/s]\n",
      "Enriching traces with SP2 features:  21%|██        | 242/1143 [00:01<00:29, 30.09traces/s]\n",
      "Enriching traces with SP2 features:  25%|██▌       | 286/1143 [00:01<00:20, 41.49traces/s]\n",
      "Enriching traces with SP2 features:  29%|██▊       | 328/1143 [00:01<00:14, 56.76traces/s]\n",
      "328it [00:01, 59.27it/s]\u001b[A\n",
      "Enriching traces with SP2 features:  34%|███▍      | 389/1143 [00:01<00:08, 86.52traces/s]\n",
      "Enriching traces with SP2 features:  38%|███▊      | 434/1143 [00:01<00:06, 114.08traces/s]\n",
      "Enriching traces with SP2 features:  41%|████      | 465/1143 [00:01<00:05, 130.10traces/s]\n",
      "Enriching traces with SP2 features:  43%|████▎     | 493/1143 [00:02<00:04, 153.33traces/s]\n",
      "493it [00:02, 152.67it/s]\u001b[A\n",
      "Enriching traces with SP2 features:  48%|████▊     | 544/1143 [00:02<00:03, 154.92traces/s]\n",
      "Enriching traces with SP2 features:  50%|████▉     | 566/1143 [00:02<00:05, 109.16traces/s]\n",
      "Enriching traces with SP2 features:  54%|█████▎    | 614/1143 [00:02<00:03, 138.42traces/s]\n",
      "Enriching traces with SP2 features:  57%|█████▋    | 650/1143 [00:03<00:03, 154.82traces/s]\n",
      "Enriching traces with SP2 features:  59%|█████▉    | 674/1143 [00:03<00:03, 151.88traces/s]\n",
      "Enriching traces with SP2 features:  62%|██████▏   | 714/1143 [00:03<00:02, 180.32traces/s]\n",
      "Enriching traces with SP2 features:  65%|██████▌   | 743/1143 [00:03<00:01, 201.06traces/s]\n",
      "Enriching traces with SP2 features:  70%|██████▉   | 797/1143 [00:03<00:01, 233.82traces/s]\n",
      "Enriching traces with SP2 features:  72%|███████▏  | 826/1143 [00:03<00:01, 208.03traces/s]\n",
      "Enriching traces with SP2 features:  75%|███████▍  | 852/1143 [00:03<00:01, 219.28traces/s]\n",
      "Enriching traces with SP2 features:  78%|███████▊  | 891/1143 [00:03<00:01, 250.93traces/s]\n",
      "Enriching traces with SP2 features:  80%|████████  | 920/1143 [00:04<00:01, 218.97traces/s]\n",
      "Enriching traces with SP2 features:  84%|████████▍ | 960/1143 [00:04<00:00, 249.65traces/s]\n",
      "960it [00:04, 246.49it/s]\u001b[A\n",
      "Enriching traces with SP2 features:  87%|████████▋ | 989/1143 [00:04<00:00, 156.14traces/s]\n",
      "Enriching traces with SP2 features:  93%|█████████▎| 1060/1143 [00:04<00:00, 203.62traces/s]\n",
      "Enriching traces with SP2 features:  96%|█████████▌| 1097/1143 [00:04<00:00, 207.97traces/s]\n",
      "Enriching traces with SP2 features: 100%|██████████| 1143/1143 [00:04<00:00, 230.66traces/s]\n"
     ]
    }
   ],
   "source": [
    "# loop through every trace and encode the presence of an activity\n",
    "sp2_prefix = \"SP2_\"\n",
    "activity_labels = [ \"{0}{1}\".format(sp2_prefix,a) for a in eventlog_df[\"concept:name\"].unique() ]\n",
    "\n",
    "def enrich_trace_with_sp2(t):\n",
    "    sp2_df = pd.DataFrame(columns=activity_labels, index=range(0,len(t)), dtype=np.bool)\n",
    "    for col in sp2_df.columns: sp2_df[col].values[:] = 0\n",
    "    sp2_df[\"{0}{1}\".format(sp2_prefix, t[\"concept:name\"][0])].values[0]  = 1\n",
    "    \n",
    "    for i in range(1,len(t)):\n",
    "        first_activity_name = t[\"concept:name\"].iloc[i]\n",
    "        col = \"{0}{1}\".format(sp2_prefix,first_activity_name)\n",
    "        \n",
    "        sp2_df.values[i] = sp2_df.values[i-1]\n",
    "        sp2_df[col].values[i] = 1\n",
    "        \n",
    "    return pd.concat([t, sp2_df], axis=1)\n",
    "\n",
    "ppool = multiprocessing.Pool(ncores)\n",
    "ttraces = []\n",
    "with tqdm(total=len(traces), desc=\"Enriching traces with SP2 features\", unit=\"traces\") as pbar:\n",
    "    for i, _ in tqdm(enumerate(ppool.imap(enrich_trace_with_sp2, traces))):\n",
    "        pbar.update()\n",
    "        ttraces.append(_)\n",
    "        \n",
    "traces = ttraces\n",
    "del ttraces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enrich with PrefixSpan features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_patterns(pt):\n",
    "    for p in pt:\n",
    "        print(\"Support: {0}%\".format(100*p[0]/len(traces)))\n",
    "        for n in p[1]:\n",
    "            print(\"    > \", int_to_event[n])\n",
    "        print()\n",
    "\n",
    "# since most patterns begin and end with the <eos> and <bos> markers, the features only become valuable towards the end...\n",
    "events       = eventlog_df[\"concept:name\"].unique()\n",
    "event_to_int = dict((c, i) for i,c in enumerate(events) if c not in [\"<bos>\",\"<eos>\"])\n",
    "int_to_event = dict((i, c) for i,c in enumerate(events) if c not in [\"<bos>\",\"<eos>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traces = save_traces[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_traces = traces[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prefixspan requires an array of arrays with one subarray for every trace\n",
    "encoded_traces = [ t[\"concept:name\"].map(event_to_int).tolist() for t in traces ]\n",
    "prefixspan_traces = PrefixSpan(encoded_traces)\n",
    "closed_sequences = prefixspan_traces.topk(25, closed=True) # support is how often the subsequence appears in total\n",
    "# http://sequenceanalysis.github.io/slides/analyzing_sequential_user_behavior_part2.pdf, slide 5\n",
    "# print_patterns(ps_topkc)\n",
    "\n",
    "# only take subsequence which are at a certain level of support? like if ss[0]/len(traces) < .90\n",
    "#ps_topkc = list(filter(lambda x: x[0]/len(traces) > .90, ps_topkc))\n",
    "closed_sequences = [ p[1] for p in closed_sequences ]\n",
    "ptraces = [ (t, closed_sequences[:], event_to_int) for t in traces ] # enrich traces with copy of mined subsequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Enriching traces with mined subsequence features:   0%|          | 0/1143 [00:00<?, ?traces/s]\n",
      "Enriching traces with mined subsequence features:   1%|          | 8/1143 [00:00<00:15, 74.14traces/s]\n",
      "Enriching traces with mined subsequence features:   2%|▏         | 25/1143 [00:00<00:12, 88.85traces/s]\n",
      "Enriching traces with mined subsequence features:   5%|▍         | 55/1143 [00:00<00:09, 111.22traces/s]\n",
      "Enriching traces with mined subsequence features:   6%|▋         | 73/1143 [00:00<00:08, 121.65traces/s]\n",
      "Enriching traces with mined subsequence features:   8%|▊         | 87/1143 [00:00<00:08, 126.59traces/s]\n",
      "Enriching traces with mined subsequence features:  10%|█         | 118/1143 [00:00<00:06, 151.45traces/s]\n",
      "Enriching traces with mined subsequence features:  14%|█▍        | 164/1143 [00:00<00:05, 186.28traces/s]\n",
      "Enriching traces with mined subsequence features:  17%|█▋        | 192/1143 [00:00<00:04, 204.53traces/s]\n",
      "Enriching traces with mined subsequence features:  20%|██        | 229/1143 [00:00<00:03, 234.48traces/s]\n",
      "Enriching traces with mined subsequence features:  23%|██▎       | 258/1143 [00:01<00:03, 248.61traces/s]\n",
      "Enriching traces with mined subsequence features:  26%|██▌       | 292/1143 [00:01<00:06, 129.40traces/s]\n",
      "Enriching traces with mined subsequence features:  29%|██▊       | 328/1143 [00:01<00:05, 159.14traces/s]\n",
      "Enriching traces with mined subsequence features:  31%|███       | 354/1143 [00:01<00:04, 176.95traces/s]\n",
      "Enriching traces with mined subsequence features:  33%|███▎      | 380/1143 [00:02<00:04, 166.12traces/s]\n",
      "Enriching traces with mined subsequence features:  41%|████      | 465/1143 [00:02<00:03, 185.71traces/s]\n",
      "Enriching traces with mined subsequence features:  45%|████▌     | 518/1143 [00:02<00:02, 228.50traces/s]\n",
      "Enriching traces with mined subsequence features:  48%|████▊     | 551/1143 [00:02<00:02, 224.25traces/s]\n",
      "Enriching traces with mined subsequence features:  51%|█████     | 581/1143 [00:02<00:02, 211.02traces/s]\n",
      "Enriching traces with mined subsequence features:  53%|█████▎    | 608/1143 [00:02<00:02, 224.10traces/s]\n",
      "Enriching traces with mined subsequence features:  58%|█████▊    | 664/1143 [00:02<00:01, 271.34traces/s]\n",
      "Enriching traces with mined subsequence features:  61%|██████    | 699/1143 [00:03<00:01, 276.37traces/s]\n",
      "Enriching traces with mined subsequence features:  64%|██████▍   | 732/1143 [00:03<00:01, 218.88traces/s]\n",
      "Enriching traces with mined subsequence features:  69%|██████▉   | 789/1143 [00:03<00:01, 268.08traces/s]\n",
      "789it [00:03, 272.77it/s]\u001b[A\n",
      "Enriching traces with mined subsequence features:  72%|███████▏  | 826/1143 [00:03<00:01, 191.78traces/s]\n",
      "Enriching traces with mined subsequence features:  75%|███████▍  | 855/1143 [00:03<00:01, 207.69traces/s]\n",
      "Enriching traces with mined subsequence features:  80%|███████▉  | 910/1143 [00:03<00:00, 248.64traces/s]\n",
      "Enriching traces with mined subsequence features:  83%|████████▎ | 944/1143 [00:04<00:00, 232.84traces/s]\n",
      "Enriching traces with mined subsequence features:  85%|████████▌ | 974/1143 [00:04<00:00, 233.95traces/s]\n",
      "Enriching traces with mined subsequence features:  88%|████████▊ | 1002/1143 [00:04<00:00, 231.02traces/s]\n",
      "Enriching traces with mined subsequence features:  90%|█████████ | 1032/1143 [00:04<00:00, 243.75traces/s]\n",
      "Enriching traces with mined subsequence features:  93%|█████████▎| 1062/1143 [00:04<00:00, 252.94traces/s]\n",
      "Enriching traces with mined subsequence features:  96%|█████████▌| 1098/1143 [00:04<00:00, 277.11traces/s]\n",
      "Enriching traces with mined subsequence features: 100%|██████████| 1143/1143 [00:04<00:00, 239.93traces/s]\n"
     ]
    }
   ],
   "source": [
    "def wrapped__enrich_trace_with_subseq(args):\n",
    "    return enrich_trace_with_subseq(*args)\n",
    "\n",
    "def enrich_trace_with_subseq(t, ps, event_to_int):\n",
    "    col_prefix = \"PFS_\"\n",
    "    subseq_labels = [ \"{0}{1}\".format(col_prefix,ss_idx) for ss_idx, ss in enumerate(ps) ]\n",
    "    subseq_df = pd.DataFrame(columns=subseq_labels, index=range(0,len(t)), dtype=np.bool)\n",
    "    \n",
    "    for col in subseq_df.columns: subseq_df[col].values[:] = 0\n",
    "    for i in range(0,len(t)): # loop through sequence, prune items from mined sequences, and once a subsequence array is empty, this subsequence has occured :)\n",
    "        activity_code = event_to_int.get(t[\"concept:name\"].iloc[i], None)\n",
    "        \n",
    "        for subseq_idx in range(0,len(ps)):\n",
    "            if ps[subseq_idx] == []:\n",
    "                continue\n",
    "            if ps[subseq_idx][0] == activity_code:\n",
    "                ps[subseq_idx].pop(0)\n",
    "                if ps[subseq_idx] == []:\n",
    "                    subseq_df.values[i:,subseq_idx] = 1\n",
    "        \n",
    "    return pd.concat([t, subseq_df], axis=1)\n",
    "\n",
    "ppool = multiprocessing.Pool(ncores)\n",
    "ttraces = []\n",
    "\n",
    "with tqdm(total=len(traces), desc=\"Enriching traces with mined subsequence features\", unit=\"traces\") as pbar:\n",
    "    for i, _ in tqdm(enumerate(ppool.imap(wrapped__enrich_trace_with_subseq, ptraces))):\n",
    "        pbar.update()\n",
    "        ttraces.append(_)\n",
    "        \n",
    "traces = ttraces\n",
    "del ttraces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(traces, open(traces_tmppath, \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "traces = pickle.load(open(traces_tmppath, \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do basic encoding of remaining features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert timestamp to total running time\n",
    "bos_idx = 0\n",
    "for i in range(0, len(traces)):\n",
    "    tlen = len(traces[i])-1\n",
    "    dfs = traces[i][\"time:timestamp\"] - traces[i][\"time:timestamp\"][bos_idx]\n",
    "    traces[i][\"time:timestamp\"] = dfs.map(lambda d: d.total_seconds())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do one-hot encoding for concept:name and Specialism code\n",
    "dummy_columns = [\"concept:name\", \"org:group\"]\n",
    "eventlog_df_dummies = pd.get_dummies(eventlog_df[dummy_columns], dtype=np.bool)\n",
    "trace_offset = 0\n",
    "\n",
    "for i in range(0,len(traces)):\n",
    "    traces[i].drop(columns=dummy_columns, inplace = True)\n",
    "    traces[i] = pd.concat([eventlog_df_dummies[trace_offset:len(traces[i])], traces[i]], ignore_index=False, axis=1)\n",
    "    trace_offset += len(traces[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(traces)):\n",
    "    traces[i][\"Specialism code\"] = pd.to_numeric(traces[i][\"Specialism code\"], errors=\"ignore\")\n",
    "    traces[i][\"Number of executions\"] = pd.to_numeric(traces[i][\"Number of executions\"], errors=\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(traces, open(traces_finalpath, \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of encoded data: 14750973.588867188 KB\n",
      "Memory usage of encoded data: 14405.247645378113 MB\n",
      "Memory usage of encoded data: 14.067624653689563 GB\n"
     ]
    }
   ],
   "source": [
    "used_bytes = sum(traces[0].memory_usage(index=True, deep=True)) * sum([len(t) for t in traces])\n",
    "print(\"Memory usage of encoded data: {0} KB\".format(used_bytes / 1024))\n",
    "print(\"Memory usage of encoded data: {0} MB\".format(used_bytes / 1024**2))\n",
    "print(\"Memory usage of encoded data: {0} GB\".format(used_bytes / 1024**3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:thesis]",
   "language": "python",
   "name": "conda-env-thesis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
