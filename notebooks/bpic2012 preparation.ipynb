{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import multiprocessing\n",
    "import math\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import re\n",
    "import copy\n",
    "from keras.utils import np_utils\n",
    "\n",
    "from opyenxes.model.XLog import XLog\n",
    "from opyenxes.data_in.XUniversalParser import XUniversalParser\n",
    "from opyenxes.classification.XEventAttributeClassifier import XEventAttributeClassifier\n",
    "\n",
    "from prefixspan import PrefixSpan\n",
    "\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### configuration\n",
    "data_path = \"../logs/bpic2011.xes\"\n",
    "traces_picklepath  = data_path.replace(\".xes\", \"_raw_traces.pickled\")\n",
    "traces_dictionarypath = data_path.replace(\".xes\", \"_dictionaries.pickled\")\n",
    "target_column = \"concept:name\"\n",
    "categorical_feature_names = [\"concept:name\", \"Section\", \"org:group\"]\n",
    "eosmarker = \"<EOS>\"\n",
    "### configuration end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_path) as bpic_file:\n",
    "    eventlog = XUniversalParser().parse(bpic_file)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncores = multiprocessing.cpu_count()\n",
    "ntraces = len(eventlog)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract data trace-wise from XES format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect all attributes\n",
    "column_names = []\n",
    "\n",
    "for event in eventlog[0]:\n",
    "    for attribute in event.get_attributes():\n",
    "        column_names.append(attribute)\n",
    "        \n",
    "column_names = set(column_names) # remove duplicates\n",
    "column_names = list(column_names)\n",
    "\n",
    "def create_dataframe_from_trace(t):\n",
    "    df = pd.DataFrame(columns=column_names, index=range(0,len(t)))\n",
    "    for event_idx, event in enumerate(t):\n",
    "        event_attributes = event.get_attributes()\n",
    "        df.iloc[event_idx][\"__case_id\"] = 0\n",
    "        \n",
    "        for attribute in event_attributes:\n",
    "            df[attribute].values[event_idx] = event_attributes[attribute].get_value()\n",
    "    \n",
    "    return df\n",
    "\n",
    "ppool = multiprocessing.Pool(ncores)\n",
    "traces = []\n",
    "\n",
    "for _ in tqdm_notebook(ppool.imap(create_dataframe_from_trace, eventlog),\n",
    "                       total=len(eventlog),\n",
    "                       unit=\"traces\"):\n",
    "        traces.append(_)\n",
    "\n",
    "ppool.close()\n",
    "del eventlog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(traces, open(traces_picklepath, \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traces = pickle.load(open(traces_picklepath, \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create complete feature set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set data types for columns that were not autodetected\n",
    "# for i in range(0,len(traces)):\n",
    "#     traces[i][\"Specialism code\"] = pd.to_numeric(traces[i][\"Specialism code\"], errors=\"ignore\")\n",
    "#     traces[i][\"Number of executions\"] = pd.to_numeric(traces[i][\"Number of executions\"], errors=\"ignore\")\n",
    "\n",
    "eventlog_df = pd.concat(traces, ignore_index=True)\n",
    "\n",
    "for col in eventlog_df.columns:\n",
    "    if eventlog_df[col].nunique() == 1:\n",
    "        for t in traces:\n",
    "            t.drop(columns=[col], inplace=True)\n",
    "        eventlog_df.drop(columns=[col], inplace=True)\n",
    "        print(\"Dropped column because it contains only a single value:\", col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eliminate correlated or unimportant features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cramers_v(confusion_matrix):\n",
    "    \"\"\" calculate Cramers V statistic for categorial-categorial association.\n",
    "        uses correction from Bergsma and Wicher,\n",
    "        Journal of the Korean Statistical Society 42 (2013): 323-328\n",
    "        https://stackoverflow.com/questions/46498455/categorical-features-correlation\"\"\"\n",
    "    chi2 = ss.chi2_contingency(confusion_matrix)[0]\n",
    "    n = confusion_matrix.sum()\n",
    "    phi2 = chi2 / n\n",
    "    r, k = confusion_matrix.shape\n",
    "    phi2corr = max(0, phi2 - ((k-1)*(r-1))/(n-1))\n",
    "    rcorr = r - ((r-1)**2)/(n-1)\n",
    "    kcorr = k - ((k-1)**2)/(n-1)\n",
    "    return np.sqrt(phi2corr / min((kcorr-1), (rcorr-1)))\n",
    "\n",
    "cramers_v_df = pd.DataFrame(index=eventlog_df.columns,\n",
    "                            columns=eventlog_df.columns,\n",
    "                            dtype=np.float)\n",
    "\n",
    "for col_a,col_b in itertools.product(eventlog_df.columns, repeat=2):   \n",
    "    if col_a == col_b:\n",
    "        cramers_v_df[col_a][col_b] = 1\n",
    "    else:\n",
    "        candidate = pd.crosstab(eventlog_df[col_a], eventlog_df[col_b]).as_matrix()\n",
    "        cramers_v_df[col_a][col_b] = cramers_v(candidate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(cramers_v_df, annot=True, linewidths=.5, cmap=\"YlGnBu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lifecyle:transition is always \"complete\"\n",
    "# Producer code correlates perfectly with org:group\n",
    "# Activity code correlates perfectly with concept:name\n",
    "for t in traces:\n",
    "    t.drop(columns=[\"Producer code\", \"Activity code\", \"Specialism code\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert timestamps to relative scale in hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert timestamp to total running time in hours\n",
    "bos_idx = 0\n",
    "for i in range(0, len(traces)):\n",
    "    tlen = len(traces[i])-1\n",
    "    dfs = traces[i][\"time:timestamp\"] - traces[i][\"time:timestamp\"][bos_idx]\n",
    "    traces[i][\"time:timestamp\"] = dfs.map(lambda d: int(d.total_seconds()/(60*60)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dictionary encoding for categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_dict = {}\n",
    "for cf in categorical_feature_names:\n",
    "    cf_dict = { 'to_int': {}, 'to_cat': {} }\n",
    "    events = eventlog_df[cf].unique().tolist()\n",
    "    if cf == target_column: events.append(eosmarker)\n",
    "    cf_dict['to_int'] = dict((c, i) for i, c in enumerate(events))\n",
    "    cf_dict['to_cat'] = dict((i, c) for i, c in enumerate(events))\n",
    "    feature_dict[cf] = cf_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create SP2 feature set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through every trace and encode the presence of an activity\n",
    "sp2_prefix = \"SP2_\"\n",
    "activity_labels = [ \"{0}{1}\".format(sp2_prefix,a) for a in eventlog_df[\"concept:name\"].unique() ]\n",
    "\n",
    "def enrich_trace_with_sp2(t):\n",
    "    sp2_df = pd.DataFrame(columns=activity_labels, index=range(0,len(t)), dtype=np.bool)\n",
    "    for col in sp2_df.columns: sp2_df[col].values[:] = 0\n",
    "    sp2_df[\"{0}{1}\".format(sp2_prefix, t[\"concept:name\"][0])].values[0]  = 1\n",
    "    \n",
    "    for i in range(1,len(t)):\n",
    "        first_activity_name = t[\"concept:name\"].iloc[i]\n",
    "        col = \"{0}{1}\".format(sp2_prefix,first_activity_name)\n",
    "        \n",
    "        sp2_df.values[i] = sp2_df.values[i-1]\n",
    "        sp2_df[col].values[i] = 1\n",
    "        \n",
    "    return sp2_df\n",
    "\n",
    "ppool = multiprocessing.Pool(ncores)\n",
    "sp2_traces = []\n",
    "\n",
    "for _ in tqdm_notebook(ppool.imap(enrich_trace_with_sp2, traces),\n",
    "                       total=len(traces),\n",
    "                       unit=\"traces\"):\n",
    "        sp2_traces.append(_)\n",
    "        \n",
    "ppool.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create PrefixSpan feature set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prefixspan requires an array of arrays with one subarray for every trace\n",
    "encoded_traces = [ t[target_column].map(feature_dict[target_column]['to_int']).tolist() for t in traces ]\n",
    "prefixspan_traces = PrefixSpan(encoded_traces)\n",
    "closed_sequences = prefixspan_traces.topk(25, closed=True) # support is how often the subsequence appears in total\n",
    "# http://sequenceanalysis.github.io/slides/analyzing_sequential_user_behavior_part2.pdf, slide 5\n",
    "\n",
    "# only take subsequence which are at a certain level of support? like if ss[0]/len(traces) < .90\n",
    "#ps_topkc = list(filter(lambda x: x[0]/len(traces) > .90, ps_topkc))\n",
    "closed_sequences = [ p[1] for p in closed_sequences ]\n",
    "pftrace_args = [ (t, closed_sequences[:], feature_dict[target_column]['to_int']) for t in traces ] # enrich traces with copy of mined subsequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrapped__enrich_trace_with_subseq(args):\n",
    "    return enrich_trace_with_subseq(*args)\n",
    "\n",
    "def enrich_trace_with_subseq(t, ps, event_to_int):\n",
    "    col_prefix = \"PFS_\"\n",
    "    subseq_labels = [ \"{0}{1}\".format(col_prefix,ss_idx) for ss_idx, ss in enumerate(ps) ]\n",
    "    subseq_df = pd.DataFrame(columns=subseq_labels, index=range(0,len(t)), dtype=np.bool)\n",
    "    \n",
    "    for col in subseq_df.columns: subseq_df[col].values[:] = 0\n",
    "    for i in range(0,len(t)): # loop through sequence, prune items from mined sequences, and once a subsequence array is empty, this subsequence has occured :)\n",
    "        activity_code = event_to_int.get(t[\"concept:name\"].iloc[i], None)\n",
    "        \n",
    "        for subseq_idx in range(0,len(ps)):\n",
    "            if ps[subseq_idx] == []:\n",
    "                continue\n",
    "            if ps[subseq_idx][0] == activity_code:\n",
    "                ps[subseq_idx].pop(0)\n",
    "                if ps[subseq_idx] == []:\n",
    "                    subseq_df.values[i:,subseq_idx] = 1\n",
    "        \n",
    "    return subseq_df\n",
    "\n",
    "ppool = multiprocessing.Pool(ncores)\n",
    "pf_traces = []\n",
    "        \n",
    "for _ in tqdm_notebook(ppool.imap(wrapped__enrich_trace_with_subseq, pftrace_args),\n",
    "                       total=len(pftrace_args),\n",
    "                       unit=\"traces\"):\n",
    "        pf_traces.append(_)\n",
    "        \n",
    "ppool.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and normalize ordinal and categorical feature sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinal_feature_names = traces[0].columns.difference(categorical_feature_names)\n",
    "n_target_classes = max(feature_dict[target_column]['to_int'].values()) + 1\n",
    "final_traces = copy.deepcopy(traces)\n",
    "\n",
    "ordinal_traces = [None] * len(traces)\n",
    "categorical_traces = [None] * len(traces)\n",
    "target_traces = [None] * len(traces)\n",
    "\n",
    "# Concatenate all features into one feature dataframe per trace\n",
    "for i in range(0, len(traces)):\n",
    "    \n",
    "    # Encode categorical features and normalize\n",
    "    for cf in categorical_feature_names:\n",
    "        final_traces[i][cf] = traces[i][cf].map(feature_dict[cf]['to_int'])\n",
    "        final_traces[i][cf] /= max(feature_dict[cf]['to_int'].values())\n",
    "    \n",
    "    # Create TARGET feature column by shifting target column\n",
    "    targets = final_traces[i][target_column].shift(-1).to_frame(\"TARGET\")\n",
    "    targets.values[len(targets)-1] = feature_dict[target_column]['to_int'][eosmarker]\n",
    "    target_traces[i] = pd.DataFrame(np_utils.to_categorical(targets, num_classes=n_target_classes, dtype='bool')).add_prefix(\"TARGET_\")\n",
    "\n",
    "    # Create separate dfs for ordinal and categorical traces\n",
    "    ordinal_traces[i] = final_traces[i][ordinal_feature_names].astype(np.float32)\n",
    "    categorical_traces[i] = final_traces[i][categorical_feature_names].astype(np.float32)\n",
    "    \n",
    "#     final_traces[i] = pd.concat([ordinal_trace,\n",
    "#                                  categorical_trace,\n",
    "#                                  sp2_traces[i],\n",
    "#                                  pf_traces[i],\n",
    "#                                  targets],\n",
    "#                                 ignore_index=False, axis=1)\n",
    "del final_traces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and save training and test sets per variable type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(123); random.shuffle(categorical_traces)\n",
    "random.seed(123); random.shuffle(ordinal_traces)\n",
    "random.seed(123); random.shuffle(target_traces)\n",
    "random.seed(123); random.shuffle(sp2_traces)\n",
    "random.seed(123); random.shuffle(pf_traces)\n",
    "\n",
    "assert(len(ordinal_traces[0]) ==\n",
    "       len(categorical_traces[0]) ==\n",
    "       len(target_traces[0]) ==\n",
    "       len(sp2_traces[0]) ==\n",
    "       len(pf_traces[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_trace_dataset(dataset, purpose='categorical', ttype='test'):\n",
    "    suffix = \"_{0}_{1}.pickled\".format(purpose, ttype)\n",
    "    p = data_path.replace(\".xes\", suffix)\n",
    "    pickle.dump(dataset, open(p, \"wb\"), protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "set_sep_idx = 900\n",
    "save_trace_dataset(feature_dict, 'mapping', 'dict',)\n",
    "\n",
    "save_trace_dataset(ordinal_traces[:set_sep_idx], 'ordinal', 'train')\n",
    "save_trace_dataset(categorical_traces[:set_sep_idx], 'categorical', 'train')\n",
    "save_trace_dataset(sp2_traces[:set_sep_idx], 'sp2', 'train')\n",
    "save_trace_dataset(pf_traces[:set_sep_idx], 'pfs', 'train')\n",
    "\n",
    "save_trace_dataset(ordinal_traces[set_sep_idx:], 'ordinal', 'test')\n",
    "save_trace_dataset(categorical_traces[set_sep_idx:], 'categorical', 'test')\n",
    "save_trace_dataset(sp2_traces[set_sep_idx:], 'sp2', 'test')\n",
    "save_trace_dataset(pf_traces[set_sep_idx:], 'pfs', 'test')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:thesis]",
   "language": "python",
   "name": "conda-env-thesis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
