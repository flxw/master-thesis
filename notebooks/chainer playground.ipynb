{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import chainer\n",
    "from chainer.backends import cuda\n",
    "from chainer import Function, gradient_check, report, training, utils, Variable\n",
    "from chainer import datasets, iterators, optimizers, serializers\n",
    "from chainer import Link, Chain, ChainList\n",
    "import chainer.functions as F\n",
    "import chainer.links as L\n",
    "from chainer.training import extensions\n",
    "import random\n",
    "import itertools\n",
    "import pandas as pd\n",
    "\n",
    "from opyenxes.model.XLog import XLog\n",
    "from opyenxes.data_in.XUniversalParser import XUniversalParser\n",
    "from opyenxes.classification.XEventAttributeClassifier import XEventAttributeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up chainer on a single GPU for now\n",
    "device_id = -1\n",
    "try:\n",
    "    cuda.check_cuda_available()\n",
    "    cuda.get_device(device_id).use()\n",
    "    import cupy\n",
    "    print(\"running on GPU, switching numpy for cupy!\")\n",
    "    xp = cupy\n",
    "    device_id = 0\n",
    "except:\n",
    "    xp = np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unknown extension: http://www.xes-standard.org/meta_time.xesext\n",
      "Unknown extension: http://www.xes-standard.org/meta_life.xesext\n",
      "Unknown extension: http://www.xes-standard.org/meta_org.xesext\n",
      "Unknown extension: http://www.xes-standard.org/meta_concept.xesext\n",
      "Unknown extension: http://www.xes-standard.org/meta_3TU.xesext\n",
      "Unknown extension: http://www.xes-standard.org/meta_general.xesext\n"
     ]
    }
   ],
   "source": [
    "bpic_path = \"../logs/bpic2011.xes\"\n",
    "\n",
    "with open(bpic_path) as bpic_file:\n",
    "    eventlog = XUniversalParser().parse(bpic_file)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset and Iterator Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect all attributes\n",
    "attribute_list = []\n",
    "\n",
    "# extract column names from any trace, here the first is used\n",
    "for event in eventlog[0]:\n",
    "    event_attributes = event.get_attributes()\n",
    "    \n",
    "    for attribute in event_attributes:\n",
    "        attribute_list.append(attribute)\n",
    "        \n",
    "attribute_list = set(attribute_list) # remove duplicates\n",
    "column_names   = [\"__case_id\"] + list(attribute_list)\n",
    "\n",
    "log_length    = sum([2+len(t) for t in eventlog])\n",
    "event_indices = range(0, log_length) # total number of entries in log\n",
    "eventlog_df   = pd.DataFrame(columns=column_names, index=event_indices)\n",
    "row_idx       = 0\n",
    "\n",
    "def set_row_value(df, row, colnames, val):\n",
    "    for column in colnames:\n",
    "        df.iloc[row][column] = val\n",
    "\n",
    "for trace_idx, raw_trace in enumerate(eventlog):\n",
    "    # insert start-of-sequence marker\n",
    "    set_row_value(eventlog_df, row_idx, column_names, \"<bos>\")\n",
    "    \n",
    "    for event_idx, event in enumerate(raw_trace):\n",
    "        event_attributes = event.get_attributes()\n",
    "        eventlog_df.iloc[row_idx][\"__case_id\"] = trace_idx\n",
    "        \n",
    "        for attribute in event_attributes:\n",
    "            eventlog_df.iloc[row_idx][attribute] = event_attributes[attribute].get_value()\n",
    "            \n",
    "        row_idx += 1\n",
    "    # finalize trace by inserting end-of-sequence marker    \n",
    "    set_row_value(eventlog_df, row_idx, column_names, \"<eos>\")\n",
    "    row_idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a generator for the generation of every sample\n",
    "def window_features(traces,windowsize):\n",
    "    for trace in traces:\n",
    "        for event_i in range(0, len(trace)-windowsize+1):\n",
    "            encoded_window = [event_to_int[trace[i]] for i in range(event_i, event_i+windowsize)]\n",
    "            yield(encoded_window)\n",
    "            \n",
    "# extract event names and enrich with beginning and end markers\n",
    "event_traces = [[ ev.get_attributes()[\"concept:name\"].get_value() for ev in trace ] for trace in bpic2011_log ]\n",
    "event_traces = [ ['<bos>'] + l + ['<eos>'] for l in event_traces ]\n",
    "\n",
    "# generate word mappings to IDs\n",
    "events       = sorted(list(set(itertools.chain.from_iterable(event_traces)))) \n",
    "event_to_int = dict((c, i) for i,c in enumerate(events))\n",
    "int_to_event = dict((i, c) for i,c in enumerate(events))\n",
    "\n",
    "# shuffle complete traces and create test and training set\n",
    "random.shuffle(event_traces)\n",
    "train_traces = event_traces[:int(.8*len(event_traces))]\n",
    "test_traces  = event_traces[int(.8*len(event_traces)):]\n",
    "\n",
    "# from these sets, create feature windows for learning\n",
    "trace_dt = xp.float32\n",
    "window_size = 5\n",
    "train_traces = xp.array([ w for w in window_features(train_traces, window_size) ], dtype=trace_dt)\n",
    "test_traces  = xp.array([ w for w in window_features(test_traces,  window_size) ],  dtype=trace_dt)\n",
    "\n",
    "#  extract all columns but the last from all rows\n",
    "train_x = train_traces[:, :4]\n",
    "# extract only the last column and put each element into an array of its own\n",
    "train_y = (train_traces[:, 4][:, None]).flatten().astype(xp.int32)\n",
    "\n",
    "#  extract all columns but the last from all rows\n",
    "test_x = test_traces[:, :4]\n",
    "# extract only the last column and put each element into an array of its own\n",
    "test_y = (test_traces[:, 4][:, None]).flatten().astype(xp.int32)\n",
    "\n",
    "train_ds = datasets.TupleDataset(train_x, train_y)\n",
    "test_ds  = datasets.TupleDataset(test_x, test_y)\n",
    "\n",
    "train_iter = chainer.iterators.SerialIterator(train_ds, 100, repeat=True, shuffle=False)\n",
    "test_iter  = chainer.iterators.SerialIterator(test_ds,  100, repeat=False, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HalfLifeModel(Chain):\n",
    "    def __init__(self, vocab_size, dim_embed=33*3, dim1=400, dim2=400, dim3=200, class_size=None):\n",
    "        super(HalfLifeModel, self).__init__()\n",
    "        if class_size is None:\n",
    "            class_size = vocab_size\n",
    "        \n",
    "        # ss = subsequence\n",
    "        # sq = sequence\n",
    "        # co = concatenated\n",
    "        self.sq_embed1    = L.EmbedID(vocab_size, dim_embed)\n",
    "        self.sq_lstm2     = L.LSTM(dim_embed, dim1, forget_bias_init=0)\n",
    "        self.sq_lstm3     = L.LSTM(dim1, dim2, forget_bias_init=0)\n",
    "        \n",
    "        self.ss_embed1 = L.Linear(vocab_size, dim_embed)\n",
    "        self.ss_lin2   = L.Linear(dim_embed, dim1)\n",
    "\n",
    "        self.co_lin1 = L.Linear(dim1+dim2, dim3)\n",
    "        self.co_lin2 = L.Linear(dim3, class_size)\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.dim_embed  = dim_embed\n",
    "        self.loss_var = Variable(xp.zeros((), dtype=np.float32))\n",
    "        self.reset_state()\n",
    "\n",
    "    def __call__(self, x, train):\n",
    "        print(x)\n",
    "        seq_window = x[0]\n",
    "        ss_vector  = x[1]\n",
    "        \n",
    "        x_uni = x_3gram[:,0]\n",
    "        y  = Variable(x_uni, volatile = not train)\n",
    "        y  = self.sq_embed1(y)     \n",
    "        y2 = self.sq_lstm2(y)\n",
    "        y2 = self.sq_lstm3(y2)        \n",
    "\n",
    "        y = Variable(sp2, volatile = not train)\n",
    "        y = self.ss_embed1(y)\n",
    "        y = self.ss_lin2(y)\n",
    "        y3 = F.relu(y)\n",
    "        \n",
    "        y = concat.concat((y2,y3) )\n",
    "        y = self.co_lin1(F.dropout(y, train=train))\n",
    "        y = F.relu(y)\n",
    "        y = self.co_lin2(F.dropout(y, train=train)) \n",
    "        \n",
    "        return y\n",
    "\n",
    "    def reset_state(self):\n",
    "        if self.loss_var is not None:\n",
    "            self.loss_var.unchain_backward()\n",
    "            \n",
    "        self.loss_var = Variable(xp.zeros((), dtype=xp.float32))\n",
    "        self.sq_lstm2.reset_state()\n",
    "        self.sq_lstm3.reset_state()\n",
    "        return\n",
    "    \n",
    "gordon = HalfLifeModel(vocab_size=4, class_size=len(events))\n",
    "model = L.Classifier(gordon, accfun=F.accuracy)\n",
    "optimizer = optimizers.MomentumSGD().setup(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linking Iterator And Optimizer together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "updater = training.StandardUpdater(train_iter, optimizer, device=device_id)\n",
    "trainer = training.Trainer(updater, (1, 'epoch'), out='result')\n",
    "\n",
    "trainer.extend(extensions.LogReport())\n",
    "trainer.extend(extensions.PrintReport(\n",
    "    ['epoch', 'main/loss', 'validation/main/loss',\n",
    "     'main/accuracy', 'validation/main/accuracy', 'elapsed_time']))\n",
    "trainer.extend(extensions.Evaluator(test_iter, model, device=device_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if device_id != -1:\n",
    "    model.to_gpu()\n",
    "\n",
    "trainer.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:thesis]",
   "language": "python",
   "name": "conda-env-thesis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
