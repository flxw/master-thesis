{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Embedding, Input, Reshape, Concatenate, concatenate, Flatten, Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### CONFIGURATION SETUP ####\n",
    "\n",
    "data_path = \"../logs/bpic2011.xes\"\n",
    "traces_finalpath = data_path.replace(\".xes\", \"_traces_encoded.pickled\")\n",
    "traces_dictionarypath = data_path.replace(\".xes\", \"_dictionaries.pickled\")\n",
    "n_sp2_features = 624\n",
    "n_pfs_features = 25\n",
    "\n",
    "traces = pickle.load(open(traces_finalpath, \"rb\"))\n",
    "feature_dict = pickle.load(open(traces_dictionarypath, \"rb\"))\n",
    "\n",
    "### CONFIGURATION SETUP END ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle complete traces and create test and training set\n",
    "random.shuffle(traces)\n",
    "sep_idx = int(0.8*len(traces))\n",
    "\n",
    "# train_traces = traces[:sep_idx]\n",
    "# test_traces  = traces[sep_idx:]\n",
    "# https://github.com/keras-team/keras/issues/3107\n",
    "# Yes, your input should be in this format (sequences, timesteps, dimensions).\n",
    "# So based on your example, your input should be in (None, 8, 2). Your input now is (8,2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the feature indices\n",
    "# data is organized like this: normal features | SP2 features | PFS features | TARGET features\n",
    "trace_columns = traces[0].columns.tolist()\n",
    "trace_columns = list(map(lambda e: bool(re.match('^TARGET$', e)), trace_columns))\n",
    "target_col_start_index = trace_columns.index(True)\n",
    "pfs_col_start_index = target_col_start_index - n_pfs_features\n",
    "sp2_col_start_index  = pfs_col_start_index - n_sp2_features\n",
    "\n",
    "# reshape X to be [samples, time steps, features]\n",
    "# X = np.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "# TODO: normalize\n",
    "# X = X / float(n_vocab)\n",
    "\n",
    "# TODO: cluster categorical features and numerical features...\n",
    "# How to understand keras feature shape requirements: https://github.com/keras-team/keras/issues/2045\n",
    "categorical_vars= traces[0].columns[1:sp2_col_start_index-1]\n",
    "train_traces = []\n",
    "for t in traces[:sep_idx]:\n",
    "    t_dict = {}\n",
    "    t_dict['x'] = [ t.iloc[:, i].values.reshape([-1,1,1]) for i in range(1,sp2_col_start_index-1)]\n",
    "    t_dict['y'] = keras.utils.np_utils.to_categorical(t.iloc[:, target_col_start_index:].values)\n",
    "    train_traces.append(t_dict)\n",
    "\n",
    "# test_x = []\n",
    "# test_y = []\n",
    "# for t in test_traces:\n",
    "#     test_x.append(t.iloc[:, :sp2_col_start_index].values.tolist())\n",
    "#     test_y.append(keras.utils.np_utils.to_categorical(t.iloc[:, target_col_start_index:].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(142, 1, 1)"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_traces[3]['x'][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[2]],\n",
       "\n",
       "       [[2]]])"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_traces[0]['x'][0].reshape([-1,1,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "model_inputs = []\n",
    "\n",
    "# create embedding layers for every input feature\n",
    "for cat_var in categorical_vars :\n",
    "    model = Sequential()\n",
    "    no_of_unique_cat  = len(feature_dict[cat_var]['to_int'])\n",
    "    embedding_size = int(min(np.ceil((no_of_unique_cat)/2), 50 ))\n",
    "    vocab  = no_of_unique_cat+1\n",
    "    \n",
    "    il = Input(shape=(None, 1), name=\"Input_{0}\".format(''.join(c for c in cat_var if c.isalnum())))\n",
    "    model_inputs.append(il)\n",
    "    \n",
    "    model = Embedding(vocab, embedding_size)(il)\n",
    "    model = Reshape(target_shape=(embedding_size,))(model)\n",
    "    models.append(model)\n",
    "    \n",
    "# merge the outputs of the embeddings in a dense layer\n",
    "main_output = concatenate(models)\n",
    "main_output = Dense(len(feature_dict[\"concept:name\"][\"to_int\"]), activation='sigmoid', name='dense_final')(main_output)\n",
    "\n",
    "full_model = Model(inputs=model_inputs, outputs=[main_output])\n",
    "full_model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the checkpoint\n",
    "# filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "# checkpoint = keras.callbacks.ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "# callbacks_list = [checkpoint]\n",
    "\n",
    "for t in train_traces:\n",
    "    full_model.fit(t['x'], t['y'], epochs=2, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_filename = \"weights-improvement-02-3.3127.hdf5\"\n",
    "model.load_weights(model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_index = [ np.argmax(p) for p in prediction ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum([prediction_index[i] == dataY[i] for i in prediction_index]) / len(prediction_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test prefixspan here\n",
    "from prefixspan import PrefixSpan\n",
    "\n",
    "event_sequences = [[ ev.get_attributes()[\"concept:name\"].get_value() for ev in trace ] for trace in bpic2011_log ]\n",
    "translated_sequences = [ [event_to_int[ev] for ev in trace] for trace in event_sequences]\n",
    "\n",
    "ps = PrefixSpan(translated_sequences)\n",
    "\n",
    "print(ps.frequent(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
