{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import multiprocessing\n",
    "\n",
    "from tqdm import *\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Embedding, Input, Reshape, concatenate, Flatten, Activation, LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/job:localhost/replica:0/task:0/device:GPU:0',\n",
       " '/job:localhost/replica:0/task:0/device:GPU:1']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.backend.tensorflow_backend._get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### CONFIGURATION SETUP ####\n",
    "\n",
    "data_path = \"../logs/bpic2011.xes\"\n",
    "traces_finalpath = data_path.replace(\".xes\", \"_traces_encoded.pickled\")\n",
    "traces_dictionarypath = data_path.replace(\".xes\", \"_dictionaries.pickled\")\n",
    "n_sp2_features = 624\n",
    "n_pfs_features = 25\n",
    "\n",
    "traces = pickle.load(open(traces_finalpath, \"rb\"))\n",
    "feature_dict = pickle.load(open(traces_dictionarypath, \"rb\"))\n",
    "\n",
    "### CONFIGURATION SETUP END ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle complete traces and create test and training set\n",
    "random.shuffle(traces)\n",
    "sep_idx = int(0.8*len(traces))\n",
    "\n",
    "# extract the feature indices\n",
    "# data is organized like this: ordinal features | categorical features | SP2 features | PFS features | TARGET features\n",
    "# needed as every of these features will get its own layer\n",
    "feature_names  = traces[0].columns\n",
    "trace_columns = list(map(lambda e: bool(re.match('^TARGET$', e)), feature_names))\n",
    "target_col_start_index = trace_columns.index(True)\n",
    "\n",
    "categorical_feature_names = feature_dict.keys()\n",
    "pfs_col_start_index = target_col_start_index - n_pfs_features\n",
    "sp2_col_start_index = pfs_col_start_index - n_sp2_features\n",
    "cat_col_start_index = sp2_col_start_index - len(categorical_feature_names)\n",
    "\n",
    "ordinal_feature_names     = feature_names[0:cat_col_start_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_input_name(var_name):\n",
    "    return \"input_{0}\".format(''.join(c for c in var_name if c.isalnum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Converting traces to Keras learning data:   0%|          | 0/1143 [00:00<?, ?traces/s]\u001b[A\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "Converting traces to Keras learning data:   0%|          | 3/1143 [00:00<01:13, 15.47traces/s]\u001b[A\n",
      "\n",
      "3it [00:00, 16.47it/s]\u001b[A\u001b[A\n",
      "Converting traces to Keras learning data:   0%|          | 5/1143 [00:00<01:59,  9.51traces/s]\u001b[A\n",
      "\n",
      "5it [00:00,  9.74it/s]\u001b[A\u001b[A\n",
      "Converting traces to Keras learning data:   1%|          | 6/1143 [00:01<06:11,  3.06traces/s]\u001b[A\n",
      "\n",
      "6it [00:01,  3.08it/s]\u001b[A\u001b[A\n",
      "Converting traces to Keras learning data:   1%|          | 9/1143 [00:01<05:03,  3.74traces/s]\u001b[A\n",
      "\n",
      "9it [00:01,  3.76it/s]\u001b[A\u001b[A\n",
      "Converting traces to Keras learning data:   2%|▏         | 25/1143 [00:03<03:57,  4.70traces/s]\u001b[A\n",
      "\n",
      "25it [00:03,  4.71it/s]\u001b[A\u001b[A\n",
      "Converting traces to Keras learning data:  11%|█         | 128/1143 [00:03<02:31,  6.69traces/s]\u001b[A\n",
      "\n",
      "128it [00:03,  6.71it/s]\u001b[A\u001b[A\n",
      "Converting traces to Keras learning data:  13%|█▎        | 149/1143 [00:04<02:04,  7.98traces/s]\u001b[A\n",
      "\n",
      "156it [00:04,  8.34it/s]\u001b[A\u001b[A\n",
      "Converting traces to Keras learning data:  15%|█▌        | 174/1143 [00:07<01:50,  8.74traces/s]\u001b[A\n",
      "\n",
      "176it [00:07,  8.52it/s]\u001b[A\u001b[A\n",
      "Converting traces to Keras learning data:  30%|██▉       | 341/1143 [00:07<01:04, 12.44traces/s]\u001b[A\n",
      "\n",
      "341it [00:07, 12.12it/s]\u001b[A\u001b[A\n",
      "Converting traces to Keras learning data:  34%|███▍      | 388/1143 [00:08<00:48, 15.55traces/s]\u001b[A\n",
      "\n",
      "388it [00:08, 15.19it/s]\u001b[A\u001b[A\n",
      "Converting traces to Keras learning data:  45%|████▍     | 510/1143 [00:10<00:31, 19.83traces/s]\u001b[A\n",
      "\n",
      "510it [00:10, 19.42it/s]\u001b[A\u001b[A\n",
      "Converting traces to Keras learning data:  46%|████▋     | 530/1143 [00:11<00:26, 23.51traces/s]\u001b[A\n",
      "\n",
      "531it [00:11, 23.39it/s]\u001b[A\u001b[A\n",
      "Converting traces to Keras learning data:  52%|█████▏    | 598/1143 [00:12<00:18, 28.87traces/s]\u001b[A\n",
      "\n",
      "598it [00:12, 28.66it/s]\u001b[A\u001b[A\n",
      "Converting traces to Keras learning data:  59%|█████▊    | 671/1143 [00:13<00:14, 31.94traces/s]\u001b[A\n",
      "\n",
      "671it [00:13, 31.78it/s]\u001b[A\u001b[A\n",
      "Converting traces to Keras learning data:  84%|████████▍ | 959/1143 [00:14<00:04, 44.10traces/s]\u001b[A\n",
      "\n",
      "959it [00:14, 43.89it/s]\u001b[A\u001b[A\n",
      "\n",
      "1143it [00:14, 77.63it/s]\u001b[A\u001b[A\n",
      "Converting traces to Keras learning data: 100%|██████████| 1143/1143 [00:14<00:00, 77.52traces/s]\u001b[A"
     ]
    }
   ],
   "source": [
    "# TODO: normalize\n",
    "# X = X / float(n_vocab)\n",
    "def wrapped__create_learning_dicts_from_trace(p):\n",
    "    return create_learning_dicts_from_trace(*p)\n",
    "# reshape X to be [samples, time steps, features]\n",
    "# How to understand keras feature shape requirements: https://github.com/keras-team/keras/issues/2045\n",
    "def create_learning_dicts_from_trace(t, sp2_col_start_index, n_sp2_features, pfs_col_start_index, n_pfs_features, target_col_start_index, feature_names):\n",
    "    t_dict = {'x':[], 'y':[]}\n",
    "    # generate one input sequence for every type of variable\n",
    "    # map every single-step batch in a dictionary that will correspond to input layer names!\n",
    "    for i in range(0, len(t)):\n",
    "        batch_dict = {}\n",
    "\n",
    "        # automatically run through all ordinal and categorical features\n",
    "        for col_idx, col in enumerate(feature_names[:sp2_col_start_index]):\n",
    "            input_name = generate_input_name(col)\n",
    "            batch_dict[input_name] = np.array(t.iloc[i, col_idx], dtype=np.float32).reshape([-1,1])\n",
    "\n",
    "        # create batches for sp2 and pfs2 seperately because of their variable encodings\n",
    "        batch_dict[generate_input_name(\"sp2\")] = np.asarray(t.iloc[i, sp2_col_start_index:pfs_col_start_index], dtype=np.float32).reshape([-1,n_sp2_features])\n",
    "        batch_dict[generate_input_name(\"pfs\")] = np.asarray(t.iloc[i, pfs_col_start_index:target_col_start_index], dtype=np.float32).reshape([-1,n_pfs_features])\n",
    "\n",
    "        t_dict['x'].append(batch_dict)\n",
    "    t_dict['y'] = keras.utils.np_utils.to_categorical(t.iloc[:, target_col_start_index:].values.reshape([-1,1,1]))\n",
    "    return t_dict\n",
    "\n",
    "# test_traces = []\n",
    "# for t in test_traces:\n",
    "#     t_dict = {}\n",
    "#     t_dict['x'] = [ t.iloc[:, i].values.reshape([-1,1,1]).astype(np.float32) for i in range(cat_col_start_index,sp2_col_start_index)]\n",
    "#     t_dict['y'] = keras.utils.np_utils.to_categorical(t.iloc[:, target_col_start_index:].values.reshape([1,-1]))\n",
    "#     test_traces.append(t_dict)\n",
    "\n",
    "ncores = multiprocessing.cpu_count()\n",
    "ppool = multiprocessing.Pool(ncores)\n",
    "train_traces = []\n",
    "traces_for_learning_dicts = [ (t, sp2_col_start_index, n_sp2_features, pfs_col_start_index, n_pfs_features, target_col_start_index, feature_names) for t in traces ]\n",
    "\n",
    "with tqdm(total=len(traces), desc=\"Converting traces to Keras learning data\", unit=\"traces\") as pbar:\n",
    "    for i, _ in tqdm(enumerate(ppool.imap(wrapped__create_learning_dicts_from_trace, traces_for_learning_dicts))):\n",
    "        pbar.update()\n",
    "        train_traces.append(_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "model_inputs = []\n",
    "\n",
    "# forward all ordinal features\n",
    "for ord_var in feature_names[:cat_col_start_index]:\n",
    "    il = Input(batch_shape=(1,1), name=generate_input_name(ord_var))\n",
    "    model = Reshape(target_shape=(1,1,))(il)\n",
    "    model_inputs.append(il)\n",
    "    models.append(model)\n",
    "\n",
    "# create embedding layers for every categorical feature\n",
    "for cat_var in categorical_feature_names :\n",
    "    model = Sequential()\n",
    "    no_of_unique_cat  = len(feature_dict[cat_var]['to_int'])\n",
    "    embedding_size = int(min(np.ceil((no_of_unique_cat)/2), 50 ))\n",
    "    vocab  = no_of_unique_cat+1\n",
    "    \n",
    "    il = Input(batch_shape=(1,1), name=generate_input_name(cat_var))    \n",
    "    model = Embedding(vocab, embedding_size)(il)\n",
    "    model = Reshape(target_shape=(1,embedding_size,))(model)\n",
    "    \n",
    "    model_inputs.append(il)\n",
    "    models.append(model)\n",
    "\n",
    "# create input and embedding for sp2/pfs2 features\n",
    "learn_sp2 = True\n",
    "sequence_embedding = None\n",
    "\n",
    "if learn_sp2:\n",
    "    il = Input(batch_shape=(1,n_sp2_features), name=generate_input_name(\"sp2\"))\n",
    "    model_inputs.append(il)\n",
    "    \n",
    "    no_of_unique_cat = n_sp2_features\n",
    "    embedding_size   = int(min(np.ceil((no_of_unique_cat)/2), 50 ))\n",
    "    vocab  = no_of_unique_cat+1\n",
    "    sequence_embedding = Embedding(vocab, embedding_size)(il)\n",
    "    sequence_embedding = Reshape(target_shape=(il.shape[1].value*embedding_size,))(sequence_embedding)\n",
    "else:\n",
    "    # TODO\n",
    "    pass\n",
    "    \n",
    "# merge the outputs of the embeddings, and everything that belongs to the most recent activity executions\n",
    "main_output = concatenate(models, axis=2)\n",
    "main_output = LSTM(25*32, batch_input_shape=(1,), stateful=True)(main_output) # should be multiple of 32 since it trains faster due to np.float32\n",
    "# main_output = LSTM(25*32, batch_input_shape=(1,25*32), stateful=True)(main_output) # should be multiple of 32 since it trains faster due to np.float32\n",
    "\n",
    "# after LSTM has learned on the sequence, bring in the SP2/PFS features, like in Shibatas paper\n",
    "main_output = concatenate([main_output, sequence_embedding])\n",
    "main_output = Dense(20*32, activation='relu', name='dense_join')(main_output)\n",
    "main_output = Dense(len(feature_dict[\"concept:name\"][\"to_int\"]), activation='sigmoid', name='dense_final')(main_output)\n",
    "\n",
    "full_model = Model(inputs=model_inputs, outputs=[main_output])\n",
    "full_model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 0/1143\n",
      "Training 1/1143\n",
      "Training 2/1143\n",
      "Training 3/1143\n",
      "Training 4/1143\n",
      "accuracy training = nan\n",
      "loss training = 4.703778266906738\n",
      "___________________________________\n"
     ]
    }
   ],
   "source": [
    "# # define the checkpoint\n",
    "# filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "# checkpoint = keras.callbacks.ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "# callbacks_list = [checkpoint]\n",
    "\n",
    "# for t in train_traces:\n",
    "#     full_model.fit(t['x'], t['y'], epochs=10, batch_size=50, callbacks=callbacks_list)\n",
    "#     model.reset_states()\n",
    "\n",
    "for epoch in range(1):\n",
    "    mean_tr_acc = []\n",
    "    mean_tr_loss = []\n",
    "    for t_idx, t in enumerate(train_traces[0:5]):\n",
    "        print(\"Training {0}/{1}\".format(t_idx,len(train_traces)))\n",
    "        for x,y in zip(t['x'],t['y']):\n",
    "            tr_loss = full_model.train_on_batch(x, y)\n",
    "            mean_tr_loss.append(tr_loss)\n",
    "        full_model.reset_states()\n",
    "\n",
    "    print('accuracy training = {}'.format(np.mean(mean_tr_acc)))\n",
    "    print('loss training = {}'.format(np.mean(mean_tr_loss)))\n",
    "    print('___________________________________')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_index = [ np.argmax(p) for p in prediction ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum([prediction_index[i] == dataY[i] for i in prediction_index]) / len(prediction_index)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:thesis]",
   "language": "python",
   "name": "conda-env-thesis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
