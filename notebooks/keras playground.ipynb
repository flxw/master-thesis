{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import multiprocessing\n",
    "import os\n",
    "\n",
    "from tqdm import *\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Embedding, Input, Reshape, concatenate, Flatten, Activation, LSTM, Dropout, Lambda\n",
    "from keras.utils import np_utils\n",
    "\n",
    "# import multi_gpu_utils2 as multi_gpu_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### CONFIGURATION SETUP ####\n",
    "\n",
    "data_path = \"../logs/bpic2011.xes\"\n",
    "traces_finalpath = data_path.replace(\".xes\", \"_traces_encoded.pickled\")\n",
    "traces_dictionarypath = data_path.replace(\".xes\", \"_dictionaries.pickled\")\n",
    "n_sp2_features = 624\n",
    "n_pfs_features = 25\n",
    "\n",
    "traces = pickle.load(open(traces_finalpath, \"rb\"))\n",
    "feature_dict = pickle.load(open(traces_dictionarypath, \"rb\"))\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "### CONFIGURATION SETUP END ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle complete traces and create test and training set\n",
    "random.shuffle(traces)\n",
    "sep_idx = 900 #int(0.8*len(traces))\n",
    "\n",
    "# extract the feature indices\n",
    "# data is organized like this: ordinal features | categorical features | SP2 features | PFS features | TARGET features\n",
    "# needed as every of these features will get its own layer\n",
    "feature_names  = traces[0].columns\n",
    "trace_columns = list(map(lambda e: bool(re.match('^TARGET$', e)), feature_names))\n",
    "target_col_start_index = trace_columns.index(True)\n",
    "\n",
    "categorical_feature_names = feature_dict.keys()\n",
    "pfs_col_start_index = target_col_start_index - n_pfs_features\n",
    "sp2_col_start_index = pfs_col_start_index - n_sp2_features\n",
    "cat_col_start_index = sp2_col_start_index - len(categorical_feature_names)\n",
    "\n",
    "ordinal_feature_names     = feature_names[0:cat_col_start_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_input_name(var_name):\n",
    "    return \"input_{0}\".format(''.join(c for c in var_name if c.isalnum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stagger_sequence(s):\n",
    "    for idx in range(1,len(s)):\n",
    "        x = s[\"concept:name\"][0:idx]\n",
    "        y = s[\"TARGET\"][idx-1]\n",
    "        yield x,y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_33 --> input_shape=(None, None, 1) output_shape=(None, None, 1)\n",
      "embedding_20 --> input_shape=(None, None, 1) output_shape=(None, None, 1, 500)\n",
      "reshape_19 --> input_shape=(None, None, 1, 500) output_shape=(None, None, 500)\n",
      "lstm_25 --> input_shape=(None, None, 500) output_shape=(None, None, 500)\n",
      "lstm_26 --> input_shape=(None, None, 500) output_shape=(None, None, 500)\n",
      "dense_final --> input_shape=(None, None, 500) output_shape=(None, None, 625)\n"
     ]
    }
   ],
   "source": [
    "batch_size = None # None translates to unknown batch size\n",
    "# [samples, time steps, features]\n",
    "il = Input(batch_shape=(batch_size,None,1))\n",
    "# main_output = il\n",
    "main_output = Embedding(624, 500)(il)\n",
    "main_output = Reshape(target_shape=(-1,500))(main_output) # reshape layer does not need to know BATCH SIZE!!!\n",
    "\n",
    "# sizes should be multiple of 32 since it trains faster due to np.float32\n",
    "main_output = LSTM(500,\n",
    "                   batch_input_shape=(batch_size,None,1),\n",
    "                   stateful=False,\n",
    "                   return_sequences=True,\n",
    "                   unroll=False,\n",
    "                   kernel_initializer=keras.initializers.Zeros())(main_output)\n",
    "main_output = LSTM(500,\n",
    "                   stateful=False,\n",
    "                   return_sequences=True,\n",
    "                   unroll=False,\n",
    "                   kernel_initializer=keras.initializers.Zeros())(main_output)\n",
    "\n",
    "main_output = Dense(len(feature_dict[\"concept:name\"][\"to_int\"]), activation='softmax', name='dense_final')(main_output)\n",
    "\n",
    "full_model = Model(inputs=[il], outputs=[main_output])\n",
    "optimizerator = keras.optimizers.adam()\n",
    "\n",
    "full_model.compile(loss='categorical_crossentropy', optimizer=optimizerator, metrics=['accuracy'])\n",
    "\n",
    "for l in full_model.layers:\n",
    "    print(l.name, \"--> input_shape={}\".format(l.input_shape), \"output_shape={}\".format(l.output_shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-3fd55bac7139>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mbatch_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mtr_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtr_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfull_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mmean_tr_acc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mmean_tr_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/thesis/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/thesis/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/thesis/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2653\u001b[0m                 array_vals.append(\n\u001b[1;32m   2654\u001b[0m                     np.asarray(value,\n\u001b[0;32m-> 2655\u001b[0;31m                                dtype=tf.as_dtype(tensor.dtype).as_numpy_dtype))\n\u001b[0m\u001b[1;32m   2656\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2657\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/thesis/lib/python3.6/site-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m     \"\"\"\n\u001b[0;32m--> 501\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    502\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "n_epochs = 1\n",
    "for epoch in range(1,n_epochs+1):\n",
    "    mean_tr_acc  = []\n",
    "    mean_tr_loss = []\n",
    "\n",
    "#     for t in tqdm(traces[:200], desc=\"Epoch {0}/{1}\".format(epoch,n_epochs)):\n",
    "    for t in traces:\n",
    "        batch_x = []\n",
    "        batch_y = []\n",
    "        \n",
    "        for x,y in stagger_sequence(t):\n",
    "            y = np_utils.to_categorical(y, num_classes=625)\n",
    "            batch_x.append(x)\n",
    "            batch_y.append(y)\n",
    "            \n",
    "        batch_x = np.array(batch_x).reshape((len(t)-1, -1, 1))\n",
    "        batch_y = np.array(batch_y)\n",
    "\n",
    "        tr_loss, tr_acc = full_model.train_on_batch(batch_x, batch_y)\n",
    "        mean_tr_acc.append(tr_acc)\n",
    "        mean_tr_loss.append(tr_loss)\n",
    "        break\n",
    "\n",
    "    mean_tr_acc = np.mean(mean_tr_acc)\n",
    "    print('Epoch {0} -- loss = {1:.5f} -- acc = {2:.5f}'.format(epoch,np.mean(mean_tr_loss), np.mean(mean_tr_acc)))\n",
    "\n",
    "    if best_acc < int(mean_tr_acc):\n",
    "        best_acc = int(mean_tr_acc)\n",
    "        full_model.save('evermann_baseline_e{0}_acc{1:.3f}.h5'.format(epoch,best_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_model = keras.models.load_model(\"evermann_baseline_e91.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[5.79371024e-03, 2.87678123e-01, 2.24107155e-09, 2.40011854e-04,\n",
       "         5.39651432e-04, 6.72204187e-04, 3.25576542e-03, 5.66909574e-02,\n",
       "         3.24004889e-03, 1.52331265e-02, 1.20667368e-02, 1.37805268e-02,\n",
       "         4.36056741e-02, 1.00682508e-02, 1.23797087e-02, 1.29512139e-02,\n",
       "         2.94028167e-02, 1.01537124e-04, 3.46898800e-03, 1.32629136e-03,\n",
       "         1.46427308e-03, 1.64528738e-03, 1.19171664e-02, 4.15197797e-02,\n",
       "         5.31600369e-03, 4.70651686e-03, 1.11822579e-02, 5.35978284e-03,\n",
       "         9.06471396e-05, 6.94872928e-04, 4.43080382e-04, 5.16606633e-05,\n",
       "         5.72092496e-02, 4.81083960e-04, 2.37390968e-10, 8.23457260e-04,\n",
       "         2.33945996e-03, 6.40116070e-07, 1.16019719e-03, 2.60202400e-02,\n",
       "         3.63005302e-03, 3.83877242e-03, 4.23235586e-04, 4.53943041e-15,\n",
       "         4.11286135e-04, 1.53355161e-03, 2.32875743e-03, 3.16886377e-04,\n",
       "         3.86347981e-10, 1.44132297e-03, 6.61979416e-10, 3.90624069e-02,\n",
       "         1.32486282e-03, 6.28044145e-05, 9.93238336e-06, 1.24891609e-04,\n",
       "         6.62750535e-05, 4.87485740e-05, 5.06858632e-05, 3.49310430e-04,\n",
       "         2.89636315e-04, 8.25308689e-06, 4.20392535e-05, 4.13724068e-13,\n",
       "         1.32839405e-03, 2.34758827e-05, 2.16001150e-09, 1.34031754e-03,\n",
       "         3.11577017e-03, 1.30129745e-03, 5.66238984e-02, 3.45323497e-04,\n",
       "         4.23303526e-09, 9.47611814e-04, 6.65784348e-03, 6.94943825e-04,\n",
       "         2.93039717e-03, 6.61872607e-03, 9.60107427e-03, 5.19465888e-04,\n",
       "         1.98324840e-03, 4.56039212e-04, 3.59567441e-03, 6.76339550e-04,\n",
       "         1.38613288e-09, 8.66988092e-04, 2.54694169e-05, 9.39801248e-05,\n",
       "         5.12549537e-04, 7.66180019e-05, 2.92318873e-04, 9.46432774e-05,\n",
       "         4.07709333e-04, 7.83027353e-05, 2.16682488e-03, 1.81565338e-04,\n",
       "         3.70002336e-12, 6.05552632e-04, 2.68398738e-03, 1.84259130e-04,\n",
       "         6.95314251e-10, 1.18063868e-03, 4.98860136e-05, 9.41769758e-05,\n",
       "         1.17437615e-08, 2.72366946e-04, 1.87602025e-04, 6.17443813e-13,\n",
       "         1.25050451e-06, 7.60168332e-05, 4.16565227e-09, 4.94697670e-05,\n",
       "         1.19829747e-04, 1.03816280e-14, 1.05983863e-05, 5.48663115e-10,\n",
       "         3.46744309e-05, 1.19824758e-11, 3.72210618e-09, 7.88983001e-14,\n",
       "         7.59056347e-05, 4.40263624e-12, 5.12377033e-13, 4.26076285e-09,\n",
       "         3.12415912e-04, 4.42837537e-11, 3.76659193e-09, 1.34529818e-10,\n",
       "         1.79092052e-13, 4.15339452e-09, 4.24489299e-05, 3.97177091e-09,\n",
       "         4.01537825e-09, 3.60619246e-09, 4.23062884e-04, 4.39724612e-09,\n",
       "         2.94474575e-07, 1.00385674e-04, 2.15192034e-04, 1.07151965e-15,\n",
       "         1.10750005e-08, 1.62584599e-11, 5.51809035e-06, 7.36639922e-05,\n",
       "         1.14485389e-04, 1.77692127e-05, 5.37722313e-04, 3.64738595e-09,\n",
       "         2.00329348e-03, 5.56224222e-05, 7.82703902e-11, 1.94095410e-04,\n",
       "         1.33559195e-04, 6.97335247e-15, 7.09744418e-05, 2.43988266e-04,\n",
       "         4.82402247e-05, 6.81847295e-18, 1.76350106e-04, 1.87667536e-13,\n",
       "         5.16797825e-20, 2.42897391e-09, 5.45481220e-04, 5.63154230e-04,\n",
       "         1.35542749e-07, 7.45259765e-09, 1.35699363e-09, 3.35896220e-06,\n",
       "         9.61845146e-14, 3.56377794e-09, 8.98545149e-09, 6.66090418e-05,\n",
       "         3.62293173e-09, 6.12540825e-05, 3.94202715e-09, 4.18682466e-09,\n",
       "         1.37552622e-10, 4.12699563e-09, 7.22935423e-04, 1.43412351e-10,\n",
       "         4.19786375e-05, 1.88172589e-09, 3.77225717e-09, 1.07591670e-11,\n",
       "         3.54318938e-04, 4.43119025e-05, 3.87895804e-09, 3.32650082e-15,\n",
       "         1.93489584e-04, 3.63810138e-14, 4.36889769e-09, 1.38208034e-09,\n",
       "         1.37067144e-03, 6.77632659e-21, 8.77433976e-13, 2.14663396e-06,\n",
       "         5.06927971e-19, 4.42036319e-09, 1.92022995e-10, 1.94058478e-20,\n",
       "         4.19312185e-09, 6.44389579e-16, 1.30909964e-12, 4.53972661e-14,\n",
       "         3.97044531e-09, 3.65118646e-09, 7.03197889e-10, 9.89681427e-15,\n",
       "         1.12333700e-15, 8.02320821e-10, 1.67870969e-12, 8.07280404e-14,\n",
       "         1.78341870e-03, 5.16906599e-15, 5.72720083e-12, 9.86144289e-23,\n",
       "         5.80723941e-11, 6.97285805e-12, 4.27402203e-09, 4.14124424e-09,\n",
       "         2.05029079e-04, 4.25128110e-09, 8.79447816e-06, 4.13118562e-09,\n",
       "         2.56813326e-08, 4.23309210e-09, 8.65723138e-08, 4.07793443e-09,\n",
       "         4.18454290e-12, 7.55151014e-14, 2.20165875e-05, 6.25076529e-04,\n",
       "         3.86432930e-09, 3.95846733e-09, 3.56255447e-09, 4.07778700e-09,\n",
       "         3.89304100e-09, 2.17505779e-15, 2.56590206e-18, 3.92518373e-09,\n",
       "         4.28052394e-09, 9.02684538e-10, 4.47138767e-12, 8.51185848e-14,\n",
       "         4.13678380e-09, 2.19464051e-14, 4.26960556e-09, 1.06725538e-10,\n",
       "         2.67041352e-04, 9.66374767e-13, 6.63709920e-11, 2.70544492e-13,\n",
       "         3.74766618e-09, 9.44154221e-04, 3.74832387e-09, 1.71908243e-09,\n",
       "         2.84959332e-14, 1.18069068e-10, 3.29747039e-04, 3.99853207e-06,\n",
       "         1.93071692e-08, 2.43533860e-09, 3.08740290e-11, 2.77819353e-14,\n",
       "         6.73168205e-12, 5.78089576e-10, 3.38293180e-17, 8.33494926e-16,\n",
       "         3.34113404e-10, 3.55744656e-14, 1.90533100e-09, 3.61605568e-09,\n",
       "         1.48947247e-05, 4.35466285e-09, 4.14356727e-09, 3.95323863e-09,\n",
       "         3.96428534e-09, 3.87065846e-09, 4.35008074e-09, 4.08889322e-09,\n",
       "         3.62556518e-09, 4.07453715e-09, 7.53515826e-12, 4.12498080e-09,\n",
       "         4.71250328e-09, 1.53927631e-05, 1.36956429e-15, 2.87289750e-13,\n",
       "         3.95771282e-09, 3.80730398e-07, 2.78749357e-10, 4.16478629e-09,\n",
       "         4.29318936e-09, 3.61878771e-09, 3.62139096e-09, 1.43456870e-11,\n",
       "         4.23610480e-09, 3.78816978e-09, 7.28258493e-16, 3.64784514e-09,\n",
       "         9.21991450e-05, 6.71306500e-12, 1.64198298e-07, 8.73244619e-14,\n",
       "         4.22173096e-09, 4.55342297e-09, 1.28324803e-16, 3.61785148e-19,\n",
       "         1.03068921e-10, 4.57554170e-18, 3.92092581e-09, 1.70456947e-20,\n",
       "         4.09963041e-09, 1.57525964e-10, 4.09786383e-09, 3.91669541e-09,\n",
       "         3.93259514e-09, 1.27391695e-13, 9.63368336e-08, 7.86245583e-11,\n",
       "         7.72528268e-14, 3.74913922e-09, 3.87215016e-09, 3.33499802e-13,\n",
       "         5.93287695e-19, 3.60363450e-09, 4.63940908e-12, 7.37647443e-12,\n",
       "         4.27042002e-09, 4.04196498e-09, 3.95916233e-09, 4.08343759e-09,\n",
       "         6.62934652e-10, 4.14761558e-09, 1.91381204e-07, 3.60172447e-09,\n",
       "         2.77220761e-06, 3.80217680e-09, 1.76963132e-12, 3.67957043e-09,\n",
       "         4.47559279e-09, 3.96272037e-09, 4.16068913e-09, 2.48182348e-16,\n",
       "         4.32926317e-09, 1.39624371e-11, 4.28603020e-09, 1.96778593e-09,\n",
       "         1.30228378e-04, 2.73582629e-10, 6.27544317e-16, 3.16205107e-09,\n",
       "         3.77710263e-09, 1.66148123e-10, 4.34795711e-09, 7.30455387e-04,\n",
       "         3.67419140e-09, 6.80478118e-10, 9.66910386e-15, 1.00803021e-09,\n",
       "         5.61431305e-17, 4.47420101e-09, 5.70067193e-09, 4.07353440e-09,\n",
       "         4.06020817e-09, 3.87716081e-09, 4.29645786e-09, 2.62334529e-11,\n",
       "         4.07281187e-09, 4.28877023e-09, 4.08827683e-09, 4.33266711e-09,\n",
       "         4.11536227e-09, 3.75359654e-09, 3.97179312e-09, 4.02668299e-09,\n",
       "         4.23812496e-09, 4.56672167e-09, 2.61821871e-17, 1.17130307e-11,\n",
       "         5.48894159e-11, 1.64424448e-16, 1.08775662e-11, 3.91319332e-09,\n",
       "         3.89451893e-09, 3.18373420e-13, 1.08817297e-16, 1.54355973e-07,\n",
       "         8.29083092e-07, 1.59835507e-14, 4.12143430e-09, 4.34468239e-09,\n",
       "         3.59913312e-14, 1.14459308e-15, 1.00770501e-17, 4.36764802e-09,\n",
       "         4.59449589e-09, 2.36190390e-11, 4.41641923e-09, 2.03033354e-11,\n",
       "         9.12743324e-14, 5.13069968e-11, 3.61206399e-13, 3.87515708e-09,\n",
       "         3.94928179e-09, 4.53207338e-09, 3.72930486e-09, 7.29675212e-20,\n",
       "         6.64002395e-11, 3.84164300e-09, 1.93140481e-09, 2.84928547e-09,\n",
       "         8.02481386e-14, 1.08109344e-09, 1.53633752e-14, 1.25907657e-15,\n",
       "         3.78465304e-09, 4.05805567e-09, 3.83547150e-09, 2.06276261e-15,\n",
       "         3.86166166e-09, 3.23482574e-10, 1.08135159e-12, 4.66686645e-09,\n",
       "         4.44957227e-09, 3.67938768e-09, 4.16169721e-09, 4.17769064e-09,\n",
       "         4.23599156e-09, 1.88402592e-11, 3.95009581e-09, 4.07797351e-09,\n",
       "         4.04588515e-12, 4.20144008e-09, 4.00253253e-09, 3.90049548e-09,\n",
       "         3.93844246e-09, 3.70358078e-09, 3.73594000e-09, 3.80939236e-09,\n",
       "         4.08005851e-09, 4.18839807e-09, 3.85292198e-09, 4.39236825e-09,\n",
       "         4.09081968e-09, 7.76381603e-06, 3.64575148e-09, 3.84930798e-09,\n",
       "         8.41092862e-10, 4.14171852e-09, 1.15535334e-16, 3.76280740e-09,\n",
       "         3.89355348e-09, 3.79383902e-09, 4.19353796e-09, 4.19313784e-09,\n",
       "         4.12972012e-09, 4.22984758e-09, 4.11840917e-09, 4.08897094e-09,\n",
       "         4.04625400e-09, 8.12468409e-12, 1.36963119e-09, 3.65821284e-09,\n",
       "         4.13773105e-09, 3.83157417e-09, 4.38868319e-09, 4.26355928e-09,\n",
       "         4.91210317e-09, 3.96392252e-09, 3.51936458e-09, 4.02507849e-09,\n",
       "         3.47780538e-09, 3.97456690e-09, 1.14853584e-08, 9.78861022e-14,\n",
       "         4.76975534e-12, 6.88906533e-14, 4.17686996e-09, 3.98835009e-09,\n",
       "         4.06813783e-09, 4.35020553e-09, 4.01610567e-09, 3.83766663e-09,\n",
       "         4.09733225e-09, 4.31796598e-09, 4.26381952e-09, 3.91271593e-09,\n",
       "         3.58060692e-09, 4.05602825e-13, 1.79258414e-10, 4.00694722e-09,\n",
       "         4.00402111e-09, 3.92095600e-09, 4.17031032e-09, 4.06662526e-09,\n",
       "         3.89229848e-09, 4.37985337e-09, 4.01263733e-09, 4.11923340e-09,\n",
       "         4.14835100e-09, 3.93853217e-09, 2.58973043e-09, 7.73531304e-13,\n",
       "         5.11867871e-14, 4.31240199e-09, 4.68263117e-09, 4.14159951e-09,\n",
       "         4.51897053e-09, 3.70266240e-09, 3.99713862e-09, 4.11511891e-09,\n",
       "         4.02903444e-09, 4.28029567e-09, 4.15486046e-09, 3.75960107e-09,\n",
       "         4.15426582e-09, 3.57851104e-09, 1.38595824e-09, 1.74655533e-11,\n",
       "         4.33185710e-09, 8.33704980e-12, 3.86859211e-09, 3.74736597e-09,\n",
       "         2.81571777e-14, 4.03110967e-09, 4.59962513e-09, 3.88466681e-09,\n",
       "         4.00919475e-09, 4.03122631e-12, 4.01979960e-09, 3.86589250e-09,\n",
       "         4.31233627e-09, 3.74516507e-09, 3.76360454e-09, 6.73494658e-15,\n",
       "         7.78836440e-11, 3.98880662e-09, 3.97224786e-09, 2.63196907e-16,\n",
       "         3.95264310e-09, 3.95722966e-09, 4.03675715e-09, 9.18948903e-16,\n",
       "         3.95701072e-09, 4.10963619e-09, 3.88314803e-09, 4.15318091e-09,\n",
       "         3.38863493e-09, 3.99780165e-09, 4.05559542e-09, 4.00099021e-09,\n",
       "         4.35271197e-09, 1.91294246e-12, 3.88652666e-09, 4.26106350e-09,\n",
       "         3.87489862e-09, 4.48058080e-09, 3.80399046e-09, 1.81713575e-10,\n",
       "         4.15819823e-09, 3.99858724e-09, 3.71596309e-09, 3.93429023e-09,\n",
       "         3.57738483e-09, 3.41892736e-09, 3.95725186e-09, 3.93272259e-09,\n",
       "         3.72427933e-09, 3.71160680e-09, 4.24294599e-09, 3.84767818e-09,\n",
       "         3.64329100e-09, 4.08597733e-09, 4.00108968e-09, 4.11540135e-09,\n",
       "         4.06336875e-09, 4.03597156e-09, 4.63028371e-09, 3.98653244e-09,\n",
       "         3.86778032e-09, 3.55216798e-08, 4.27376090e-09, 4.39853842e-09,\n",
       "         3.89948385e-09, 3.92035782e-09, 3.02782891e-13, 3.87704269e-09,\n",
       "         3.99867117e-09, 4.30059011e-09, 1.18572110e-10, 3.93101285e-09,\n",
       "         4.03902112e-09, 6.11498443e-16, 3.76350373e-09, 3.70544595e-09,\n",
       "         3.88697163e-09, 4.00444122e-09, 3.40568262e-09, 3.24417669e-16,\n",
       "         4.60186289e-09, 4.24824176e-09, 3.91438792e-09, 3.72359721e-09,\n",
       "         4.07289757e-09, 3.86899757e-09, 4.49239801e-09, 7.33774372e-16,\n",
       "         1.59699545e-11, 4.05147427e-09, 4.11193346e-09, 4.47583126e-09,\n",
       "         1.09788772e-13, 4.35693126e-09, 4.57146276e-09, 4.02061273e-09,\n",
       "         3.98254274e-09, 7.26758843e-15, 4.52273019e-09, 4.05264888e-09,\n",
       "         4.45858594e-09, 4.31000124e-09, 3.77126419e-09, 3.63756936e-09,\n",
       "         1.36607155e-01]]], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_model.predict(np.array([batch_x[0:1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successive testing\n",
      "0.66% ==> 7 624\n",
      "1.32% ==> 7 624\n",
      "1.99% ==> 74 1\n",
      "2.65% ==> 75 1\n",
      "3.31% ==> 76 32\n",
      "3.97% ==> 8 32\n",
      "4.64% ==> 9 1\n",
      "5.30% ==> 10 1\n",
      "5.96% ==> 77 624\n",
      "6.62% ==> 11 11\n",
      "7.28% ==> 12 1\n",
      "7.95% ==> 24 1\n",
      "8.61% ==> 25 32\n",
      "9.27% ==> 26 624\n",
      "9.93% ==> 39 1\n",
      "10.60% ==> 40 1\n",
      "11.26% ==> 13 32\n",
      "11.92% ==> 14 624\n",
      "12.58% ==> 78 1\n",
      "13.25% ==> 79 1\n",
      "13.91% ==> 67 32\n",
      "14.57% ==> 80 32\n",
      "15.23% ==> 81 1\n",
      "15.89% ==> 49 1\n",
      "16.56% ==> 69 32\n",
      "17.22% ==> 15 32\n",
      "17.88% ==> 82 1\n",
      "18.54% ==> 0 1\n",
      "19.21% ==> 1 1\n",
      "19.87% ==> 32 624\n",
      "20.53% ==> 212 1\n",
      "21.19% ==> 70 75\n",
      "21.85% ==> 16 1\n",
      "22.52% ==> 7 1\n",
      "23.18% ==> 7 1\n",
      "23.84% ==> 7 624\n",
      "24.50% ==> 9 1\n",
      "25.17% ==> 10 1\n",
      "25.83% ==> 11 624\n",
      "26.49% ==> 12 624\n",
      "27.15% ==> 39 1\n",
      "27.81% ==> 40 1\n",
      "28.48% ==> 44 32\n",
      "29.14% ==> 45 32\n",
      "29.80% ==> 49 1\n",
      "30.46% ==> 15 1\n",
      "31.13% ==> 22 624\n",
      "31.79% ==> 23 624\n",
      "32.45% ==> 51 1\n",
      "33.11% ==> 16 1\n",
      "33.77% ==> 253 624\n",
      "34.44% ==> 7 32\n",
      "35.10% ==> 7 1\n",
      "35.76% ==> 7 1\n",
      "36.42% ==> 7 32\n",
      "37.09% ==> 76 624\n",
      "37.75% ==> 88 1\n",
      "38.41% ==> 88 32\n",
      "39.07% ==> 88 1\n",
      "39.74% ==> 143 32\n",
      "40.40% ==> 143 143\n",
      "41.06% ==> 143 15\n",
      "41.72% ==> 143 143\n",
      "42.38% ==> 9 144\n",
      "43.05% ==> 144 1\n",
      "43.71% ==> 144 32\n",
      "44.37% ==> 144 144\n",
      "45.03% ==> 144 145\n",
      "45.70% ==> 89 144\n",
      "46.36% ==> 89 35\n",
      "47.02% ==> 89 1\n",
      "47.68% ==> 145 91\n",
      "48.34% ==> 145 91\n",
      "49.01% ==> 145 91\n",
      "49.67% ==> 145 89\n",
      "50.33% ==> 11 89\n",
      "50.99% ==> 35 1\n",
      "51.66% ==> 35 1\n",
      "52.32% ==> 35 1\n",
      "52.98% ==> 12 624\n",
      "53.64% ==> 36 1\n",
      "54.30% ==> 36 624\n",
      "54.97% ==> 36 1\n",
      "55.63% ==> 41 624\n",
      "56.29% ==> 41 1\n",
      "56.95% ==> 41 624\n",
      "57.62% ==> 146 1\n",
      "58.28% ==> 146 32\n",
      "58.94% ==> 146 1\n",
      "59.60% ==> 146 8\n",
      "60.26% ==> 46 144\n",
      "60.93% ==> 46 9\n",
      "61.59% ==> 220 1\n",
      "62.25% ==> 90 32\n",
      "62.91% ==> 90 1\n",
      "63.58% ==> 90 35\n",
      "64.24% ==> 80 1\n",
      "64.90% ==> 91 90\n",
      "65.56% ==> 91 32\n",
      "66.23% ==> 91 91\n",
      "66.89% ==> 91 91\n",
      "67.55% ==> 15 91\n",
      "68.21% ==> 15 1\n",
      "68.87% ==> 15 624\n",
      "69.54% ==> 15 1\n",
      "70.20% ==> 23 624\n",
      "70.86% ==> 51 1\n",
      "71.52% ==> 16 32\n",
      "72.19% ==> 87 1\n",
      "72.85% ==> 23 32\n",
      "73.51% ==> 51 1\n",
      "74.17% ==> 16 32\n",
      "74.83% ==> 23 1\n",
      "75.50% ==> 51 624\n",
      "76.16% ==> 16 1\n",
      "76.82% ==> 23 624\n",
      "77.48% ==> 51 1\n",
      "78.15% ==> 16 32\n",
      "78.81% ==> 7 1\n",
      "79.47% ==> 7 624\n",
      "80.13% ==> 9 1\n",
      "80.79% ==> 10 624\n",
      "81.46% ==> 11 1\n",
      "82.12% ==> 12 624\n",
      "82.78% ==> 15 1\n",
      "83.44% ==> 23 624\n",
      "84.11% ==> 51 1\n",
      "84.77% ==> 16 32\n",
      "85.43% ==> 23 1\n",
      "86.09% ==> 51 624\n",
      "86.75% ==> 16 1\n",
      "87.42% ==> 23 624\n",
      "88.08% ==> 51 1\n",
      "88.74% ==> 16 32\n",
      "89.40% ==> 23 1\n",
      "90.07% ==> 51 624\n",
      "90.73% ==> 16 1\n",
      "91.39% ==> 23 624\n",
      "92.05% ==> 51 1\n",
      "92.72% ==> 16 32\n",
      "93.38% ==> 23 1\n",
      "94.04% ==> 51 624\n",
      "94.70% ==> 16 1\n",
      "95.36% ==> 23 624\n",
      "96.03% ==> 51 1\n",
      "96.69% ==> 53 32\n",
      "97.35% ==> 32 1\n",
      "98.01% ==> 32 32\n",
      "98.68% ==> 32 1\n",
      "99.34% ==> 1 624\n"
     ]
    }
   ],
   "source": [
    "for t in traces[sep_idx: sep_idx+1]:\n",
    "    batch_x = t[\"concept:name\"].values.reshape((-1,1))\n",
    "    batch_y = t[\"TARGET\"].values\n",
    "    \n",
    "    print(\"Successive testing\")\n",
    "    for i in range(1,len(batch_x)):\n",
    "        yhat = full_model.predict(np.array([batch_x[0:i]]))\n",
    "        yhat = [np.argmax(yh) for yh in yhat[0]][-1]\n",
    "    \n",
    "        print(\"{0:2.2f}% ==>\".format(i / len(batch_x) * 100), batch_y[0:i][-1], yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_index = [ np.argmax(p) for p in prediction ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum([prediction_index[i] == dataY[i] for i in prediction_index]) / len(prediction_index)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:thesis]",
   "language": "python",
   "name": "conda-env-thesis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
