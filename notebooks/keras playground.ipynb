{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import multiprocessing\n",
    "\n",
    "from tqdm import *\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Embedding, Input, Reshape, concatenate, Flatten, Activation, LSTM\n",
    "from keras.utils  import multi_gpu_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/job:localhost/replica:0/task:0/device:GPU:0',\n",
       " '/job:localhost/replica:0/task:0/device:GPU:1']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.backend.tensorflow_backend._get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### CONFIGURATION SETUP ####\n",
    "\n",
    "data_path = \"../logs/bpic2011.xes\"\n",
    "traces_finalpath = data_path.replace(\".xes\", \"_traces_encoded.pickled\")\n",
    "traces_dictionarypath = data_path.replace(\".xes\", \"_dictionaries.pickled\")\n",
    "n_sp2_features = 624\n",
    "n_pfs_features = 25\n",
    "\n",
    "traces = pickle.load(open(traces_finalpath, \"rb\"))\n",
    "feature_dict = pickle.load(open(traces_dictionarypath, \"rb\"))\n",
    "\n",
    "### CONFIGURATION SETUP END ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle complete traces and create test and training set\n",
    "random.shuffle(traces)\n",
    "sep_idx = int(0.8*len(traces))\n",
    "\n",
    "# extract the feature indices\n",
    "# data is organized like this: ordinal features | categorical features | SP2 features | PFS features | TARGET features\n",
    "# needed as every of these features will get its own layer\n",
    "feature_names  = traces[0].columns\n",
    "trace_columns = list(map(lambda e: bool(re.match('^TARGET$', e)), feature_names))\n",
    "target_col_start_index = trace_columns.index(True)\n",
    "\n",
    "categorical_feature_names = feature_dict.keys()\n",
    "pfs_col_start_index = target_col_start_index - n_pfs_features\n",
    "sp2_col_start_index = pfs_col_start_index - n_sp2_features\n",
    "cat_col_start_index = sp2_col_start_index - len(categorical_feature_names)\n",
    "\n",
    "ordinal_feature_names     = feature_names[0:cat_col_start_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_input_name(var_name):\n",
    "    return \"input_{0}\".format(''.join(c for c in var_name if c.isalnum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting traces to Keras learning data:   0%|          | 0/1143 [00:00<?, ?traces/s]\n",
      "Converting traces to Keras learning data:   0%|          | 1/1143 [00:00<09:34,  1.99traces/s]\n",
      "Converting traces to Keras learning data:   0%|          | 5/1143 [00:01<08:24,  2.26traces/s]\n",
      "Converting traces to Keras learning data:   1%|          | 14/1143 [00:04<07:20,  2.56traces/s]\n",
      "Converting traces to Keras learning data:  16%|█▋        | 188/1143 [00:06<04:25,  3.60traces/s]\n",
      "Converting traces to Keras learning data:  17%|█▋        | 200/1143 [00:07<03:14,  4.84traces/s]\n",
      "Converting traces to Keras learning data:  25%|██▍       | 285/1143 [00:07<02:05,  6.83traces/s]\n",
      "Converting traces to Keras learning data:  33%|███▎      | 372/1143 [00:07<01:19,  9.71traces/s]\n",
      "Converting traces to Keras learning data:  48%|████▊     | 551/1143 [00:08<00:43, 13.66traces/s]\n",
      "Converting traces to Keras learning data:  52%|█████▏    | 598/1143 [00:08<00:29, 18.69traces/s]\n",
      "Converting traces to Keras learning data:  54%|█████▍    | 621/1143 [00:10<00:32, 15.95traces/s]\n",
      "Converting traces to Keras learning data:  63%|██████▎   | 715/1143 [00:11<00:19, 22.16traces/s]\n",
      "Converting traces to Keras learning data:  65%|██████▍   | 742/1143 [00:11<00:13, 29.85traces/s]\n",
      "Converting traces to Keras learning data:  66%|██████▋   | 760/1143 [00:13<00:25, 15.19traces/s]\n",
      "Converting traces to Keras learning data:  73%|███████▎  | 830/1143 [00:14<00:15, 20.52traces/s]\n",
      "Converting traces to Keras learning data:  94%|█████████▎| 1070/1143 [00:14<00:02, 29.14traces/s]\n",
      "1070it [00:14, 29.30it/s]\u001b[A\n",
      "Converting traces to Keras learning data: 100%|██████████| 1143/1143 [00:14<00:00, 78.33traces/s]\n"
     ]
    }
   ],
   "source": [
    "# TODO: normalize\n",
    "# X = X / float(n_vocab)\n",
    "def wrapped__create_learning_dicts_from_trace(p):\n",
    "    return create_learning_dicts_from_trace(*p)\n",
    "# reshape X to be [samples, time steps, features]\n",
    "# How to understand keras feature shape requirements: https://github.com/keras-team/keras/issues/2045\n",
    "def create_learning_dicts_from_trace(t, sp2_col_start_index, n_sp2_features, pfs_col_start_index, n_pfs_features, target_col_start_index, feature_names):\n",
    "    t_dict = {'x':[], 'y':[]}\n",
    "    # generate one input sequence for every type of variable\n",
    "    # map every single-step batch in a dictionary that will correspond to input layer names!\n",
    "    for i in range(0, len(t)):\n",
    "        batch_dict = {}\n",
    "\n",
    "        # automatically run through all ordinal and categorical features\n",
    "        for col_idx, col in enumerate(feature_names[:sp2_col_start_index]):\n",
    "            input_name = generate_input_name(col)\n",
    "            batch_dict[input_name] = np.array(t.iloc[i, col_idx], dtype=np.float32).reshape([-1,1])\n",
    "\n",
    "        # create batches for sp2 and pfs2 seperately because of their variable encodings\n",
    "        batch_dict[generate_input_name(\"sp2\")] = np.asarray(t.iloc[i, sp2_col_start_index:pfs_col_start_index], dtype=np.float32).reshape([-1,n_sp2_features])\n",
    "        batch_dict[generate_input_name(\"pfs\")] = np.asarray(t.iloc[i, pfs_col_start_index:target_col_start_index], dtype=np.float32).reshape([-1,n_pfs_features])\n",
    "\n",
    "        t_dict['x'].append(batch_dict)\n",
    "    t_dict['y'] = keras.utils.np_utils.to_categorical(t.iloc[:, target_col_start_index:].values.reshape([-1,1,1]))\n",
    "    return t_dict\n",
    "\n",
    "ncores = multiprocessing.cpu_count()\n",
    "ppool = multiprocessing.Pool(ncores)\n",
    "train_traces = []\n",
    "traces_for_learning_dicts = [ (t, sp2_col_start_index, n_sp2_features, pfs_col_start_index, n_pfs_features, target_col_start_index, feature_names) for t in traces ]\n",
    "\n",
    "with tqdm(total=len(traces), desc=\"Converting traces to Keras learning data\", unit=\"traces\") as pbar:\n",
    "    for i, _ in tqdm(enumerate(ppool.imap(wrapped__create_learning_dicts_from_trace, traces_for_learning_dicts))):\n",
    "        pbar.update()\n",
    "        train_traces.append(_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'feature_names' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-7fe6356b7f7a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# forward all ordinal features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mord_var\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeature_names\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mcat_col_start_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mil\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgenerate_input_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mord_var\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mReshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mil\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'feature_names' is not defined"
     ]
    }
   ],
   "source": [
    "models = []\n",
    "model_inputs = []\n",
    "\n",
    "# forward all ordinal features\n",
    "for ord_var in feature_names[:cat_col_start_index]:\n",
    "    il = Input(batch_shape=(1,1), name=generate_input_name(ord_var))\n",
    "    model = Reshape(target_shape=(1,1,))(il)\n",
    "    model_inputs.append(il)\n",
    "    models.append(model)\n",
    "\n",
    "# create embedding layers for every categorical feature\n",
    "for cat_var in categorical_feature_names :\n",
    "    model = Sequential()\n",
    "    no_of_unique_cat  = len(feature_dict[cat_var]['to_int'])\n",
    "    embedding_size = int(min(np.ceil((no_of_unique_cat)/2), 50 ))\n",
    "    vocab  = no_of_unique_cat+1\n",
    "    \n",
    "    il = Input(batch_shape=(1,1), name=generate_input_name(cat_var))    \n",
    "    model = Embedding(vocab, embedding_size)(il)\n",
    "    model = Reshape(target_shape=(1,embedding_size,))(model)\n",
    "    \n",
    "    model_inputs.append(il)\n",
    "    models.append(model)\n",
    "\n",
    "# create input and embedding for sp2/pfs2 features\n",
    "learn_sp2 = True\n",
    "sequence_embedding = None\n",
    "\n",
    "if learn_sp2:\n",
    "    il = Input(batch_shape=(1,n_sp2_features), name=generate_input_name(\"sp2\"))\n",
    "    model_inputs.append(il)\n",
    "    \n",
    "    no_of_unique_cat = n_sp2_features\n",
    "    embedding_size   = int(min(np.ceil((no_of_unique_cat)/2), 50 ))\n",
    "    vocab  = no_of_unique_cat+1\n",
    "    sequence_embedding = Embedding(vocab, embedding_size)(il)\n",
    "    sequence_embedding = Reshape(target_shape=(no_of_unique_cat*embedding_size,))(sequence_embedding)\n",
    "else:\n",
    "    # TODO\n",
    "    pass\n",
    "    \n",
    "# merge the outputs of the embeddings, and everything that belongs to the most recent activity executions\n",
    "main_output = concatenate(models, axis=2)\n",
    "main_output = Reshape(target_shape=(1,))\n",
    "main_output = LSTM(25*32, stateful=True)(main_output) # should be multiple of 32 since it trains faster due to np.float32\n",
    "main_output = LSTM(25*32, stateful=True)(main_output) # should be multiple of 32 since it trains faster due to np.float32\n",
    "\n",
    "# after LSTM has learned on the sequence, bring in the SP2/PFS features, like in Shibatas paper\n",
    "main_output = concatenate([main_output, sequence_embedding], axis=1)\n",
    "main_output = Dense(20*32, activation='relu', name='dense_join')(main_output)\n",
    "main_output = Dense(len(feature_dict[\"concept:name\"][\"to_int\"]), activation='sigmoid', name='dense_final')(main_output)\n",
    "\n",
    "for l in full_model.layers:\n",
    "    print(l.name, \"input_shape={}\".format(l.input_shape), \"output_shape={}\".format(l.output_shape))\n",
    "\n",
    "full_model = Model(inputs=model_inputs, outputs=[main_output])\n",
    "full_model = multi_gpu_model(full_model)\n",
    "full_model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 0/1143\n",
      "Training 1/1143\n",
      "Training 2/1143\n",
      "Training 3/1143\n",
      "Training 4/1143\n",
      "accuracy training = nan\n",
      "loss training = 4.569851398468018\n",
      "___________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/felix.wolff2/anaconda3/envs/thesis/lib/python3.6/site-packages/numpy/core/fromnumeric.py:2920: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/home/felix.wolff2/anaconda3/envs/thesis/lib/python3.6/site-packages/numpy/core/_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "# # define the checkpoint\n",
    "# filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "# checkpoint = keras.callbacks.ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "# callbacks_list = [checkpoint]\n",
    "\n",
    "# for t in train_traces:\n",
    "#     full_model.fit(t['x'], t['y'], epochs=10, batch_size=50, callbacks=callbacks_list)\n",
    "#     model.reset_states()\n",
    "\n",
    "for epoch in range(1):\n",
    "    mean_tr_acc = []\n",
    "    mean_tr_loss = []\n",
    "    for t_idx, t in enumerate(train_traces[0:5]):\n",
    "        print(\"Training {0}/{1}\".format(t_idx,len(train_traces)))\n",
    "        for x,y in zip(t['x'],t['y']):\n",
    "            tr_loss = full_model.train_on_batch(x, y)\n",
    "            mean_tr_loss.append(tr_loss)\n",
    "        full_model.reset_states()\n",
    "\n",
    "    print('accuracy training = {}'.format(np.mean(mean_tr_acc)))\n",
    "    print('loss training = {}'.format(np.mean(mean_tr_loss)))\n",
    "    print('___________________________________')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_index = [ np.argmax(p) for p in prediction ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum([prediction_index[i] == dataY[i] for i in prediction_index]) / len(prediction_index)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:thesis]",
   "language": "python",
   "name": "conda-env-thesis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
