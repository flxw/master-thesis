{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Embedding, Input, Reshape, concatenate, Flatten, Activation, LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### CONFIGURATION SETUP ####\n",
    "\n",
    "data_path = \"../logs/bpic2011.xes\"\n",
    "traces_finalpath = data_path.replace(\".xes\", \"_traces_encoded.pickled\")\n",
    "traces_dictionarypath = data_path.replace(\".xes\", \"_dictionaries.pickled\")\n",
    "n_sp2_features = 624\n",
    "n_pfs_features = 25\n",
    "\n",
    "target_column = \"concept:name\"\n",
    "categorical_features = [\"concept:name\", \"Specialism code\", \"org:group\"]\n",
    "\n",
    "traces = pickle.load(open(traces_finalpath, \"rb\"))\n",
    "feature_dict = pickle.load(open(traces_dictionarypath, \"rb\"))\n",
    "\n",
    "### CONFIGURATION SETUP END ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle complete traces and create test and training set\n",
    "random.shuffle(traces)\n",
    "sep_idx = int(0.8*len(traces))\n",
    "\n",
    "# train_traces = pd.concat(traces[:sep_idx])\n",
    "# test_traces  = traces[sep_idx:]\n",
    "# https://github.com/keras-team/keras/issues/3107\n",
    "# Yes, your input should be in this format (sequences, timesteps, dimensions).\n",
    "# So based on your example, your input should be in (None, 8, 2). Your input now is (8,2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the feature indices\n",
    "# data is organized like this: normal features | SP2 features | PFS features | TARGET features\n",
    "trace_columns = traces[0].columns.tolist()\n",
    "trace_columns = list(map(lambda e: bool(re.match('^TARGET$', e)), trace_columns))\n",
    "target_col_start_index = trace_columns.index(True)\n",
    "pfs_col_start_index = target_col_start_index - n_pfs_features\n",
    "sp2_col_start_index = pfs_col_start_index - n_sp2_features\n",
    "cat_col_start_index = sp2_col_start_index - len(categorical_features)\n",
    "\n",
    "# reshape X to be [samples, time steps, features]\n",
    "# X = np.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "# TODO: normalize\n",
    "# X = X / float(n_vocab)\n",
    "\n",
    "# TODO: understand output shaping?!\n",
    "# How to understand keras feature shape requirements: https://github.com/keras-team/keras/issues/2045\n",
    "train_traces = []\n",
    "for t in traces[:sep_idx]:\n",
    "    t_dict = {}\n",
    "    t_dict['x'] = [ t.iloc[:, i].values.reshape([-1,1,1]) for i in range(cat_col_start_index,sp2_col_start_index)]\n",
    "    t_dict['y'] = keras.utils.np_utils.to_categorical(t.iloc[:, target_col_start_index:].values)\n",
    "    train_traces.append(t_dict)\n",
    "\n",
    "test_traces = []\n",
    "for t in test_traces:\n",
    "    t_dict = {}\n",
    "    t_dict['x'] = [ t.iloc[:, i].values.reshape([-1,1,1]) for i in range(cat_col_start_index,sp2_col_start_index)]\n",
    "    t_dict['y'] = keras.utils.np_utils.to_categorical(t.iloc[:, target_col_start_index:].values)\n",
    "    test_traces.append(t_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "model_inputs = []\n",
    "\n",
    "# create embedding layers for every categorical feature\n",
    "for cat_var in categorical_features :\n",
    "    model = Sequential()\n",
    "    no_of_unique_cat  = len(feature_dict[cat_var]['to_int'])\n",
    "    embedding_size = int(min(np.ceil((no_of_unique_cat)/2), 50 ))\n",
    "    vocab  = no_of_unique_cat+1\n",
    "    \n",
    "    il = Input(shape=(None, 1), name=\"Input_{0}\".format(''.join(c for c in cat_var if c.isalnum())))\n",
    "    model_inputs.append(il)\n",
    "    \n",
    "    model = Embedding(vocab, embedding_size)(il)\n",
    "    model = Reshape(target_shape=(1,embedding_size))(model)\n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the outputs of the embeddings in a dense layer\n",
    "main_output = concatenate(models, axis=2)\n",
    "main_output = LSTM(400)(main_output)\n",
    "main_output = Dense(len(feature_dict[\"concept:name\"][\"to_int\"]), activation='sigmoid', name='dense_final')(main_output)\n",
    "\n",
    "full_model = Model(inputs=model_inputs, outputs=[main_output])\n",
    "full_model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the checkpoint\n",
    "# filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "# checkpoint = keras.callbacks.ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "# callbacks_list = [checkpoint]\n",
    "\n",
    "for t in train_traces:\n",
    "    full_model.fit(t['x'], t['y'], epochs=1, batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_index = [ np.argmax(p) for p in prediction ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum([prediction_index[i] == dataY[i] for i in prediction_index]) / len(prediction_index)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
