from numpy import percentile
from keras.preprocessing.sequence import pad_sequences
from math import ceil
import numpy as np

def format_datasets(model_formatted_data_fn, datapath, target_variable):
    train_X, train_Y, test_X, test_Y = model_formatted_data_fn(datapath, target_variable)
    max_batch_size = ceil(0.01 * len(test_Y))
    
    # loop through every dictionary key and group (since the elements had the same order before, they should have after)
    for input_name in train_X.keys():
        train_X_layer = train_X[input_name]
        grouped_train_X = {}

        # create a dictionary entry for every timeseries length and put the traces in the appropriate bin
        for i in range(0, len(train_X_layer)):
            tl = len(train_X_layer[i])
            elX = np.array(train_X_layer[i])

            if tl in grouped_train_X:
                if len(grouped_train_X[tl][-1]) < max_batch_size:
                    grouped_train_X[tl][-1].append(elX)
                else:
                    grouped_train_X[tl].append([elX])
            else:
                grouped_train_X[tl] = [[elX]]
                
        flattened = [item for sublist in grouped_train_X.values() for item in sublist]
        train_X[input_name] = [np.array(l) for l in flattened]

    # similarly loop through the targets to cluster them
    grouped_train_Y = {}
    
    for elY in train_Y:
        tl = len(elY)
        
        if tl in grouped_train_Y:
                if len(grouped_train_Y[tl][-1]) < max_batch_size:
                    grouped_train_Y[tl][-1].append(elY)
                else:
                    grouped_train_Y[tl].append([elY])
            
    flattened = [item for sublist in grouped_train_Y.values() for item in sublist]
    train_Y = np.array([np.array(l) for l in flattened ])
    
    # prevent extreme batch sizes when traces are not very different
#    for name in train_X.keys():
#        harmonized_arrays = []
#        
#        for batch in train_X[name]:
#            if batch.shape[0] > max_batch_size:
#                splits = int(len(batch)/max_batch_size)+1
#                harmonized_arrays.append(np.array_split(batch, splits, axis=0))
 #               
 #       train_X[name] = [item for sublist in harmonized_arrays for item in sublist]
 ##       
 #   harmonized_arrays = []
 #   for batch in train_Y:
  #      if batch.shape[0] > max_batch_size:
  ##          splits = int(len(batch)/max_batch_size)+1
  #          harmonized_arrays.append(np.array_split(batch, splits, axis=0))
  #  train_Y = [item for sublist in harmonized_arrays for item in sublist]

    # finish the testing set
    n_y_cols = test_Y[0].shape[1]
    test_targets  = [ t.reshape((1, -1, n_y_cols)) for t in test_Y ]
    
    test_inputs  = {}
    for layer_name in test_X.keys():
        n_x_cols = test_X[layer_name][0].shape[1]
        test_inputs[layer_name]  = [ t.reshape((1, -1, n_x_cols)) for t in test_X[layer_name] ]
                  
    return train_X, train_Y, test_inputs, test_targets